[ { "title": "Tistory에서 GitHub Pages로 블로그 마이그레이션 완전 가이드", "url": "/posts/Tistory%EC%97%90%EC%84%9C-GitHub-Pages%EB%A1%9C-%EB%B8%94%EB%A1%9C%EA%B7%B8-%EB%A7%88%EC%9D%B4%EA%B7%B8%EB%A0%88%EC%9D%B4%EC%85%98-%EC%99%84%EC%A0%84-%EA%B0%80%EC%9D%B4%EB%93%9C/", "categories": "1. 기술, 서버, 데이터, 클라우드", "tags": "", "date": "2026-01-11 00:00:00 +0900", "snippet": "블로그를 Tistory에서 GitHub Pages(Jekyll)로 마이그레이션하는 과정은 생각보다 복잡합니다. 이 글에서는 112개의 포스트, 788개의 이미지를 성공적으로 마이그레이션한 전체 과정과 발생했던 에러들, 그리고 해결 방법을 공유하고자 합니다.왜 GitHub Pages로 이전했나?Tistory는 사용하기 쉽지만, 다음과 같은 한계가 있습니다: 플랫폼 의존성: Tistory 서비스 종료 시 데이터 손실 위험 완전한 커스터마이징 불가: 제한된 HTML/CSS 수정만 가능 버전 관리 불가: 포스트 변경 이력 관리 어려움 데이터 포팅 어려움: 다른 플랫폼으로 이전 시 복잡한 과정GitHub Pages는 이러한 문제를 해결합니다: 완전한 소유권: 모든 데이터를 git 저장소로 관리 버전 제어: git을 통한 완전한 변경 이력 관리 무제한 커스터마이징: 전체 코드 제어 가능 무료 호스팅: 백업 및 배포 자동화 마크다운 기반: 포맷 독립적인 콘텐츠 관리전체 마이그레이션 프로세스1단계: 포스트 추출 및 마이그레이션1.1 Tistory 스크래핑먼저 Tistory 블로그에서 모든 포스트를 추출해야 합니다.import requestsfrom bs4 import BeautifulSoupfrom pathlib import Pathimport timeBASE_URL = &quot;https://songseungwon.tistory.com&quot;PAGES = 12 # 페이지 수def scrape_tistory_posts(): &quot;&quot;&quot;Tistory에서 모든 포스트 URL 추출&quot;&quot;&quot; posts = [] for page in range(1, PAGES + 1): url = f&quot;{BASE_URL}?page={page}&quot; response = requests.get(url) soup = BeautifulSoup(response.content, &#39;html.parser&#39;) # 포스트 링크 추출 for link in soup.find_all(&#39;a&#39;, {&#39;class&#39;: &#39;post-link&#39;}): post_url = link.get(&#39;href&#39;) if post_url: posts.append(post_url) time.sleep(0.5) # 서버 부하 분산 return posts1.2 포스트 내용 추출def extract_post_content(url): &quot;&quot;&quot;개별 포스트에서 제목, 날짜, 내용 추출&quot;&quot;&quot; response = requests.get(url) soup = BeautifulSoup(response.content, &#39;html.parser&#39;) # 제목 추출 title = soup.find(&#39;h1&#39;, {&#39;class&#39;: &#39;title-article&#39;}) title_text = title.get_text(strip=True) if title else &quot;Untitled&quot; # 날짜 추출 (형식: 2025. 6. 10. 18:54) date_elem = soup.find(&#39;span&#39;, {&#39;class&#39;: &#39;date&#39;}) date_text = date_elem.get_text(strip=True) if date_elem else &quot;&quot; date_formatted = convert_date_format(date_text) # 2025-06-10 # 본문 추출 article = soup.find(&#39;div&#39;, {&#39;class&#39;: &#39;article-view&#39;}) return { &#39;title&#39;: title_text, &#39;date&#39;: date_formatted, &#39;content&#39;: article, &#39;url&#39;: url }def convert_date_format(date_str): &quot;&quot;&quot;Tistory 날짜 형식을 Jekyll 형식으로 변환 2025. 6. 10. 18:54 -&amp;gt; 2025-06-10 &quot;&quot;&quot; parts = date_str.split(&#39;.&#39;) year = parts[0].strip() month = parts[1].strip().zfill(2) day = parts[2].strip().zfill(2) return f&quot;{year}-{month}-{day}&quot;2단계: 이미지 추출 및 최적화2.1 이미지 다운로드def download_images(article, post_id): &quot;&quot;&quot;포스트에서 모든 이미지를 다운로드하고 URL 업데이트&quot;&quot;&quot; images_dir = Path(&quot;assets/images/posts&quot;) images_dir.mkdir(parents=True, exist_ok=True) img_counter = 0 for img in article.find_all(&#39;img&#39;): src = img.get(&#39;src&#39;) or img.get(&#39;data-src&#39;) if not src: continue try: # 이미지 URL 정규화 img_url = resolve_image_url(src, BASE_URL) response = requests.get(img_url, timeout=5) response.raise_for_status() # 파일 형식 감지 ext = get_image_extension(img_url, response) img_name = f&quot;{post_id}-{img_counter}{ext}&quot; img_dest = images_dir / img_name # 이미지 저장 with open(img_dest, &#39;wb&#39;) as f: f.write(response.content) # 로컬 경로로 업데이트 (중요!) img[&#39;src&#39;] = f&quot;/assets/images/posts/{img_name}&quot; img_counter += 1 except Exception as e: print(f&quot;이미지 다운로드 실패: {img_url} - {e}&quot;) img.decompose() # 실패한 이미지 제거def resolve_image_url(src, base_url): &quot;&quot;&quot;상대경로, CDN URL, 프로토콜 없는 URL 등 처리&quot;&quot;&quot; if src.startswith(&#39;http&#39;): return src elif src.startswith(&#39;//&#39;): return &#39;https:&#39; + src elif src.startswith(&#39;/&#39;): return base_url + src else: return base_url + &#39;/&#39; + srcdef get_image_extension(url, response): &quot;&quot;&quot;Content-Type으로부터 파일 확장자 감지&quot;&quot;&quot; content_type = response.headers.get(&#39;content-type&#39;, &#39;&#39;).lower() type_to_ext = { &#39;image/jpeg&#39;: &#39;.jpg&#39;, &#39;image/png&#39;: &#39;.png&#39;, &#39;image/gif&#39;: &#39;.gif&#39;, &#39;image/webp&#39;: &#39;.webp&#39; } for mime_type, ext in type_to_ext.items(): if mime_type in content_type: return ext # URL에서 확장자 추출 path = url.split(&#39;?&#39;)[0] return Path(path).suffix or &#39;.jpg&#39;2.2 WebP 압축 (75% 크기 감소)원본 이미지: 167.51 MB → 압축 후: 41.08 MB (126.43 MB 절약)from PIL import Imagedef compress_to_webp(image_path, quality=85): &quot;&quot;&quot;이미지를 WebP 형식으로 압축&quot;&quot;&quot; try: img = Image.open(image_path) # RGBA를 RGB로 변환 (압축률 향상) if img.mode in (&#39;RGBA&#39;, &#39;LA&#39;, &#39;P&#39;): background = Image.new(&#39;RGB&#39;, img.size, (255, 255, 255)) if img.mode == &#39;P&#39;: img = img.convert(&#39;RGBA&#39;) background.paste(img, mask=img.split()[-1] if img.mode in (&#39;RGBA&#39;, &#39;LA&#39;) else None) img = background elif img.mode not in (&#39;RGB&#39;, &#39;L&#39;): img = img.convert(&#39;RGB&#39;) # WebP로 저장 webp_path = image_path.with_suffix(&#39;.webp&#39;) img.save( webp_path, &#39;WEBP&#39;, quality=quality, method=6 # 최대 압축 ) # 원본 이미지 삭제 image_path.unlink() return webp_path except Exception as e: print(f&quot;WebP 압축 실패: {image_path} - {e}&quot;) return None2.3 마크다운으로 변환HTML을 마크다운으로 변환하면서 이미지 링크 보존이 중요합니다.from markdownify import markdownifydef convert_to_markdown(article): &quot;&quot;&quot;HTML을 마크다운으로 변환 이미지 링크 보존이 핵심! &quot;&quot;&quot; # 먼저 이미지를 다운로드하고 src를 업데이트 download_images(article, post_id) # 그 후에 마크다운으로 변환 markdown_content = markdownify( str(article), heading_style=&quot;underlined&quot; ) # 과도한 줄바꿈 정리 markdown_content = re.sub(r&#39;\\n\\s*\\n\\s*\\n&#39;, &#39;\\n\\n&#39;, markdown_content).strip() return markdown_content3단계: Jekyll 포스트 생성3.1 YAML Front Matter 작성def create_jekyll_post(title, date, content, categories, post_id): &quot;&quot;&quot;Jekyll 형식의 포스트 파일 생성&quot;&quot;&quot; # 파일명 생성 (YYYY-MM-DD-title.md) safe_title = re.sub(r&#39;[^\\w\\s-]&#39;, &#39;&#39;, title) safe_title = re.sub(r&#39;[-\\s]+&#39;, &#39;-&#39;, safe_title) filename = f&quot;_posts/{date}-{safe_title}.md&quot; # YAML Front Matter front_matter = f&quot;&quot;&quot;---layout: posttitle: {title}date: {date}categories: {categories}---&quot;&quot;&quot; # 포스트 저장 with open(filename, &#39;w&#39;, encoding=&#39;utf-8&#39;) as f: f.write(front_matter + content) return filename4단계: 카테고리 추출 및 매핑4.1 Tistory에서 카테고리 추출def extract_tistory_categories(): &quot;&quot;&quot;Tistory의 모든 카테고리 추출&quot;&quot;&quot; url = &quot;https://songseungwon.tistory.com/category&quot; response = requests.get(url) soup = BeautifulSoup(response.content, &#39;html.parser&#39;) categories = {} for link in soup.find_all(&#39;a&#39;, href=True): href = link.get(&#39;href&#39;, &#39;&#39;) if &#39;/category/&#39; in href: text = link.get_text(strip=True) if text: categories[text] = href return categories4.2 포스트에 카테고리 매핑def infer_category_from_title(title): &quot;&quot;&quot;포스트 제목에서 카테고리 추론&quot;&quot;&quot; title_lower = title.lower() # 키워드 기반 카테고리 분류 category_keywords = { &quot;Web/JavaScript&quot;: [&#39;javascript&#39;, &#39;html&#39;, &#39;css&#39;, &#39;nodejs&#39;, &#39;npm&#39;], &quot;ML/DL&quot;: [&#39;machine&#39;, &#39;deep&#39;, &#39;learning&#39;, &#39;neural&#39;, &#39;gan&#39;, &#39;xgboost&#39;], &quot;Finance&quot;: [&#39;주식&#39;, &#39;stock&#39;, &#39;etf&#39;, &#39;bitcoin&#39;, &#39;crypto&#39;, &#39;금융&#39;, &#39;finance&#39;], &quot;Server/Data/Cloud&quot;: [&#39;docker&#39;, &#39;kubernetes&#39;, &#39;elastic&#39;, &#39;gcp&#39;, &#39;cloud&#39;], &quot;Quantitative Investment&quot;: [&#39;정량&#39;, &#39;quantitative&#39;, &#39;매크로&#39;, &#39;macro&#39;], } for category, keywords in category_keywords.items(): if any(kw in title_lower for kw in keywords): return category return &quot;Technology&quot; # 기본값def add_category_to_post(post_file, category): &quot;&quot;&quot;포스트의 YAML에 카테고리 추가&quot;&quot;&quot; with open(post_file, &#39;r&#39;, encoding=&#39;utf-8&#39;) as f: content = f.read() # YAML 파싱 parts = content.split(&#39;---&#39;, 2) front_matter = parts[1] body = parts[2] # 카테고리 추가 front_matter = re.sub( r&#39;(date: [^\\n]+\\n)&#39;, r&#39;\\1categories: [&quot;&#39; + category + &#39;&quot;]\\n&#39;, front_matter ) # 저장 with open(post_file, &#39;w&#39;, encoding=&#39;utf-8&#39;) as f: f.write(f&quot;---{front_matter}---{body}&quot;)5단계: 에러 처리 및 검증5.1 발생한 주요 에러들에러 1: 제어 문자 (Control Characters)Error: YAML Exception reading post file:control characters are not allowed at line 1 column 1ERROR: Input contains prohibited control code point U+0008원인: Tistory에서 추출한 일부 포스트에 백스페이스(U+0008) 등의 제어 문자 포함해결:def remove_control_characters(text): &quot;&quot;&quot;제어 문자 제거&quot;&quot;&quot; cleaned = &#39;&#39;.join( ch for ch in text if ord(ch) &amp;gt;= 32 or ch in &#39;\\n\\t\\r&#39; ) return cleaned.replace(&#39;\\x00&#39;, &#39;&#39;)에러 2: 파일명의 과도한 대시예: 2019-09-23---NodeJS---NPM--PM2.md (3개 이상의 대시)해결:def clean_filename(filename): &quot;&quot;&quot;연속된 대시 제거&quot;&quot;&quot; clean = re.sub(r&#39;-{2,}&#39;, &#39;-&#39;, filename) return clean.strip(&#39;- &#39;)에러 3: 이미지 링크 누락원인: HTML을 `.get_text()`로 변환하면 모든 HTML 태그가 제거되어 이미지 태그도 함께 삭제됨해결:# 잘못된 방법 ❌content = article.get_text() # 이미지 링크 손실# 올바른 방법 ✅# 1단계: 이미지 다운로드 및 src 업데이트for img in article.find_all(&#39;img&#39;): # ... 이미지 다운로드 img[&#39;src&#39;] = f&quot;/assets/images/posts/{img_name}&quot;# 2단계: 마크다운으로 변환 (이미지 링크 보존)content = markdownify(str(article))에러 4: 잘못된 HTML 태그로 인한 빌드 실패ERROR: Invalid first code point of tag name U+D0DC원인: 마크다운 코드블록의 HTML 예제가 실제 HTML 태그로 렌더링됨예: &amp;lt;태그&amp;gt;, &amp;lt; 46, &amp;lt; 0.05가 HTML로 해석됨해결:# 잘못된 방법 ❌```html&amp;lt;script src=&quot;bg.js&quot;&amp;gt;&amp;lt;/script&amp;gt;올바른 방법 ✅&amp;amp;lt;script src=&quot;bg.js&quot;&amp;amp;gt;&amp;amp;lt;/script&amp;amp;gt;**에러 5: Tistory 아티팩트 텍스트**포스트 하단에 다음 텍스트가 자동으로 추가됨:- 공유하기- 게시글 관리- 관성을 이기는 데이터- 저작자표시 (새창열림)**해결**:```pythondef remove_tistory_artifacts(content): &quot;&quot;&quot;Tistory 아티팩트 제거&quot;&quot;&quot; artifacts = [ r&quot;^\\s*공유하기\\s*$&quot;, r&quot;^\\s*게시글 관리\\s*$&quot;, r&quot;^\\s*관성을 이기는 데이터\\s*$&quot;, r&quot;^\\s*저작자표시\\s*\\(새창열림\\)\\s*$&quot;, ] for pattern in artifacts: content = re.sub(pattern, &#39;&#39;, content, flags=re.MULTILINE) return content5.2 검증 스크립트import yamldef validate_posts(): &quot;&quot;&quot;모든 포스트 검증&quot;&quot;&quot; posts_dir = Path(&#39;_posts&#39;) valid_count = 0 error_count = 0 for post_file in posts_dir.glob(&#39;*.md&#39;): try: with open(post_file, &#39;r&#39;, encoding=&#39;utf-8&#39;) as f: content = f.read() # YAML 파싱 (문법 체크) if content.startswith(&#39;---&#39;): parts = content.split(&#39;---&#39;, 2) yaml.safe_load(parts[1]) # 제어 문자 확인 if any(ord(ch) &amp;lt; 32 and ch not in &#39;\\n\\t\\r&#39; for ch in content): print(f&quot;제어 문자 발견: {post_file.name}&quot;) error_count += 1 continue valid_count += 1 except Exception as e: print(f&quot;오류: {post_file.name} - {e}&quot;) error_count += 1 print(f&quot;\\n✅ 유효한 포스트: {valid_count}&quot;) print(f&quot;❌ 오류: {error_count}&quot;) return valid_count, error_count최종 결과마이그레이션 통계 항목 수치 총 포스트 112개 총 이미지 788개 원본 이미지 크기 167.51 MB 압축 후 이미지 크기 41.08 MB 크기 감소 126.43 MB (75.5% 절약) 포스트 검증 성공률 100% (112/112) 카테고리 매핑 100% (112/112) 카테고리 분포 카테고리 포스트 수 비율 Finance (금융) 32 28.6% Technology (기술) 23 20.5% Web/JavaScript 17 15.2% ML/DL (머신러닝) 16 14.3% Server/Data/Cloud 13 11.6% Quantitative Investment 8 7.1% Business Analytics 3 2.7% Git 커밋 히스토리마이그레이션 전체 과정은 다음과 같은 커밋으로 기록되었습니다:002797c migrated 112 posts from Tistory7092b7f Fix HTML code blocks in Web-HTML post14cf009 Fix all remaining HTML code blocks06010c6 Fix HTML code blocks - escape all tags7f974a4 Remove Tistory blog artifacts from all postsaac9db5 Add categories to all posts배운 점 및 권장사항1. 사전 계획의 중요성마이그레이션 전에 다음을 확인하세요: 포스트 수 및 이미지 수 파일 명명 규칙 카테고리 구조 예상 소요 시간2. 자동화 스크립트 작성수동으로 하나씩 처리하는 것은 비현실적입니다. Python으로 자동화 스크립트를 작성하세요.3. 단계별 검증각 단계 후 검증을 수행하세요: 포스트 추출 후: 포스트 수 확인 이미지 다운로드 후: 이미지 개수 및 링크 확인 마크다운 변환 후: YAML 문법 검증 최종: 모든 포스트 빌드 테스트4. 버전 관리 활용Git을 활용하여 각 단계를 기록하세요:git add -Agit commit -m &quot;Step 1: Extract posts from Tistory&quot;git commit -m &quot;Step 2: Download and compress images&quot;git commit -m &quot;Step 3: Convert to Jekyll format&quot;5. 에러 로깅모든 오류를 기록하고 분류하세요:import logginglogging.basicConfig( filename=&#39;migration.log&#39;, level=logging.INFO, format=&#39;%(asctime)s - %(levelname)s - %(message)s&#39;)try: # 작업 수행 passexcept Exception as e: logging.error(f&quot;Error: {e}&quot;, exc_info=True)마이그레이션 스크립트 제공전체 마이그레이션 프로세스를 위한 통합 스크립트는 다음 저장소에서 확인할 수 있습니다:https://github.com/sw-song/sw-song.github.io포함된 스크립트: migrate.py: 메인 마이그레이션 스크립트 compress_images.py: WebP 압축 remove_tistory_artifacts.py: 아티팩트 제거 map_post_categories.py: 카테고리 매핑 validate_posts.py: 포스트 검증결론Tistory에서 GitHub Pages로의 마이그레이션은 초기에는 복잡해 보이지만, 체계적으로 접근하면 성공할 수 있습니다. 주요 포인트는: 자동화: 수백 개의 포스트는 수동으로 처리 불가능 검증: 각 단계에서 결과를 확인하고 오류 처리 문서화: 각 단계와 오류를 기록 버전 관리: Git으로 진행 상황 추적이 가이드가 여러분의 블로그 마이그레이션에 도움이 되길 바랍니다!참고 자료 Jekyll 공식 문서 GitHub Pages 가이드 Markdownify 문서 BeautifulSoup 문서" }, { "title": "국내/해외 ETF 매매시 일반 위탁 계좌 vs 절세 계좌(연금저축, IRP, ISA) 세금 차이 정리", "url": "/posts/%EA%B5%AD%EB%82%B4-%ED%95%B4%EC%99%B8-ETF-%EB%A7%A4%EB%A7%A4%EC%8B%9C-%EC%9D%BC%EB%B0%98-%EC%9C%84%ED%83%81-%EA%B3%84%EC%A2%8C-vs-%EC%A0%88%EC%84%B8-%EA%B3%84%EC%A2%8C-%EC%97%B0%EA%B8%88%EC%A0%80%EC%B6%95-IRP-ISA-%EC%84%B8%EA%B8%88/", "categories": "2. 도메인, 자산운용", "tags": "", "date": "2025-06-10 00:00:00 +0900", "snippet": "ETF 매매 시 세금 (*배당소득세는 원천징수) 일반 위탁 계좌 국내상장 국내주식형 ETF: 매도 -&amp;gt; 증권거래세 X 매매차익 -&amp;gt; 비과세 배당소득 -&amp;gt; 배당소득세 15.4% 국내상장 국내주식형 외 ETF: 매도 -&amp;gt; 증권거래세 X 매매차익 -&amp;gt; 배당소득세 15.4% 배당소득 -&amp;gt; 배당소득세 15.4% 해외상장 ETF(*위탁계좌 only): 매도 -&amp;gt; 증권거래세 X (*SEC fee 소액 발생 0.00x 수준) 매매차익 -&amp;gt; 매매차익 250만원 초과분 양도소득세 22% (*분리과세) 매당소득 -&amp;gt; 배당소득세 15.4% 절세 계좌: 연금 계좌(연금저축, IRP): 연금수령 -&amp;gt; 연금소득세 3.3 ~ 5.5% (*분리과세) 연금 외 수령 -&amp;gt; 기타 소득세 16.5% 연말정산 세액공제 -&amp;gt; 납입금 내 최대 600만 원 한도(*2023년부터 400-&amp;gt;600 증액)에 대해 13.2% *16.5% if under 총 급여 5,500만 원(or 종합소득 4,500만 원) *연말정산 환급액 = 기납부세액 - (소득 공제 후 산출 세액 - 세액공제액) ! 세액 공제액은 *소득 공제 후 산출 세액을 초과할 수 없음 (소득 공제 후 산출 세액 &amp;gt;= 세액공제액)*** 종합자산관리 계좌(중개형 ISA): 중도해지 -&amp;gt;  위탁 계좌 과세와 동일 만기해지 -&amp;gt; 손익통산(*매매차익+분배금) 200만 원(*서민형 400만 원) 초과분 9.9% (*분리과세) 참고) ISA·연금저축·IRP ‘절세계좌 3종’ 혜택 하나씩 비교해 보자면? 살 때보다 팔 때 알아야 할, ETF 세금의 모든 것 소득세법 제61조(세액감면액 및 세액공제액의 산출세액 초과 시의 적용방법 등)관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "[Dolt + Python + MySQL] 금융 분석을 위한 재무제표 데이터베이스 구축과 실행", "url": "/posts/Dolt-Python-MySQL-%EA%B8%88%EC%9C%B5-%EB%B6%84%EC%84%9D%EC%9D%84-%EC%9C%84%ED%95%9C-%EC%9E%AC%EB%AC%B4%EC%A0%9C%ED%91%9C-%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4-%EA%B5%AC%EC%B6%95%EA%B3%BC/", "categories": "1. 기술, 서버, 데이터, 클라우드", "tags": "", "date": "2025-05-09 00:00:00 +0900", "snippet": "재무제표 데이터를 확보할 때 yfinance를 많이 사용하며, 긴 시계열에 대해서는 공시자료를 스크래핑하여 필요한 table 영역만 적절히 포멧팅할 수도 있다. 가장 좋은 건 yfinance 같은 무료 api를 활용해 적정 주기마다 데이터를 가져와 적재해 두는 것인데, 상장된 모든 주가 재무정보를 개인 PC나 클라우드 서버에 저장해 두는 건 개인으로서 여간 부담되는 일이 아니다.이때, 약소하게나마 대안으로 사용해 볼 수 있는 것이 다른 사람들이 공개해 둔 DB인데, DropBox나 DoltHub가 대표적이다. 코드가 익숙하다면 DoltHub에서 매우 쉽고 빠르게 DB 전체를 Pull 할 수 있으며 아래 예시로 사용한 1.26 GB짜리 재무데이터를 수 초 내에 모두 다운로드 가능하다.DoltHub는 코드 버전관리를 위해 Git을 사용하는 것처럼, SQL DB의 버전관리 목적으로 이용할 수 있다. fork, clone, branch, merge, push, pull 등 git command를 그대로 차용하기 때문에 git에 익숙하다면 어렵지 않게 적응이 가능하다. Open Source라 Github에도 코드베이스가 공개되어 있으며 여타 Open Source가 그런 것처럼 Enterprise Support 등에 대해 별도의 가격 플랜은 존재한다.예시 저장소는 미국 기업을 대상으로 하며, 상장된 모든 기업은 아니지만 상당히 많은 기업의 10년 이상 적재된 raw data를 받아볼 수 있다.post-no-preference / earnings사용법은 Github이나 Dolt Document에서 간단히 살펴볼 수 있으니 각 명령어에 대한 설명은 공식 문서를 참고하길 바라며, 본문에서는 필요한 데이터를 추출하는 프로세스만 가볍게 다뤄보겠다.먼저 Dolt는 Git과 같은 버전관리 도구이므로, 버전관리 대상 폴더를 지정하여 init 시키는 것이 선행되어야 한다. 만약, Dolt를 현재 작업 중인 PC에서 처음 사용한다면 DoltHub에 가입하고 가입 정보를 global config로 설정해줘야 한다. Git의 Remote 저장소를 GitHub로 설정하기 위한 작업과 동일하다.user.email, user.name 뒤에는 각자 계정 정보를 입력한다.dolt init 명령어를 수행하면 .dolt 폴더와 함께 버전관리에 필요한 파일들이 생성되고 작업 폴더에서 SQL DB의 변경된 버전을 저장하고, 원격 저장소에 밀어 넣는 등의 작업을 수행할 수 있다.그러나 우리는 필요한 데이터를 가져오기만 하면 되기 때문에 dolt clone 명령어와 함께 필요한 repository를 지정해주면, 현재 경로 하위에 earnings라는 폴더가 생성되고, 해당 폴더 내 .dolt 폴더 역시 확인할 수 있다.위 예시에서는 명령어 실행을 위해 임시 폴더를 사용하였음데이터가 잘 가져왔다면 dolt를 활용해 쿼리를 실행해본다. mysql이 설치되어 있다면 기본적으로 mysql, information_schema 2개의 database는 보일 것이고, 추가된 earnings database와 그 하위 table을 확인할 수 있다.아래는 데이터를 일부 추출한 예시다.우리는 sql 서버를 dolt로 띄우고, 띄운 서버를 python으로 접근해 데이터를 쿼리 할 것이다. dolt sql-server 명령어를 수행하면 sql 로컬 서버를 기본 포트 3306번으로 띄울 수 있다.python에서는 mysql-connector-python 패키지를 설치한 다음 아래와 같이 명령어를 수행하면 SQL 서버에 진입 가능하고, pandas read_sql 함수를 통해 쿼리가 동일하게 실행되어야 한다.이제 자유롭게 필요한 데이터를 가공해 분석에 활용할 수 있는 상태다. 위와 같이 DoltHub로 기반 DB를 clone 해서 사용할 수도 있고, yfinance 등 다른 api를 통해 주기적으로 필요한 데이터만 가공, 적재할 수도 있다.어떤 방식이든 결국 원하는 재무제표 정보를 전기간 내 손실 없이 자체 관리하는 것이 필요해진다. 금융 분석을 위한 데이터를 온전히 외부에 의존하기에는 어떤 api든 높은 비용이 발생하게 되므로 Local Server에 데이터를 적재하고 후처리 하는 일련의 데이터 파이프라인 구축 작업을 고려해야 한다. 이때, DoltHub를 사용해 보면 어떤 구조로 ERD를 구성하고 데이터를 관리해야 하는지 감을 잡을 수 있으니 첫 선택지로 좋은 옵션이다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "[파이썬 퀀트 투자] 벨류에이션 멀티플 기반 피어 그룹 스크리닝", "url": "/posts/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%ED%80%80%ED%8A%B8-%ED%88%AC%EC%9E%90-%EB%B2%A8%EB%A5%98%EC%97%90%EC%9D%B4%EC%85%98-%EB%A9%80%ED%8B%B0%ED%94%8C-%EA%B8%B0%EB%B0%98-%ED%94%BC%EC%96%B4-%EA%B7%B8%EB%A3%B9-%EC%8A%A4%ED%81%AC%EB%A6%AC%EB%8B%9D/", "categories": "4. 실전, 계량 투자 분석", "tags": "", "date": "2025-05-08 00:00:00 +0900", "snippet": "이번 글에서는 계량적 방법을 활용해 타겟 기업의 유사 그룹을 추출하고, 추출한 그룹 내 벨류에이션 멀티플에 대한 상대평가를 진행해 본다. 그전에, 동일한 스타일로 분석 대상 타겟 기업을 선정하는 것은 본 블로그의 [파이썬 퀀트 투자] 좋은 기업을 찾아보자 - 미국 기술주 멀티플 EDA에서 확인할 수 있다.1. 데이터셋 준비항상 데이터셋을 준비하는 것이 먼저다. 분석 대상 기업은 Block Inc(Ticker: XYZ, 이하 Block)로, Software &amp;amp; Services 산업에 속하니 해당 산업군 내 모든 티커를 가져온 다음 재무제표를 추출할 것이다. 재무제표는 이후 멀티플 계산에 사용된다.1-1. 산업군 티커 추출Ticker는 야후 파이낸스 api로 쿼리 할 수 있으며, 직접 만든 모듈을 사용할 것이다. 작성한 코드는 github repository 내 utils 폴더에 공개해 두었다.1-2. Historical 재무제표 추출다음, 재무제표 데이터를 가져온다. 아래 코드는 개인 local database에 저장된 데이터를 직접 쿼리 하는 예시인데 이러한 방대한 데이터를 개인적으로 DB화 해두고 있는 경우는 흔치 않을 것이니 추후 공개된 데이터를 쉽게 개인 서버로 카피하는 방안에 대해서도 다뤄보겠다.Local DB에 만들어둔 데이터가 없다면 위 작업은 불가능하다. 대신 야후 파이낸스를 이용하는 방법도 있다. 탐색 기간이 짧을 뿐, 시장에서 거래 중인 모든 기업의 최근 5년간 재무제표를 아래와 같이 확보할 수 있다.e.g. 현금흐름표e.g. 재무상태표1-3. 산업군 필터링재무제표 데이터를 앞서 추출한 티커 목록으로 필터링한다. 산업군마다 멀티플의 양상이 상이하므로, 동일 산업군 데이터만 가지고 유사 그룹을 추출하기 위함이다.###2. 산업군 통계필요한 데이터는 모두 만들었고, 드디어 통계분석을 해볼 수 있다.2-1. Total Asset동일 산업군의 자산 추이는 어떨까? 히스토그램으로 뽑아보았을 때 자산 규모 기준 하위 10% 그룹이 절대다수(약 97%)를 차지한다. 타겟 기업 Block 역시 여기에 속하지만, 결코 낮은 수준은 아니다.2-2. Total Equity자기 자본의 경우 음수인 영역이 꽤나 비중 있게 보인다. Block의 위치는 앞서 봤던 자산 규모를 고려할 때 자본의 비중은 상대적으로 높다.2-3. Income Statement현금 흐름은 매출, 총이익, 순이익 세 지표를 함께 나열했는데, Block은 모두 산업 평균을 웃도는 수준이다. 점선 좌측부터 우측으로 갈수록 “최근 지표”다. 현금 흐름이 지속적으로 개선되는 모양새다.3. 유사 그룹 비교산업에서 더 깊이 들어가 재무제표 멀티플이 Block과 유사한 피어 그룹을 대상으로 분석해보려 한다. 이를 위해 멀티플을 정의하고, 멀티플 갭이 +/- 20% 이내에 들어오는 기업을 유사 그룹으로 분류하도록 하자.3-1. 유사 그룹 추출멀티플은 매출 대비 총이익, 매출 대비 순이익 2가지로 놓고, 적정 오차(20%) 수준에 들어오는 기업들을 추출한다.340개 기업 중 총 19개 기업이 유사 그룹으로 추출되었다. 스크리닝 대상을 좁히는 전략으로 충분해 보인다.3-2. 시가 총액 비교유사 기업들의 현재 주가와 주식 유통량으로 시가총액을 계산한 후 연도별 시가총액 추이를 그려보았다.Block은 한 때, 시가총액이 유사 그룹 내 가장 높았으며 현재 평균 수준으로 돌아온 상태다. 다른 기업들과는 시총 추이가 상이하므로 이런 기업을 분석할 때에는 반드시 이상 구간의 이벤트를 정성적으로 확인할 필요가 있다. 다만 해당 내용은 본문의 의도에서는 많이 벗어나는 내용이므로 제외한다.3-3 시가 총액 상위 피어 EV/EBITDA 비교EV(Enterprise Value)는 이 기업을 통째로 인수할 때 드는 비용으로, 시가총액에서 부채를 제하고 보유 현금을 더해서 계산할 수 있다. 멀티플은 항상 비율로 계산해야 적절한 상대 평가가 이루어질 수 있기 때문에 EV와 함께 보편적으로 사용되는 EBITDA를 기준 척도로 사용하도록 하자. 참고로 EBITDA는 이자비용, 감가상각비, 세금 등을 제한 영업이익이므로 해당 기업의 실질적인 이익 창출 능력을 투명하게 보여주는 지표로써 멀티플 계산에 흔히 사용된다.이 멀티플을 활용해 피어 그룹 내 시총 상위 기업들 간 벨류에이션 수준을 비교해 보도록 하자. 그룹을 쪼개고 붙여가며 비교 평가하는 것은 묶음을 달리 했을 때 그 군집에 대한 인사이트가 달라지기 때문에 이를 얻고자 함이다.*상단 EV 계산 부분 생략 - github repository 참고바 사이즈(width)는 EV를 의미하고 annotation으로 계산한 멀티플(EV/EBITDA)을 표시했다(좋은 방법은 아니지만, y축을 2개 쓰는 등의 차트 복잡도를 높이는 것보다 오히려 직관적이고 유용할 때가 있다). 이 차트에서 기대했던 것은 EV 대비 EV/EBITDA가 극적으로 낮은 것인데(낮을수록 저평가된 것이다) 아쉽게도 그렇진 않았다. Ticker:ADP의 경우 Ticker:ACN 대비 상대적으로 고평가 된 모습을 볼 수 있다.3-4. 그룹 내 EV/EBITDA 비교이번에는 전체 피어 그룹 대상으로 EV/EBITDA 평균치와 각 기업들의 멀티플 수준을 확인해 보자.*상단 EV/EBITDA 계산 부분 생략 - github repository 참고멀티플 평균은 15.5배로, Block의 멀티플보다 4.8이 높다. 앞서 Market Cap을 확인했을 때 폭락하다시피 시총이 내려앉아서인지 현재의 멀티플은 매우 좋고, 저평가된 상태로 볼 수 있다. Ticker:TYL의 경우 시총 상위 그룹에 속하지도 않았는데 멀티플을 51.7배 받고 있다. EV가 매우 높거나 EBITDA가 매우 낮은 것이다.3-5. 상이한 기업 간 주요 재무 지표 비교TYL이 XYZ보다 5배나 높은 멀티플이 잡히는 원인을 조금 더 살펴보도록 하자. 주가가 낮은데 EV/EBITDA가 높으면 일반적으로 현금 흐름이 좋지 않은 경우다.실제로 TYL과 XYZ의 여러 지표들을 직접 비교해 보면 역시 현금 흐름 측면에서 극명히 갈린다. 부채 수준이 TYL이 매우 낮아 보이나, 낮은 현금 흐름 고려할 때 그렇지도 않다.EBITDA와 달리 EV는 10% 내외 오차 수준이다.XYZ의 산업 평균 대비 준수한 멀티플은 현금 흐름의 개선에 있다.이와 같이 분석 대상 기업에 대해 재무제표 기준 멀티플을 산업, 피어그룹 내 비교 평가할 수 있다. 개인이 동일한 내용을 산업 보고서, 공시 자료, 그리고 주가 차트를 돌아다니며 리서치하기에는 시간적인 제약이 크기 때문에 계량적 방법을 유연하게 활용할 수 있는 것은 큰 도움이 된다.물론 이렇게 선별해 낸 기업을 즉시 투자하는 건 다른 얘기다. 상대 평가로는 해당 산업 자체에 대한 벨류에이션 리스크를 상쇄할 수 없으며, 어느 시점에 어느 비중으로 매매할 것인가 등의 투자로 이어지는 의사결정을 위해서는 수많은 절차적 고려 사항들이 존재한다. 그럼에도 불구하고 스크리닝과 가치평가에 계량적 도구를 활용하는 것은 지속적이고 반복적인 투자 의사결정에 필수적인 요소이므로 최소한 시장 지수가 아닌 개별 종목에 투자하고자 한다면 자신만의 스크리닝 전략과 도구를 개발하고 꾸준히 발전시켜 나가는 노력이 반드시 필요하겠다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "[파이썬 퀀트 투자] 좋은 기업을 찾아보자 - 미국 기술주 멀티플 EDA", "url": "/posts/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%ED%80%80%ED%8A%B8-%ED%88%AC%EC%9E%90-%EC%A2%8B%EC%9D%80-%EA%B8%B0%EC%97%85%EC%9D%84-%EC%B0%BE%EC%95%84%EB%B3%B4%EC%9E%90-%EB%AF%B8%EA%B5%AD-%EA%B8%B0%EC%88%A0%EC%A3%BC-%EB%A9%80%ED%8B%B0%ED%94%8C-EDA/", "categories": "4. 실전, 계량 투자 분석", "tags": "", "date": "2025-05-04 00:00:00 +0900", "snippet": "투자 프로세스에서 가장 선행되어야 할 것은 좋은 기업을 찾는 일이다. 퀀트 투자에서 스크리닝 단계는 넓고 단순한 분석에서 출발해 좁고 복잡한 분석으로 이어진다. 동일 집단 내 상대적으로 재무 상태가 양호한 기업들을 스크리닝해 투자 후보군을 추려내면 이후 후보군에 대해 여러 가지 가치평가 및 안전마진 계산을 수행한다.어떤 기준으로 후보군을 추려낼 것인지 명확하지 않을 때는 대표적인 재무 멀티플을 사용해 볼 수 있다. 야후 파이낸스 파이썬 패키지는 이미 계산된 멀티플을 제공하고 있으니 이를 활용해 미국 상장 소프트웨어 기술 기업 중 재무 상태가 양호한 후보군을 추려내는 작업을 진행해 보자.1. yfiance로 주요 기업 티커 및 재무 지표 추출타깃 시장의 모든 티커를 추출하는 함수와 멀티플을 추출하는 함수를 작성한다.# import libraries to get financial statement data# Import required librariesimport pandas as pdimport numpy as npimport yfinance as yfimport requestsimport jsonfrom datetime import datetime, timedeltaimport matplotlib.pyplot as pltimport seaborn as snsdef get_symbols(region=&#39;us&#39;, sector=&#39;Technology&#39;, peer_group=&#39;Software &amp;amp; Services&#39;): # Get software &amp;amp; services tickers from Yahoo Finance query = yf.EquityQuery(&#39;and&#39;,[ yf.EquityQuery(&#39;EQ&#39;,[&#39;region&#39;, region]), yf.EquityQuery(&#39;EQ&#39;,[&#39;sector&#39;, sector]), yf.EquityQuery(&#39;EQ&#39;,[&#39;peer_group&#39;, peer_group]) ]) offset = 0 size = 250 res = yf.screen(query=query, offset=0, size=250, sortAsc=True) df_symbols = pd.DataFrame(res[&#39;quotes&#39;])[[&#39;symbol&#39;,&#39;shortName&#39;]] if res[&#39;total&#39;] &amp;gt; size: for i in range(res[&#39;total&#39;]//size): offset += size res = yf.screen(query=query, offset=offset, size=size, sortAsc=True) df_symbols = pd.concat([df_symbols, pd.DataFrame(res[&#39;quotes&#39;])[[&#39;symbol&#39;,&#39;shortName&#39;]]]) df_symbols.reset_index(drop=True, inplace=True) return df_symbolsdef get_valuation_multiples(symbol): def safe_get(info_dict, key): try: result = info_dict.get(key) if result is None: print(f&quot;Warning: {key} returned None&quot;) return result except: print(f&quot;Error getting {key}&quot;) return None def safe_ratio(info_dict, numerator_key, denominator_key): try: numerator = safe_get(info_dict, numerator_key) denominator = safe_get(info_dict, denominator_key) if numerator is None or denominator is None: print(f&quot;Warning: Could not calculate ratio {numerator_key}/{denominator_key} - numerator or denominator is None&quot;) return None if denominator == 0: print(f&quot;Warning: Could not calculate ratio {numerator_key}/{denominator_key} - denominator is 0&quot;) return None return numerator / denominator except: print(f&quot;Error calculating ratio {numerator_key}/{denominator_key}&quot;) return None # Get info and initialize multiples dict info = yf.Ticker(symbol).info multiples = {} # Direct values multiples[&#39;PER&#39;] = safe_get(info, &#39;trailingPE&#39;) multiples[&#39;PBR&#39;] = safe_get(info, &#39;priceToBook&#39;) multiples[&#39;EV/Revenue&#39;] = safe_get(info, &#39;enterpriseToRevenue&#39;) multiples[&#39;EV/EBITDA&#39;] = safe_get(info, &#39;enterpriseToEbitda&#39;) multiples[&#39;PEG&#39;] = safe_get(info, &#39;trailingPegRatio&#39;) multiples[&#39;Profit Margin&#39;] = safe_get(info, &#39;profitMargins&#39;) multiples[&#39;Operating Margin&#39;] = safe_get(info, &#39;operatingMargins&#39;) multiples[&#39;ROA&#39;] = safe_get(info, &#39;returnOnAssets&#39;) multiples[&#39;ROE&#39;] = safe_get(info, &#39;returnOnEquity&#39;) multiples[&#39;Beta&#39;] = safe_get(info, &#39;beta&#39;) multiples[&#39;Debt/Equity&#39;] = safe_get(info, &#39;debtToEquity&#39;) # Calculated ratios multiples[&#39;Cash/Revenue&#39;] = safe_ratio(info, &#39;totalCash&#39;, &#39;totalRevenue&#39;) multiples[&#39;Debt/Revenue&#39;] = safe_ratio(info, &#39;totalDebt&#39;, &#39;totalRevenue&#39;) return multiplesget_symbols() 함수를 기본 인자로 실행하면 아래와 같이 339개의 티커명, 기업명을 가져올 수 있다.이어서 각 티커마다 순회하며 필요로 하는 멀티플을 추출해 데이터프레임으로 생성해 준다.df_symbols = get_symbols()# Create empty list to store resultsresults = []# Iterate through each symbolfor idx, row in df_symbols.iterrows(): # Get valuation multiples for current symbol multiples = get_valuation_multiples(row[&#39;symbol&#39;]) # Combine symbol, shortname and multiples into dict result_dict = { &#39;symbol&#39;: row[&#39;symbol&#39;], &#39;shortName&#39;: row[&#39;shortName&#39;] } result_dict.update(multiples) # Append to results list results.append(result_dict) # Print progress print(f&quot;Processed {idx+1}/{len(df_symbols)} symbols&quot;)# Create dataframe from resultsdf_results = pd.DataFrame(results)df_results위 출력된 Warning에 작성된 것처럼 티커마다 특정 멀티플은 제공하지 않는 경우가 있는데, 간단한 수식이라 굳이 yfinance가 제공하는 데이터에 제한하지 않고 직접 계산할 수도 있다. 여기서 이 작업은 생략하고, 멀티플 중 하나라도 누락된 값이 있으면 해당 티커는 제외하겠다.df_results_womissing = df_results.dropna().reset_index(drop=True)df_results_womissing###2. 상호 독립적인 멀티플 추출다각도의 재무 상태를 살피기 위해 많은 멀티플을 필요로 하진 않는다. 오히려 총 13개나 되는 멀티플을 모두 고려하면 자칫 비슷한 지표들, 혹은 연관 지표들의 가중치가 강하게 들어가 특정 재무 지표에 쏠린 후보군이 만들어질 수 있다.따라서 상관분석을 통해 서로 연관성이 0에 가까운 지표들을 추출하겠다. 우선 상관도 히트맵을 그려보면 함께 움직이는 지표들, 그리고 반대로 움직이는 지표들을 수치로 확인할 수 있다.#visualize correlation matrixplt.figure(figsize=(8, 4))sns.heatmap(df_results_womissing.iloc[:,2:].corr(), annot=True, cmap=&#39;coolwarm&#39;, annot_kws={&#39;size&#39;: 6})plt.show()이 모든 지표들 중 자기 자신을 제외한 다른 지표로부터의 영향도(상관계수) 절댓값이 0.1보다 낮은 지표들을 아래와 같이 추출한다.# Find multiples with correlation &amp;amp;lt; 0.1 (low correlation)low_corr = abs(df_results_womissing.iloc[:,2:].corr()) &amp;lt; 0.1# Initialize list to store independent multiplesindependent_multiples = []# Start with first multipleremaining_cols = list(low_corr.columns)current_col = remaining_cols[0]independent_multiples.append(current_col)remaining_cols.remove(current_col)# Find additional multiples that are uncorrelated with all selected oneswhile remaining_cols: for col in remaining_cols[:]: # Check if column is uncorrelated with all selected multiples is_independent = True for selected in independent_multiples: if not low_corr.loc[selected, col]: is_independent = False break if is_independent: independent_multiples.append(col) remaining_cols.remove(col) # If no more independent multiples found, break if not any(col in remaining_cols for col in independent_multiples): breakprint(&quot;Independent multiples with no correlation to each other:&quot;)print(independent_multiples)low_corr이렇게 추출한 독립 요인들의 상관계수를 별도로 히트맵으로 그려보면 모든 상관계수가 0에 근접함을 확인할 수 있다.plt.figure(figsize=(6, 3))sns.heatmap(df_results_womissing[independent_multiples].corr(), annot=True, cmap=&#39;coolwarm&#39;, center=0, fmt=&#39;.3f&#39;, annot_kws={&#39;size&#39;: 6})plt.title(&#39;Correlation Heatmap of Independent Multiples&#39;, size=&#39;small&#39;)plt.xticks(size=&#39;small&#39;)plt.yticks(size=&#39;small&#39;)plt.tight_layout()plt.show()###3. 타깃 멀티플 기준 최상위 기업 선별앞서 기업 선별에 활용할 멀티플을 PER, PBR, PEG, Cash/Revenue로 타겟팅했다. Cash/Revenue는 높을수록 좋고, 그 외는 분자가 가격이므로 낮을수록 좋다. 이 기준으로 각 기업들을 정렬해 보자.# Create 2x2 subplotsn_metrics = len(independent_multiples)fig, axes = plt.subplots(2, 2, figsize=(12, 6))axes = axes.flatten()# Plot bar chart for each metricfor i, metric in enumerate(independent_multiples): if i &amp;amp;gt;= 4: # Only show first 4 metrics break # Sort data by metric and get top 20 companies # Sort ascending for all metrics except Cash/Revenue ascending = True if metric != &#39;Cash/Revenue&#39; else False data = df_results_womissing.sort_values(by=metric, ascending=ascending).head(20) # Create bar plot sns.barplot(data=data, x=&#39;symbol&#39;, y=metric, ax=axes[i]) # Customize plot sort_direction = &quot;Lowest&quot; if ascending else &quot;Highest&quot; axes[i].set_title(f&#39;{sort_direction} 20 Companies by {metric}&#39;, fontsize=10) axes[i].tick_params(axis=&#39;x&#39;, rotation=45, labelsize=8) axes[i].tick_params(axis=&#39;y&#39;, labelsize=8) axes[i].set_xlabel(&#39;Company Symbol&#39;, fontsize=8) axes[i].set_ylabel(metric, fontsize=8)plt.tight_layout()plt.show()PBR의 경우 지표가 0보다 낮으면 자본잠식 상태임을 고려하고, 나머지 정렬 상태를 확인한다. 종합 순위를 매기기 위해 각 지표별 순위를 매기고 4개 지표의 평균 순위로 후보군 Top 10을 선정한다.# Create a scoring system based on multiple rankings# For each multiple, rank companies from 1-n (n being total companies)# Lower values are better for all metrics EXCEPT Cash/Revenue where higher is betterscoring_metrics = independent_multiples.copy()df_scores = df_results_womissing.copy()for metric in scoring_metrics: # For all metrics except Cash/Revenue, lower is better so ascending=True # For Cash/Revenue, higher is better so ascending=False ascending = False if metric == &#39;Cash/Revenue&#39; else True # Rank companies for this metric (1 is best) df_scores[f&#39;{metric}_rank&#39;] = df_scores[metric].rank(ascending=ascending)# Calculate average rank across all metricsrank_columns = [col for col in df_scores.columns if col.endswith(&#39;_rank&#39;)]df_scores[&#39;avg_rank&#39;] = df_scores[rank_columns].mean(axis=1)# Sort by average rank and get top 10 companiestop_10_companies = df_scores.sort_values(&#39;avg_rank&#39;).head(10)# Display results with key metricsdisplay_columns = [&#39;symbol&#39;, &#39;shortName&#39;] + independent_multiples + [&#39;avg_rank&#39;]print(&quot;\\nTop 10 Companies Based on Multiple Analysis:&quot;)print(top_10_companies[display_columns].to_string(index=False))# Create visualization of the top 10 companies&#39; metricsplt.figure(figsize=(8, 4))# Plot heatmap of metrics for top 10 companiesmetrics_data = top_10_companies[independent_multiples]metrics_data = (metrics_data - metrics_data.mean()) / metrics_data.std() # Standardize for better visualization# Convert metrics_data to numeric values and handle any non-numeric valuesmetrics_data = metrics_data.apply(pd.to_numeric, errors=&#39;coerce&#39;)# Create heatmap only if data is numericif not metrics_data.isna().all().all(): sns.heatmap(metrics_data, annot=True, cmap=&#39;RdYlGn_r&#39;, xticklabels=independent_multiples, yticklabels=top_10_companies[&#39;symbol&#39;], fmt=&#39;.2f&#39;, annot_kws={&#39;size&#39;: 6}) # Smaller annotation font size plt.title(&#39;Standardized Metrics Heatmap for Top 10 Companies&#39;, fontsize=8) plt.xticks(fontsize=8, rotation=45) # Smaller x-axis labels plt.yticks(fontsize=8) # Smaller y-axis labels plt.tight_layout() plt.show()else: print(&quot;Error: No numeric data available for heatmap visualization&quot;)종합 순위로 정렬하고, 각 값을 지표별 0을 중심으로 표준화하여 히트맵으로 표현했다. 잭 도시가 창업한 미국 모바일 결제 기업 XYZ를 포함한 상위 두 개의 기업이 전자 결제 서비스, 다음은 고객관리 서비스 두 기업이 뒤를 잇는다.동종 산업군의 각 지표 평균을 보면 언급한 기업들의 상대적 우위를 확인할 수 있다.# Calculate average scores across all companies for comparisonavg_scores = df_scores[independent_multiples].mean()# Create figure with multiple subplots for each metricfig, axes = plt.subplots(2, 2, figsize=(15, 10))axes = axes.ravel()# Plot each independent multiple as a bar chartfor i, metric in enumerate(independent_multiples): ax = axes[i] x = np.arange(len(top_10_companies[&#39;symbol&#39;])) width = 0.35 # Plot company values ax.bar(x, top_10_companies[metric], width, label=&#39;Company Value&#39;, alpha=0.7) # Plot market average as horizontal line ax.axhline(y=avg_scores[metric], color=&#39;r&#39;, linestyle=&#39;--&#39;, label=f&#39;Market Average ({avg_scores[metric]:.2f})&#39;) ax.set_title(f&#39;{metric} Comparison&#39;) ax.set_xlabel(&#39;Companies&#39;) ax.set_ylabel(&#39;Value&#39;) ax.set_xticks(x) ax.set_xticklabels(top_10_companies[&#39;symbol&#39;], rotation=45) ax.legend()plt.suptitle(&#39;Top 10 Companies vs Market Average - Key Metrics&#39;, fontsize=14)plt.tight_layout()plt.show()# Print numerical comparisoncomparison = pd.DataFrame({ &#39;Market Average&#39;: avg_scores, &#39;Top 10 Average&#39;: top_10_companies[independent_multiples].mean()})print(&quot;\\nNumerical Comparison with Market Averages:&quot;)print(comparison.round(3))저렴한 기업을 무작정 매수하는 것이 아닌 좋은 기업을 낮은 가격에 매수하는 것이 투자 전략의 핵심이다. 따라서 저렴한 자산을 스크리닝 하기에 앞서 위와 같이 펀더멘탈이 양호한 기업들을 먼저 선별하는 것이 매우 중요하다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "[파이썬 퀀트 투자] 시장은 정말 효율적일까? - 통계적 차익거래(Statistical Arbitrage) 백테스팅 시뮬레이션", "url": "/posts/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%ED%80%80%ED%8A%B8-%ED%88%AC%EC%9E%90-%EC%8B%9C%EC%9E%A5%EC%9D%80-%EC%A0%95%EB%A7%90-%ED%9A%A8%EC%9C%A8%EC%A0%81%EC%9D%BC%EA%B9%8C-%ED%86%B5%EA%B3%84%EC%A0%81-%EC%B0%A8%EC%9D%B5%EA%B1%B0%EB%9E%98-Statistical-A/", "categories": "4. 실전, 계량 투자 분석", "tags": "", "date": "2025-04-29 00:00:00 +0900", "snippet": "통계적 차익거래는 움직임이 유사한 둘 이상의 자산에서 괴리가 발생하면 매수 및 매도를 통해 차익을 얻는 계량 투자 전략이다. 예를 들어 금광 채굴 회사의 주가는 금 가격과 상관성이 높을 테니, 금 가격만큼 충분히 주가가 오르지 않으면 매수하고 기다렸다가 적정 수준까지 주가가 따라잡았을 때 청산하는 식이다.그러나 금과 금 채굴회사처럼 이미 잘 알려진 유사 자산에서 알파를 기대하긴 어렵고, 시장에서 잘 알려지지 않은 자산 쌍을 통계적으로 발굴해내야 한다. 보편적인 방법으로 K-means clustering 같은 거리 기반 비지도학습 알고리즘을 사용할 수 있다.먼저 나스닥에 상장한 100개 자산에서 유사한 군집들을 묶고, 각 군집 내에서 상관계수가 가장 높은 쌍에 대해 차익거래 백테스팅을 수행해보자.1. 데이터 수집나스닥 종목 리스트로 구글링 하니 위키피디아가 검색된다.https://en.wikipedia.org/wiki/Nasdaq-1001-1. 웹 크롤링Ticker 목록을 크롤링하는 함수를 하나 작성해 준다.# Get NASDAQ-100 tickers using Wikipediaimport requestsfrom bs4 import BeautifulSoupdef get_nasdaq100_tickers(): url = &quot;https://en.wikipedia.org/wiki/Nasdaq-100&quot; response = requests.get(url) soup = BeautifulSoup(response.text, &#39;html.parser&#39;) # Find the table with Nasdaq-100 components tables = soup.find_all(&#39;table&#39;, {&#39;class&#39;: &#39;wikitable&#39;}) for table in tables: if &#39;Ticker&#39; in str(table): df = pd.read_html(str(table))[0] # Get the ticker column (may need adjustment based on Wikipedia table structure) tickers = df[&#39;Ticker&#39;].tolist() return tickers return []1-2. yfinance함수로 Ticker 목록을 가져온 다음, Ticker를 순회하며 야후 파이낸스 api로 실제 주가 데이터를 수집한다.# Get NASDAQ-100 tickersnasdaq100_tickers = get_nasdaq100_tickers()# Get NASDAQ composite index datanasdaq = yf.Ticker(&quot;^IXIC&quot;)nasdaq_data = nasdaq.history(period=&quot;5y&quot;)# Download historical data for all NASDAQ-100 stocksstock_data = {}print(&quot;Downloading data for NASDAQ-100 stocks...&quot;)for ticker in nasdaq100_tickers: try: stock = yf.Ticker(ticker) data = stock.history(period=&quot;5y&quot;)[&#39;Close&#39;] if not data.empty: stock_data[ticker] = data print(f&quot;Downloaded {ticker}&quot;) except Exception as e: print(f&quot;Error downloading {ticker}: {e}&quot;) prices = pd.DataFrame(stock_data) returns = prices.pct_change() # daily returns2. Clustering(군집 분석)2-1. 최적의 군집 수 찾기주가 데이터를 각 움직임 특성에 따라 N개 그룹으로 구분하고자 한다. N을 지정해야 하는데, 임의로 지정하기보다 최적의 N을 찾는 elbow method를 사용하겠다. elbow method는 각 군집 내 요소들에 대해 속한 군집의 중심으로부터 거리 합을 계산하여 그 값이 최소 혹은 충분히 낮은 수준으로 떨어지는 군집의 수를 찾는 방법론이다.# Normalize prices to start at 100 for fair comparisonfrom sklearn.cluster import KMeansnormalized_prices = complete_prices / complete_prices.iloc[0] * 100# Determine optimal number of clusters using elbow methodinertias = []K = range(1, 11)for k in K: kmeans = KMeans(n_clusters=k, random_state=42) kmeans.fit(normalized_prices.T) # Transpose to cluster stocks, not timestamps inertias.append(kmeans.inertia_)# Plot elbow curveplt.figure(figsize=(12, 6))plt.plot(K, inertias, &#39;bx-&#39;)plt.xlabel(&#39;k&#39;)plt.ylabel(&#39;Inertia&#39;)plt.title(&#39;Elbow Method For Optimal k&#39;)plt.grid(True)plt.show()군집의 수가 늘어날수록 중심으로부터의 거리 합이 작아지는데, 그렇다고 군집을 수치에 따라 무한정 늘리면 군집 내 요소가 하나, 둘 수준으로 과하게 적게 포함되어 그룹으로 묶겠다는 의도를 제대로 반영할 수 없다. 따라서 충분히 낮은 수치를 만드는 구간인 6이 군집의 수로 적당하다.2-2. 유사 자산 그룹 묶기# Choose optimal k (let&#39;s use 4 clusters) and perform final clusteringoptimal_k = 6kmeans = KMeans(n_clusters=optimal_k, random_state=42)clusters = kmeans.fit_predict(normalized_prices.T)# Plot clustersplt.figure(figsize=(15, 8))for i in range(optimal_k): # Get tickers in this cluster cluster_tickers = normalized_prices.columns[clusters == i] # Plot mean trajectory of cluster cluster_mean = normalized_prices[cluster_tickers].mean(axis=1) plt.plot(normalized_prices.index, cluster_mean, linewidth=2, label=f&#39;Cluster {i+1}&#39;) # Print cluster members and count print(f&quot;\\nCluster {i+1} members ({len(cluster_tickers)} stocks):&quot;) print(&#39;, &#39;.join(cluster_tickers))plt.title(&#39;Stock Price Clusters (Common Period Only)&#39;)plt.xlabel(&#39;Date&#39;)plt.ylabel(&#39;Normalized Price&#39;)plt.legend()plt.grid(True)plt.show()# Print summary statistics for each clusterprint(&quot;\\nCluster Summary Statistics:&quot;)for i in range(optimal_k): cluster_tickers = normalized_prices.columns[clusters == i] cluster_returns = returns.loc[valid_dates, cluster_tickers].mean(axis=1) print(f&quot;\\nCluster {i+1}:&quot;) print(f&quot;Average Return: {cluster_returns.mean():.4f}&quot;) print(f&quot;Return Std Dev: {cluster_returns.std():.4f}&quot;)많지 않은 군집 수로 클러스터링을 수행했음에도 자산이 하나씩만 포함된 군집이 3개나 존재한다. 다른 자산들과 묶이기에는 움직임 특성에 뚜렷한 차이가 있는 것이다. 특히 5번 클러스터에 묶인 팔란티어 같은 경우 다른 군집과 다소 상이한 움직임을 보이고 있다.3. 차익거래 백테스팅우리는 각 군집 내에서 상관관계가 높은 쌍을 찾아야 하므로, 클러스터 내 요소 수가 하나인 경우 제외하고 1번, 4번, 6번 클러스터만 분석에 활용한다.selected_clusters = [0, 3, 5]# Dictionary to store resultspair_trading_results = {}for cluster_idx in selected_clusters: print(f&quot;\\nAnalyzing Cluster {cluster_idx + 1}&quot;) # Get tickers in this cluster cluster_tickers = normalized_prices.columns[clusters == cluster_idx] cluster_prices = normalized_prices[cluster_tickers] # Calculate all possible pairs in the cluster pairs = [] correlations = [] for i in range(len(cluster_tickers)): for j in range(i+1, len(cluster_tickers)): stock1, stock2 = cluster_tickers[i], cluster_tickers[j] correlation = cluster_prices[stock1].corr(cluster_prices[stock2]) pairs.append((stock1, stock2)) correlations.append(correlation) # Select the pair with highest correlation best_pair_idx = np.argmax(correlations) stock1, stock2 = pairs[best_pair_idx] correlation = correlations[best_pair_idx] print(f&quot;Best pair: {stock1} - {stock2} (correlation: {correlation:.4f})&quot;) # Calculate spread spread = cluster_prices[stock1] - cluster_prices[stock2] # Calculate z-score of spread z_score = (spread - spread.mean()) / spread.std() # Define trading signals entry_threshold = 2.0 # Enter position when |z-score| &amp;gt; 2 exit_threshold = 0.0 # Exit position when z-score crosses 0 # Initialize position and returns arrays position = np.zeros(len(z_score)) returns = np.zeros(len(z_score)) # Implement trading strategy for i in range(1, len(z_score)): # If no position is open if position[i-1] == 0: if z_score[i] &amp;gt; entry_threshold: position[i] = -1 # Short stock1, long stock2 elif z_score[i] &amp;amp;lt; -entry_threshold: position[i] = 1 # Long stock1, short stock2 # If position is open else: if (position[i-1] == 1 and z_score[i] &amp;amp;gt; exit_threshold) or \\ (position[i-1] == -1 and z_score[i] &amp;lt; exit_threshold): position[i] = 0 # Close position else: position[i] = position[i-1] # Maintain position # Calculate returns if position[i] != 0: stock1_return = (cluster_prices[stock1].iloc[i] / cluster_prices[stock1].iloc[i-1] - 1) stock2_return = (cluster_prices[stock2].iloc[i] / cluster_prices[stock2].iloc[i-1] - 1) returns[i] = position[i] * (stock1_return - stock2_return) # Calculate strategy metrics cumulative_returns = np.cumprod(1 + returns) - 1 sharpe_ratio = np.sqrt(252) * returns.mean() / returns.std() # Store results pair_trading_results[f&quot;Cluster_{cluster_idx + 1}&quot;] = { &#39;pair&#39;: (stock1, stock2), &#39;correlation&#39;: correlation, &#39;cumulative_returns&#39;: cumulative_returns, &#39;sharpe_ratio&#39;: sharpe_ratio } # Plot results plt.figure(figsize=(15, 10)) plt.subplot(2, 1, 1) plt.plot(cluster_prices.index, z_score) plt.axhline(y=entry_threshold, color=&#39;r&#39;, linestyle=&#39;--&#39;) plt.axhline(y=-entry_threshold, color=&#39;r&#39;, linestyle=&#39;--&#39;) plt.axhline(y=0, color=&#39;k&#39;, linestyle=&#39;-&#39;) plt.title(f&#39;Z-Score of Spread ({stock1} - {stock2})&#39;) plt.grid(True) plt.subplot(2, 1, 2) plt.plot(cluster_prices.index, cumulative_returns) plt.title(&#39;Cumulative Returns&#39;) plt.grid(True) plt.tight_layout() plt.show() print(f&quot;Sharpe Ratio: {sharpe_ratio:.2f}&quot;) print(f&quot;Total Return: {cumulative_returns[-1]:.2%}&quot;)# Print overall summaryprint(&quot;\\nOverall Strategy Summary:&quot;)for cluster, results in pair_trading_results.items(): print(f&quot;\\n{cluster}:&quot;) print(f&quot;Pair: {results[&#39;pair&#39;][0]} - {results[&#39;pair&#39;][1]}&quot;) print(f&quot;Correlation: {results[&#39;correlation&#39;]:.4f}&quot;) print(f&quot;Sharpe Ratio: {results[&#39;sharpe_ratio&#39;]:.2f}&quot;) print(f&quot;Total Return: {results[&#39;cumulative_returns&#39;][-1]:.2%}&quot;)각 군집별 최적의 쌍을 찾고, 둘의 스프레드가 2 표준편차를 벗어나면 가격이 높은 자산은 매도하고 낮은 자산은 매수하도록 했다.3-1. Baker Hughes &amp;amp; Cisco첫 번째 군집에서는 BKR과 CSCO가 상관계수 89%로 움직임이 가장 유사했는데, long-short 전략을 수행했을 때 2년이 안 되는 기간 동안 약 48%의 수익률을 보였다. sharpe ratio 역시 1.8로 매우 우수한 수준이다.3-2. Door Dash &amp;amp; Netflix이어서 도어대시와 넷플릭스는 상관계수 96%로 첫 번째 그룹보다 높은 상관성을 보인다. 그러나 상관성이 높다고 차익거래 성과가 더 좋은 것은 아니다.오히려 스프레드 변동성이 낮고, 스프레드 발생 구간이 짧아 트레이딩 효용은 떨어진다. 또, 최근 발생한 스프레드에 대해 수익이 발생하려면 스프레드 내구간으로 재진입해야 하나 아직 그 시점이 오지 않아 손실이 발생하고 있는 것을 감안해야 한다.3-3. GOOG &amp;amp; GOOGL한국으로 치면 보통주, 우선주 개념의 의결권 차이만 있을 뿐 둘은 사실상 같은 구글 주식이다. 당연히 상관계수는 99%로 가장 높고, 여기서 알파는 찾을 수 없었다. sharpe ratio도 낮고 return은 시장 수익률에 미치지 못했다.알려진 시장은 효율적이고, 서두에서 언급했듯 누구나 동일 혹은 유사 자산으로 인지할 수 있는 자산 쌍에 대해서는 차익거래를 통한 수익을 취할 수 없다.이렇게 파이썬으로 간단히 통계적 차익거래를 시뮬레이션해 보았다. 자산 움직임에 따른 유사 군집으로 먼저 타겟 집단을 좁히고, 타겟 집단 내에서 상관계수가 가장 높은 쌍을 찾아 해당 자산 쌍에 대해 long-short 매매를 진행했을 때 스프레드 변동성이 크고 잦은 Baker Hughes &amp;amp; Cisco에서 가장 높은 수익률을 확인할 수 있었다.자동매매가 판치는 자산 시장에서도 찰나 혹은 꽤나 긴 시간 동안 비효율은 발생할 수 있으며, 시장 국면에 따라 이러한 비효율을 발굴하여 상관계수가 높은 자산 쌍을 타겟으로 차익거래를 시도해 볼 수 있다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "2025년 장단기 국채금리 기반 마켓 베타 국면 분석 - 단기 금리 급락시 기저 원인을 포착할 것", "url": "/posts/2025%EB%85%84-%EC%9E%A5%EB%8B%A8%EA%B8%B0-%EA%B5%AD%EC%B1%84%EA%B8%88%EB%A6%AC-%EA%B8%B0%EB%B0%98-%EB%A7%88%EC%BC%93-%EB%B2%A0%ED%83%80-%EA%B5%AD%EB%A9%B4-%EB%B6%84%EC%84%9D-%EB%8B%A8%EA%B8%B0-%EA%B8%88%EB%A6%AC-%EA%B8%89%EB%9D%BD%EC%8B%9C-%EA%B8%B0%EC%A0%80-%EC%9B%90%EC%9D%B8%EC%9D%84-%ED%8F%AC/", "categories": "2. 도메인, 금융", "tags": "", "date": "2025-02-12 00:00:00 +0900", "snippet": "1. 불 vs 베어 불(bull): 황소가 공격할 때 뿔을 들어 올리는 모습을 빗댄. 상승장을 의미 베어(bear): 곰이 공격할 때 앞발을 내리치는 모습을 빗댄, 하락장을 의미. shutterstock2. 스티프닝 vs 플래트닝 스티프닝(steepening): 장기채와 단기채 수익률 괴리가 커지는 현상. 장단기는 주로 동시에 오르내리는데, 오를 때 장기채가 단기채보다 더 많이 오르고, 내릴 때는 더 많이 떨어지는 모습. 플래트닝(flattenin): 장기채와 단기채 수익률 괴리가 작아지는 현상. 오를 때 단기채가 장기채 수준으로, 혹은 그 이상 많이 오르고, 내릴 때는 적게 떨어지는 모습. 3. 불 스티프닝 vs 베어 스티프닝 불 스티프닝(bull steepening): 채권 가격이 “상승”하면서 장단기 수익률 괴리가 커지는 현상. 단기채 가격이 급등하여 수익률은 급락하는 한편, 상대적으로 장기채 가격은 소폭 상승하여 수익률도 소폭 하락. 베어 스티프닝(bear steepening): 채권 가격이 “하락”하면서 장단기 수익률 괴리가 커지는 현상. 장기채 가격이 급락하여 수익률도 급등하는 한편, 상대적으로 단기채 가격은 소폭 하락하여 수익률도 소폭 상승 4. 불 플래트닝 vs 베어 플래트닝 불 플래트닝(bull flattening): 채권 가격이 “상승”하면서 장단기 수익률 괴리는 작아지는 현상. 장기채 수요 및 가격 급등으로 인해 수익률이 급락하고, 단기채 수익률은 소폭 하락. 베어 플래트닝(bear flateening): 채권 가격이 “하락”하면서 장단기 수익률 괴리도 작아지는 현상. 단기채 수요 및 가격 급락으로 인해 수익률이 급등하고, 장기채 수익률은 소폭 상승 5. 마켓 베타 국면 비교 불 스티프닝: 채권 가격 상승(수익률 하락), 단기채 금리 하락률 &amp;gt; 장기채 금리 하락률, 장단기 스프레드 상승, 베타 1.85 불 플래트닝: 채권 가격 상승(수익률 하락), 단기채 금리 하락률 &amp;lt; 장기채 금리 하락률, 장단기 스프레드 하락, 베타 10.87 베어 스티프닝: 채권 가격 하락(수익률 상승), 단기채 금리 상승률 &amp;lt; 장기채 금리 상승률, 장단기 스프레드 상승, 베타 9.5 베어 플래트닝: 채권 가격 하락(수익률 상승), 단기채 금리 상승률 &amp;gt; 장기채 금리 상승률, 장단기 스프레드 하락, 베타 10.31 코로나 이후 채권 금리는 계속 오르고 있고, 장단기 역전 시기도 길었던 만큼 현재 국면은 ‘베어 플래트닝’ 상태(위 차트 흰색 영역)이며,이후에는 역사적으로, 잦은 빈도의 불 플래트닝 -&amp;gt; 불 스티프닝 진입 사이클을 확인할 수 있다.intermarketandmore.finanza.com/장기 금리가 상승/하락을 반복하여 정체되어 있는 만큼, 불 스티프닝으로 진입하는 국면에 선대응해야 하며, “금리 취약 자산들에 대한” 포트폴리오 조정이 필요해보인다. 이는 시장 참여자들이 단기 안전자산으로 자금을 급격히 이동시키는 현상일 수 있고, 유동성이 마르면 주식 시장에는 순간적인 급락이 이어질 것이다.그러나, 미국 연준이 리세션을 매우 심각하게 고려해 단기금리를 큰 폭으로 인하하거나, 또 동시에 정부 정책자금이 단기 국채 매입에 활용되어 국채 가격이 상승한다면 오히려 유동성을 풍부하게 만들 수 있다. 이 경우 할인율도 함께 감소해 기술주들의 미래 기대 수익률이 현재 추정 가치에 잘 반영될 수 있기 때문에 단기 금리의 움직임 기저면을 기민하게 확인할 필요가 있겠다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "달러 통화량과 유동성 비율을 활용한 금, 비트코인 가치평가", "url": "/posts/%EB%8B%AC%EB%9F%AC-%ED%86%B5%ED%99%94%EB%9F%89%EA%B3%BC-%EC%9C%A0%EB%8F%99%EC%84%B1-%EB%B9%84%EC%9C%A8%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-%EA%B8%88-%EB%B9%84%ED%8A%B8%EC%BD%94%EC%9D%B8-%EA%B0%80%EC%B9%98%ED%8F%89%EA%B0%80/", "categories": "4. 실전, 글로벌 매크로 분석", "tags": "", "date": "2025-01-26 00:00:00 +0900", "snippet": "비트코인은 2009년 첫 등장 이후 디지털 자산으로서 독자적인 위치를 구축해왔다. 초기에는 주로 기술 커뮤니티와 소규모 투자자들 사이에서 주목받았지만, 최근에는 글로벌 금융 시스템과 경제 변화에 따라 “디지털 금”이라는 별칭으로 주목받고 있다. 동시에, 중앙은행의 양적 완화 정책, 지속적인 통화 팽창, 그리고 인플레이션에 대한 우려가 커지면서 비트코인 적정 가치에 대한 의문이 제기되고 있다.전통적인 자산과 달리, 비트코인은 현금 흐름, 배당, 혹은 사용 가치를 기준으로 평가하기 어렵다. 다만, 금과 비트코인은 모두 희소성을 기반으로 하는 자산이며, 공급이 제한적이라는 공통점을 가지고 있어 금의 적정가를 산출할 수 있다면 비트코인도 현재 가치를 간접 추정해볼 수 있겠다. 이번 분석에서는 통화량(M0, M1, M2)과 금의 유동성을 기반으로 비트코인의 가치를 추정해보고, 글로벌 유동성과 인플레이션 상황에서의 안전 자산의 가격 움직임을 이해해 볼 것이다.이를 위해 통화량 데이터와 금, 비트코인 관련 데이터를 수집하고 이를 바탕으로 비트코인의 가치를 경제적 맥락에서 분석한다. 금과 비트코인의 가격 및 유동성 추이를 비교하며, 비트코인의 시가총액을 금과 동일한 수준으로 가정했을 때의 적정 가격을 추정함으로써 현시점의 비트코인 가격 움직임을 해석해본다.Step 1. 데이터 수집 및 전처리 1-1. FRED API를 통한 통화량 데이터 추출통화량 데이터를 수집하기 위해 Federal Reserve Economic Data(FRED) API를 사용하여 M0, M1, M2 데이터를 확보했다. M0는 물리적 통화(현금), M1은 현금+수표 계좌, M2는 예금 및 기타 금융 상품을 포함한 통화 지표로, 각각의 통화량은 경제의 유동성을 이해하는 데 핵심적인 역할을 한다. 1-2. CoinMarketCap 웹사이트에서 비트코인 데이터 수집비트코인의 가격과 공급량은 CoinMarketCap 웹사이트에서 스크래핑하였다. 비트코인의 유통량은 블록체인 네트워크의 총 발행량과 직접적으로 연결되며, 이는 비트코인의 희소성과 가치 평가의 주요 변수로 작용한다. 1-3. 금 가격 및 채굴량 데이터 수집 및 통합World Gold Council과 Our World in Data 웹사이트에서 금의 가격 및 채굴량 데이터를 수집하여, 비트코인과 금의 유통량과 가격을 비교할 수 있는 데이터셋으로 구성한다. 금의 채굴량은 공급의 유한성을 반영하며, 이는 비트코인과 유사한 가치 평가 구조를 가진다.World Gold Council - 2010년 이후 채굴량, 가격 데이터Our World in Data - 1681년부터의 금 생산량 데이터데이터 통합 - 전기간 market cap 계산위 작업을 통해 2024년 3분기까지의 금 가격 및 시총 데이터를 모두 추출했다. 참고로, 25.1 기준 1t은 35274oz이므로 연간 채굴량 약 3600t(분기별 900t)을 oz로 환산하면 연간 1,269,842,400oz, 분기별 31,736,600oz가 된다. 작성 시점(25.1.3) 기준 금은 온스(oz) 당 2,634달러, 총 금 채굴량은 201,296t이며 온스로 환산하면 7,100,515,104oz다. 이는 시총으로 18,702,756,783,936달러(18 trillion)다.Step 2. 연도별 달러 통화, 금, 비트코인 유동성 증가 추이 2-1. 통화량(M0, M1, M2) 및 통화승수의 추이M0, M1, M2 통화량의 추이를 시계열로 분석한 결과, 2020년 코로나 팬데믹 이후 급격한 증가가 관찰되었다. 특히 M2는 정부의 양적 완화 정책과 재정 확대의 영향을 크게 받았다. 통화승수(Money Multiplier)는 시중은행의 대출 증가가 유동성 확대에 어떻게 기여했는지를 보여준다.참고) 우리나라의 경우 1979년까지 주로 M1을 기준으로 통화량을 관리했다. 이후 경제 규모가 커지고 다양한 금융기관이 출현하면서 M1이 실물경제의 동향을 제대로 반영하지 못한다고 판단돼 M2를 기준 통화지표로 사용하고 있다. M1은 측정이 정확하고 편리한 장점이 있지만, 실물경제의 규모를 정확히 반영하지 못하는 문제가 발생해 우리나라를 비롯한 많은 나라에서 M2를 기준 통화지표로 사용하고 있다. 최근에는 금융의 발전으로 가상화폐를 비롯해 금융상품이 더 다양해지고 있다. 그뿐 아니라 금융시장의 구조도 급격히 변해가는 상황에서 M2도 실물경제와 괴리가 많이 발생하고 있어 한층 더 포괄적인 통화지표를 사용해야 한다는 주장이 제기되고 있다.2020년 대규모 양적 완화 이후 m0대비 m1 통화승수는 증가했으나 m0 대비 m2 통화승수는 그렇지 않다. 돈을 풀어서 은행에 단기예금으로 많이 유입되었으나, 장기 예금이나 적금 등 은행 예치 상품으로 묶기보다 비은행 금융 자산으로 이동한 것이다. M1 승수의 급격한 증가는 사람들이 현금을 덜 보유하고 요구불예금과 같은 단기 거래 수단을 선호하고 있음을 나타내며, 이는 거래와 소비가 활발해지고 있음을 반영한다. 한편 M2 승수의 감소 또는 정체는 자금이 은행 중심의 장기 예금에서 벗어나 비은행 금융 자산으로 이동하고 있다는 신호로, 이는 금융 시장의 성장과 동시에 전통적인 은행 시스템이 축소되고 있음을 의미한다.이러한 양상을 고려할 때 금, 비트코인 가치평가를 위해 m2를 사용하는 것이 적절하다. 유동성이 다소 떨어지는, 저축성 예금을 포함하는 광의통화(m2)가 실질 통화량을 반영하기 때문이다. 아래 차트에서 2020년 이후 m2에서 m1을 제외한, 유동성이 낮은 저축성 예금 잔고가 급격히 떨어지는 모습을 볼 수 있다. 저축성 예금을 시중 통화량에서 제외하는 것은 분석상의 비약이 클 수 있다. 2-2. 금과 비트코인의 유통량 변화2010년 이후 금과 비트코인의 유통량은 정량적으로 비교될 수 있는 수준으로 가고 있는데, 금은 이미 채굴 수익이 비용 임계점에 도달하여 연간 생산량이 매우 일정한 상태이고 비트코인 역시 21백만 개라는 공급 한계에 접근하고 있다.Step 3. 유동성 비율을 통한 금, 비트코인 가격 추정 3-1. 유동성 비율 기반 추정 가능성금의 가격은 통화량(M2) 대비 금의 유동성(Gold Liquidity)으로 대변될 수 있다는 가정 하에, 두 변수의 비율을 분석했다. 금의 유동성 대비 M2의 증가율은 금의 가치 평가에 유의미한 설명력을 제공하며, 금 가격의 상승이 전반적인 통화 팽창의 결과임을 보여준다. 3-2. 상관 분석 및 회귀 모형 검증금 가격을 M2 증가율과 금 유동성 증가율의 비율로 회귀 분석한 결과, 결정계수(R²)가 0.49로 나타났다. 이는 단일 변수로서 금 가격을 설명하기에 비교적 높은 설명력을 가진다. 이러한 결과는 금의 가격이 글로벌 유동성 확대와 밀접히 연결되어 있음을 시사한다.위 차트에서  잔차들이 x축에 대해 무작위로 분포해야 이상적인 회귀 모델이지만  여기서는 잔차가 특정 패턴(곡선 형태)을 따르는 것처럼 보이며,  이는 회귀 모델이 데이터의 비선형성을 잘 설명하지 못하고 있다는 신호일 수 있다.  하지만 본 회귀 분석의 목적이 예측이 아니므로 정상성을 엄밀히 확보하기보다 단일 변수에 대한 설명력을 파악하는 데에 제한적으로 활용할 필요가 있다.참고로, 위 회귀모형을 활용해서 추정값을 구한다면 오차를 이와 같은 수준(2번 행 &amp;gt; 1번 행)으로 줄일 수 있다.Step 4. 금과 비트코인 비교 분석을 통한 비트코인 가치 추정 4-1. 비트코인의 유동성 비율 분석비트코인의 가격 역시 M2 증가량 대비 비트코인 공급량 증가율의 비율로 추정했지만, 결과는 금과 달리 큰 괴리를 보였다. 특히 2020년 이후 비트코인의 가격 급등은 단순한 유동성 비율로 설명되지 않을 정도로 이례적이었다. 4-2. 금과 비트코인의 가격 상승률 비교2020년을 기준으로 금은 약 1.4배로 증가한 반면, 비트코인은 약 10배로 증가했다. 이는 비트코인의 가격 상승이 금보다 약 8.13배 더 빠른 속도로 진행되었음을 보여준다. 이러한 차이는 비트코인이 희소성과 투기적 매수세로 인해 가격이 과도하게 상승했을 가능성을 암시한다.한편 두 가격 상승률의 움직임은 유사한 수준(correlation 0.7)으로 달러 유동성에 반응하는 자산임을 확인할 수 있다. 4-3. 금과 비트코인의 시가총액 비교금의 시가총액은 비트코인 대비 여전히 압도적으로 크지만, 비트코인은 2020년 이후 금의 약 10% 수준에 도달했다. 그러나 이러한 비율은 꾸준히 증가하는 추세라기보다는 변동성이 크며, 투자자들의 비트코인에 대한 신뢰와 시장의 유동성 상태에 크게 의존하고 있다. 4-4. 금 시가총액 기반 비트코인 가치 재추정비트코인이 금과 동일한 시가총액을 가진다고 가정하면, 현재 비트코인의 가격은 금 시가총액 대비 약 8.13배의 조정을 통해 추정될 수 있다. 이 추정을 통해 계산한 비트코인의 가치는 현재 시장 가격과 유사한 수준으로 나타났다. 이는 시장이 비트코인을 금과 유사한 달러 헷지 자산으로 평가하고 있음을 시사한다.단순 유동성 비(대체자산 유동성 / 달러 유동성)로 보면 금은 2010년 이후 1.8배, 비트코인은 2020년 이후 1.2배까지 올라왔다. 1.2배는 2020년 이후 비트코인 가격 상승률에 비하면 매우 낮은 수준으로, 유동성 비로는 가격 상승에 대한 설명력이 충분하지 않다. 이 때, 비트코인과 금이 결국 동일한 대체자산으로, 동일한 시가총액을 형성할 것이다 라는 가정을 가지면 시가총액 비(금 시총 대비 비트코인 시총)는 곧 시장의 “비트코인 저평가 심리”를 정량화한 수치가 된다. 따라서 이 수치의 역수 8.13은 비트코인 가격을 형성하는 가중치가 될 수 있으며 위 계산에 활용하였다.Step 5. 결론비트코인의 가치는 전통적인 자산인 금과 같이 제한된 공급량과 희소성이라는 측면에서 달러 대체자산으로 인식되고 있으며, 그에 따른 평가 차이로 인한 높은 변동성으로 가치 평가가 매우 복잡하다. 이에 본 분석은 통화량과 유동성 비율을 기반으로 비트코인의 가치 추정을 시도해보았고, 금과 비트코인의 시가총액 비로 시장 평가 심리를 가중치로 활용해 추정 가치를 보정해보았다. 결과적으로 현재 가격은 “금과 비트코인 시가총액 비는 결국 1:1로 수렴할 것이다”라는 가정 하에 단기적으로는 적정가(심리만 반영한 가격상승률), 장기적으로는 저평가 상태(0.1:1) 임을 알 수 있다.그러나 비트코인의 높은 변동성과 투기적 특성은 여전히 주의가 필요한 요소로 남아 있고, 정교한 평가를 위해 통화 정책 및 다른 주요 투자 자산(채권, 주식, 비트코인 파생상품) 등 다양한 요인을 통합한 다각적 분석이 필요하겠다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "[파이썬 금융 데이터 분석] 미국 부채사이클 기반 2025년 리세션 리스크 평가", "url": "/posts/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EA%B8%88%EC%9C%B5-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%B6%84%EC%84%9D-%EB%AF%B8%EA%B5%AD-%EB%B6%80%EC%B1%84%EC%82%AC%EC%9D%B4%ED%81%B4-%EA%B8%B0%EB%B0%98-2025%EB%85%84-%EB%A6%AC%EC%84%B8%EC%85%98-%EB%A6%AC%EC%8A%A4%ED%81%AC-%ED%8F%89%EA%B0%80/", "categories": "4. 실전, 글로벌 매크로 분석", "tags": "", "date": "2025-01-12 00:00:00 +0900", "snippet": "2025년 세계 경제는 중요한 기로에 서 있다. 국가 경제의 핵심 지표 중 하나는 바로 부채 사이클이다. 미국의 부채 수준, 이자 지급액, 그리고 통화량(M0, M1, M2)을 분석함으로써 경기 침체 리스크를 이해하고, 이에 따라 적절한 경제 전략을 수립할 수 있다.본 글에서는 미국 부채 사이클을 기반으로 현 시점 경제 상황을 분석하고, 통화량을 고려해 2025년 경기 침체 리스크를 평가한다.Step 1. 데이터 수집부채 사이클과 경기 침체 리스크 간의 관계를 이해하기 위해 Federal Reserve Economic Data(FRED) API에서 데이터를 수집한다. 다음과 같은 변수를 포함하도록 한다. 이자 지급액(Interest Payments) 국내총생산(GDP) 연방 부채(Federal Debt) 통화기반(M0) 통화 공급(M1, M2)추가로, 다음과 같은 파생 변수를 생성하여 경제 생산량과 부채 수준의 관계를 분석한다. GDP 대비 이자 지급액 비율(Interest Payments to GDP): 경제 대비 이자 부담 수준 측정. GDP 대비 부채 비율(Debt to GDP): 경제 생산량에 대한 연방 부채의 비율.Step 2. 데이터 전처리 및 성장률 시각화먼저 모든 변수의 데이터 기간을 일치시키고 각 변수의 성장률을 계산하여 시간에 따른 변화율을 분석한다. 좌측 열: 각 지표(GDP, 부채, 이자 지급 등)의 시계열 데이터를 보여준다. 이 차트로 부채와 GDP 간의 관계나 이자 지급 수준의 변화 등 경제 전반의 트렌드를 확인할 수 있다. 우측 열: 각 지표의 성장률 기반으로 저점, 고점을 확인하며 경제 확장기, 불황기를 정성적으로 평가해본다. 예를 들어, 부채 성장률의 급격한 상승은 대표적인 경제 발작 징후다.부채가 급증할 때 경제 성장률은 점차 둔화되는 경향을 보인다. 부채가 과중해지면 기업과 가계의 상환 부담이 커져 소비와 투자가 위축되고 경제 성장이 어려워진다는 경제 이론을 뒷받침하고 있다.또, 불황이 시작되기 전에 부채는 빠르게 증가하는 경향이 있다. 부채가 과도하게 증가하면 이자 지급 부담이 커지는데, 금리 상승 시기에 경제 전반에 부담을 주어 불황을 초래할 가능성이 커지는 것이다.Step 3. 부채 사이클 변화ChatGPT를 활용하여 각 경제 주기 정보를 획득한다. 생성형 AI를 사용할 때에는 명확한 출처와 근거를 수령하여 정보의 사실 여부를 항상 확인해야 한다.부채 주기를 분석하기 위해 각 경제 주기를 확장기(1), 중립기(0), 불황기(-1) label로 구분하고, 이렇게 구분한 label을 활용해 성장률 데이터를 시각화한다. 차트를 보면 부채가 급격히 증가하는 시점이 불황기와 대게 겹치는 모습을 확인할 수 있다.위 차트를 통해 각 사이클별로 아래와 같이 해석해볼 수 있겠다. 확장기: 부채는 상대적으로 안정적인 수준에서 증가하고, 경제 성장률도 긍정적인 모습을 보인다. 이 시점에서는 부채가 경제 성장을 촉진하는 역할을 할 수 있다. 불황기: 불황이 시작되면 부채 수준이 급격히 증가하는 경향이 있으며, 경제 성장률은 둔화된다. 부채가 경제 성장을 저해하는 요소로 작용하며, 불황의 심화로 이어진다. 중립기: 중립기에는 부채와 경제 성장률이 일정한 수준에서 균형을 이루는 경향이 있으며, 경제는 안정적인 상태를 유지한다.Step 4. 부채 사이클별 성장률 분포 통계 분석앞에서 부채 사이클을 펼쳐놓고 봤다면, 이번에는 각 부채사이클마다의 성장률 분포를 확인해보고, 각 지표가 사이클 단계(확장기, 불황기, 중립기)마다 어떤 분포적 특징을 가지는지 파악한다. cycle: 부채 사이클 단계 (-1, 0, 1 등) gdp_growthrate: GDP 성장률 dept_growthrate: 부채 증가율 dept_to_gdp_growthrate: GDP 대비 부채 성장률 interest_payments_growthrate: 이자비용 증가율 interest_payments_to_gdp_growthrate: GDP 대비 이자비용 성장률 박스플롯 시각화: 부채 사이클에 따른 주요 변수들의 분포를 시각적으로 확인하기 위해 박스플롯을 생성했다. sns.boxplot을 사용하여 각 변수의 중앙값, 사분위 범위, 그리고 이상치(outliers)를 비교할 수 있다. 이를 통해 각 변수의 변동성과 분포 형태가 부채 사이클 단계에 따라 어떻게 달라지는지 파악한다. 통계 값 요약 및 히트맵 시각화: 데이터프레임의 .describe() 메서드를 활용하여, 각 변수에 대해 평균(mean), 표준편차(std), 최소값(min), 최대값(max), 중앙값(50%) 등 주요 통계 값을 계산하였다. 계산된 통계 값은 사이클 단계별로 그룹화하여 요약하였으며, 이를 히트맵 형태로 시각화하였다. 히트맵은 변수 간 통계 값의 상대적인 크기와 패턴을 한눈에 파악할 수 있도록 도와준다. 불황기(-1)에 분산이 크고, 특히 gdp 성장률이 매우 저조한 것을 볼 수 있다. gdp 대비 부채성장률은 다른 시기에 비해 급격히 높아 이자성장률을 웃도는 현상을 보인다.부채 사이클 단계에 따라 GDP 성장률과 부채 증가율의 분포가 뚜렷하게 달라짐을 확인할 수 있었다. 예를 들어, 호황기(1)에서는 GDP 성장률이 비교적 안정적인 분포를 가지며 중앙값이 높은 반면, 불황기(-1)에서는 GDP 성장률이 더 큰 변동성을 보였다.한편, 표준편차(std) 히트맵에서는 불황기에서 모든 변수의 변동성이 증가하는 패턴이 관찰되었으며, 동일한 결과를 아래와 같이 확인해 볼 수도 있다.Step 5. 과거 데이터 기반 2025년 리세션 리스크 평가여러 경제 건전성 지표가 있지만, gdp 성장률, dept 성장률이 특히 부채사이클 특성을 뚜렷하게 대변하고 있음을 확인했다. 그렇다면 두 지표에 대한 최근(2023~2024) 움직임을 확인하고, 이러한 움직임이 리세션 초입의 동일 분포 움직임과 유사한가 평가해보도록 하자.평가를 위해 각 불황기 직전 3개월에 해당하는 일자 정보를 추출한다.해당 시기의 gdp 성장률, dept 성장률 평균 및 표준편차 데이터를 구하고, 평가 시기(2023~2024)의 동일 지표와 비교한다.부채사이클 불황기 직전 시기와 분포 통계가 굉장히 유사하다. 비교를 위해 확장기 직전 시기도 동일하게 평가해보도록 한다.현시점은 확장기 초입보다 불황기 초입과 분포 통계가 확연히 유사함을 알 수 있다.Step 6. 통화량 고려 부채 평가 및 결론좌측 차트에서 부채는 지속적으로 증가해왔고, 2008년, 2020년에 급증했던 것을 볼 수 있다. 이에 앞서 확인한 gdp 성장률을 고려할 때 현시점은 부채사이클 중 불황기 초입에 도래했음을 짐작할 수 있다.물론, 위 해석에는 과거 통계치를 기준으로 현재를 평가하는 것에는 외생 변수가 동일하다는 가정이 내포되어 있다. 기축통화국인 미국에서 특히 고려해 볼 법한 주요 외생 변수는 통화공급량인데, 이 공급량이 늘어나면 화폐가치가 하락하고 기발행된 부채를 희석시키는 효과가 발생하기 때문이다.우측 차트를 보면 각 통화지표(m0: 본원통화, m1:협의통화, m2:광의통화) 대비 부채 수준을 보여주는데, 통화 발생량 대비 부채 수준은 되려 감소했다. 이렇게 2008년을 기점으로 시중의 통화 공급 상황이 완전히 달라졌으므로 현재의 부채 수준이 정말 상대적으로 과대한가? 불황기 초입으로 볼 수 있는가?라는 질문에 100% 확신을 가지는 것은 위험하다.그러나 화폐가치는 상대적이다. 타 국가 대비(특히 유럽) 미국의 양적완화 수준은 여전히 낮은 수준이며, 위 차트에서 가시적으로 보이는 것처럼 부채에 내재된 화폐가치가 급격히 떨어졌다고 판단하긴 어렵다. 따라서 불황기로 진입할 가능성이 결코 낮지 않으며 이에 진중하게 대비할 필요가 있겠다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "미 장단기 국채 금리차 기반 경기 침체 예측 분석(ARIMA + Probit)", "url": "/posts/%EB%AF%B8-%EC%9E%A5%EB%8B%A8%EA%B8%B0-%EA%B5%AD%EC%B1%84-%EA%B8%88%EB%A6%AC%EC%B0%A8-%EA%B8%B0%EB%B0%98-%EA%B2%BD%EA%B8%B0-%EC%B9%A8%EC%B2%B4-%EC%98%88%EC%B8%A1-%EB%B6%84%EC%84%9D-ARIMA-Probit/", "categories": "2. 도메인, 금융", "tags": "", "date": "2024-12-29 00:00:00 +0900", "snippet": "본 글에서는 ARIMA 모형을 활용해 미래 미국채 Spread 추이를 continuous하게 예측하고, 이어서 Probit 모형을 통해 ARIMA 예측값을 경기침체 여부로 변환한다. 경제 예측 모델링을 수행할 때에는 가급적 단순한(파라미터가 적은) 모델, 해석가능한 모델을 사용하는 것이 좋다. 일반적인 인식과 달리 경제 데이터는 딥러닝과 같은 모델 아키텍처로 적합시킬 만큼 대용량이 아닐 뿐더러 제대로 적합되지 않았을 때 Fat Tail Risk가 미치는 영향이 치명적일 수 있기 때문이다.따라서 시계열 모델 파라미터를 분리 해석할 수 있는 ARIMA 모형, 종속변수가 binary(이진) 변수일 때 yes or no 에 대한 발생 확률을 가장 단순하게 설명가능한 Probit 모형을 사용하고, 이를 통해 현재의 미국채 Spread로 미래 경기침체 향방을 추정 시도해보려 한다.Step 1. 데이터 수집 및 전처리먼저 FRED API에서 미국 장단기 금리(10년물, 2년물) 데이터, 그리고 Recession(경기침체) Indicator 데이터를 불러온다. Recession Indicator Ticker는 USREC이며, 아래 이미지와 같이 지난 연도별 경기침체 여부를 1 or 0으로 기록하고 있다.https://fred.stlouisfed.org/series/USRECARIMA 모형으로 금리차를 예측할 것이므로 모델 입력값으로 넣을 spread를 계산한다. 계산한 장단기 금리 spread와 역사적 recession 시계열은 아래 차트에서 확인할 수 있다. (연도별 recession 여부는 red-flag로 표시해 두었다.)스프레드 역전 시기 직후에는 항상 불황이 이어졌고, 다가올 2025년에도 동일한 상황이 반복될 수 있다.Step 2. 금리 스프레드 시계열 패턴 분석ARIMA 모델링을 수행하기 위해서 시계열 인자(입력 변수)는 정상성(Stationarity)을 만족해야 한다. 정상성은 시점에 따라 분포 통계(평균, 분산)가 변하지 않는 상태를 의미하며, 이것이 변한다면 분포의 영속성을 전제로 하는 통계 분석은 불가하다.정상성 검정(ADF Test) 결과 p-value가 보편적인 significance level(유의 수준) 보다 낮다. 이에 “단위근이 존재한다”라는 ADF Test 귀무가설을 기각할 수 있다. 단위근이 존재하지 않으면 정상시계열일 가능성이 높지만 항상 그런 것은 아니다. ADF Test는 전체 시계열의 통계적 특성을 검사하므로 단위근으로 인한 단기적인, 약한 자기상관성은 발견해 내기 어려울 수 있다.따라서 위와 같이 자기상관성, 부분자기상관성을 차트로 확인해 볼 필요가 있다. 그 결과 Spread의 시차별 Correlation을 보여주는 첫 번째 행에서 직전연도의 스프레드가 당해연도 스프레드와 매우 높은 상관성이 존재함을 알 수 있었다.이렇게 자기상관성이 존재하는 시계열은 시차에 따라 통계적 분포를 달리하므로 예측 모형의 추론을 어렵게 만든다. 이 경우 2가지 선택지가 존재한다. 차분을 수행한 다음 정상시계열로 만들고 예측하거나, 자기상관성을 고려한 모델링을 수행하는 것이다.Step 3. ARIMA 기반 금리 스프레드 예측ARIMA 모형은 AR(p)+I(d)+MA(q)로 구성되고, p, d, q는 각각 자기상관성이 존재하는 차수, 모델 적합을 위해 필요한 차분 수, 오차연관성이 존재하는 차수를 의미한다. 각 인자를 유효 범위 내 다르게 설정하여 적합한 ARIMA 모델의 aic, bic 지표를 보면 앞서 언급한 선택지 2가지, 그러니까 차분을 수행한 다음 예측(p=0, d=1, q=0) 하거나 자기상관성을 고려한 모델링을 수행(p=1, d=0, q=0) 한 결과 trade-off가 존재함을 확인할 수 있다. aic(akaike information criterion)와 bic(bayesian information criterion)는 모델이 잘 적합했는가를 보여주며, 수치가 낮을수록 좋다.다만, 모델 파라미터(p+q+intercept)를 과하게 쓰면 aic는 낮아질지라도 bic는 그렇지 않은데, bic는 과최적화 가능성을 마이너스 요소로 취급해 주기 때문이다. 따라서 두 지표(aic, bic) 사이 밸런스를 같이 보면서 ‘최소한의 파라미터로 최대한의 적합을 이루었는가?’를 평가해야 한다.model_010의 경우 파라미터를 사용하지 않았고(*차분은 모델 파라미터로 취급하지 않는다), model_100은 1개의 파라미터(q=1)를 사용했다. 1차 자기상관성을 고려한 모델 적합을 수행하였고, 파라미터를 사용하지 않은 model_010 대비 bic는 높지만 전체 모델 중 aic가 가장 낮다. 잘 적합된 것이다.여기서, model_010을 선택하지 않는 이유는 model_100보다 높은 aic 수치도 있지만 차분을 수행함으로 인해 정보손실이 발생하는 것을 피하려는 의도도 있다. 우리는 경제 예측 모형을 만들고 있고, 금리 스프레드의 경우 과거(전월, 전년도)가 현재(당월, 당해연도) 스프레드에 영향을 주는 추세적 움직임이 반드시 존재하기 때문에 이러한 움직임을 모델에 녹여내는 것이 합리적이다. 또한, 앞서 정상성 검정을 수행했을 때 자기상관성이 존재하긴 하나 전체 기간에 대해 비정상 시계열로 보기도 어렵기 때문에 일정 범위 내 통계분포가 유지되는 특성 역시 고려해 굳이 차분을 수행하지 않겠다.위 결론을 토대로 ARIMA(1,0,0) 모형에 과거 데이터를 적합시키고, 미래 12개월 금리 스프레드를 예측하였다. 신뢰구간이 넓긴 하나 전통 계량통계 모형의 한계로 인식하고, 이 범위 내 개별 시나리오를 구상해야 한다. 모형은 현실을 굉장히 축소한 형태이므로 경제 이론이나 현실 사건에 근거하지 않은, 단순히 모델의 추정치에만 의지하는 것으로는 효용을 얻기 어렵다.Step 4. Probit 기반 경기침체 확률 예측이제 ARIMA로 예측한 미래 12개월 금리 데이터를 다시 Probit 모형에 적합시키고, recession 여부를 예측해보려 한다. 그전에 Probit 모형이 spread 단일 변수만으로 recession에 대한 유효한 예측 효용을 가질 수 있는가?를 확인했다.위 Probit Regression Results에서 coefficient table을 보면 Spread 변수의 coefficient가 -0.08로, Spread가 증가하면 recession 확률이 낮아질 것으로 예상할 수 있다. 그러나 해당 변수의 P-value가 0.371로, -0.08이라는 계수는 통계적으로 유의하지 않다. 따라서 spread 단일 변수로 recession을 예측하기에 Probit 모형이 충분히 적절하진 않은 것으로 보인다.모형에서 Spread 변수의 계수는 음수이므로, 입력-출력 간에 반비례 경향성을 보인다. ARIMA(1,0,0)는 2025년 1월부터 해당 연도 연말까지 스프레드는 점차 증가할 것으로 예측했고, 이에 따라 Probit 회귀 모형은 Recession 리스크가 감소할 것으로 예측했다.또, 모형 추정 결과로 볼 때 Recession 리스크 자체가 아주 높은 수준은 아닌데, 과거 시계열에서 recession 직전 장단기 스프레드 역전이 항상 발생했던 것을 고려하면 모델의 추정치를 온전히 신뢰하기는 어렵다. (*Probit 모형은 내부적으로 z-score로 1차 추론 결과를 얻고 이를 CDF 함수를 사용해 확률값으로 변환해 준다. 2025년 1월부터 11월까지 11% 수준의 불황 리스크가 존재한다고 해석할 수 있다.)Step 5. 결론 / 인사이트장단기 금리차는 역사적으로 유효한 경기 선행 지표로 활용될 수 있다. 이에 ARIMA 모형으로 미래 금리차를 1차 예측하고 예측된 금리차를 Probit 모델을 통해 경기침체 확률로 2차 예측했다.Probit 모형으로 과거 금리 스프레드와 경기 불황 지표를 적합시켰을 때 두 지표는 서로 약한 반비례 관계를 갖고 있으며 2025년 금리 스프레드가 높아질 것이라고 예측한 ARIMA 모형 추정 결과에 따라 Probit 모형은 경기 불황 가능성이 점차 낮아질 것으로 예측하고 있다.그러나 모델 추정 결과에 대한 통계적 유의성이 낮은 수준이므로 이러한 모델링 방법론을 투자에 직접 활용하기보다 경제적 근거와 추가 정량적 지표를 활용하여 의사결정 보조 수단으로 참고하는 정도가 적절하겠다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "파이썬 금융 데이터 분석 - 디플레이션 국면의 나스닥 환헤지 전략 평가", "url": "/posts/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EA%B8%88%EC%9C%B5-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%B6%84%EC%84%9D-%EB%94%94%ED%94%8C%EB%A0%88%EC%9D%B4%EC%85%98-%EA%B5%AD%EB%A9%B4%EC%9D%98-%EB%82%98%EC%8A%A4%EB%8B%A5-%ED%99%98%ED%97%A4%EC%A7%80-%EC%A0%84%EB%9E%B5-%ED%8F%89%EA%B0%80/", "categories": "1. 기술, 머신러닝, 딥러닝", "tags": "", "date": "2024-12-28 00:00:00 +0900", "snippet": "현시점에서 Nasdaq 100, 환헤지 Nasdaq 100 중 어느 쪽에 베팅하는 것이 유리할까?Step 1. 데이터 수집우선, 전략 평가를 위한 데이터를 수집한다. Nasdaq 100 H, UH 지수: 각각 환율 헷지를 적용한 경우, 그렇지 않은 경우 자산 가격 변화를 비교하기 위함이다. 원/달러 환율: 헷지, 언헷지 간 성과 차이를 분석하기 위해 환율 데이터를 불러온다. 금 가격: 경제 불확실성과 안전자산으로서의 달러 수요를 함께 이해하기 위해 사용할 것이다. 금과 달러 관계를 살펴보면서 현재 환율 상승이 경기 불황에 기인한 것인지 파악할 수 있다Step 2. 데이터 전처리 및 로그 변환자산 가격의 시계열 데이터는 지수적 증감이 있기 때문에 이를 직접 비교할 경우 시각적 착시가 발생할 수 있다. 따라서 로그 변환을 통해 증가율(%) 해석이 가능하도록 전처리한다.위 subplots에서 두 번째 행 2개 차트가 각각 로그 변환된 nasdaq100 H, UH 그리고 원달러 환율을 보여주고 있다.그리고 예상할 수 있듯이, 달러 헤지 하지 않은 nasdaq100과 헤지 된 nasdaq100의 가격 갭은 환율 상승분과 거의 동일하다.환언헤지 자산의 수익률은 자산의 기초 수익율 + 환율 상승률과 같기 때문에 원/달러 환율 상승이 달러 언헤지 자산의 성과를 직접적으로 견인하게 된다. 따라서 환율 상승효과를 보려면 헤지 자산이 아닌 언헤지 자산을 매수해야 한다.Step 3. 현재 환율 위치 평가2024년 12월 기점 원달러 환율은 지난 20년 대비 평균 + 2 표준편차를 초과한 높은 수준에 위치해 있다. 이는 2008년 이후로 가장 높은 상태인데, 현재 환율 변동은 당시와 같은 글로벌 금융위기가 아닌 국내 정치적 이슈에 상당 부분 기인함을 고려할 때 중단기적 하방 압력이 강할 수 있다.Step 4. 금 가격을 통한 달러 강세 요인 분석한편, 국내 정치 이슈를 논외로 하고 현재의 달러 강세가 경기 사이클의 일시적 효과인지, 혹은 구조적 변화에 따른 것인지를 금 가격과의 상관관계를 통해 확인할 수 있다. 이를 위해 20년치 금 가격 지수를 불러와 원/달러 환율과의 연도별 상관계수를 추출한다.금이 달러 가치 하락을 헤지하는 수단으로 활용될 수 있기 때문에 원/달러 환율과 금 가격은 디커플링 되는 경향이 있는데, 역사적으로 드물게 코로나19와 같은 경기불황 심리가 강하게 발생하는 사건에서 커플링 되기도 한다. 위 차트에서 금 가격과 원/달러 환율이 양의 상관관계를 보이는 경우 파란색, 반대의 경우 빨간색으로 표시하고, 상관계수 절댓값이 1에 가까울수록 투명도가 낮게 설정했다. 2005년 이후 둘 사이 상관계수는 2019년이 0.7로 가장 높았고, 그다음이 2024년 0.54 수준이다.금이 절대적 안전자산이라 하면, 달러는 타 국가 화폐 대비 상대적 안전자산이다. 따라서 코로나19 시기에 경제 불황 우려로 안전자산 수요가 급증하여 두 자산은 동시에 가격이 상승하는데 2024년 역시 유사한 현상으로 풀이된다.한편, 2008년 금융위기 당시에는 오히려 디커플링이 유지되었는데, 금융 위기의 경우 자산 가격 하락으로 인한 대규모 마진콜로 금 매도 압력이 발생했고, 동시에 안전자산 수요에 의한 달러 강세와 금 약세가 공존했기 때문이다. 실질적인 자산 가격 하락이 발생하지 않는다면 단순 경기 불황에 대한 우려 심리가 두 자산의 커플링을 유발하고 이러한 커플링은 역사적으로 길게 지속되지 않았다. 종합하면, 현재 원/달러 환율은 역사적으로 매우 고점에 있고 경기 불황 우려에 따른 금 가격과의 커플링 시기임을 고려할 때 환헤지 자산에 투자하여 원/달러 환율 하락으로 인한 자산 수익률 감소를 방어하는 편이 유리하겠다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "미국 근원 소비자물가지수(Core Consumer Price Index) 변화율 추정 - Python 활용", "url": "/posts/%EB%AF%B8%EA%B5%AD-%EA%B7%BC%EC%9B%90-%EC%86%8C%EB%B9%84%EC%9E%90%EB%AC%BC%EA%B0%80%EC%A7%80%EC%88%98-Core-Consumer-Price-Index-%EB%B3%80%ED%99%94%EC%9C%A8-%EC%B6%94%EC%A0%95/", "categories": "1. 기술, 머신러닝, 딥러닝", "tags": "", "date": "2023-12-05 00:00:00 +0900", "snippet": "1. Core CPI 개요 및 BIS API 사용법근원 소비자물가지수는 기존 소비자물가지수(CPI)에서 외부 공급 충격 요인이 될 수 있는 식품과 에너지 부문을 제외한 지수다. 미국의 월별 소비자 가격 변동을 보여주는 대표 지수이므로 글로벌 인플레이션 수준을 가늠하고 연준의 금리 정책을 전망하는 단서가 된다.미국 노동통계국이 운영하는 U.S. BUREAU OF LABOR STATISTICS에서는 CPI를 포함해 여러 글로벌 경제 지표를 API로 제공하고 있으며 파이썬 requests 모듈로 간단히 필요한 데이터를 호출할 수 있다.BIS 제공 Public Data API - Python Sample Code중요한 부분은 requests.post()의 인자로 던져주는 data다. 지표 코드(seriesid)와 수집할 기간(startyear, endyear)을 json 형식으로 입력해야 한다. 그럼 지표 코드는 어디서 확인할 수 있을까?BIS는 대중적으로 사용되는 지표들을 Top Picks로 모아서 보여주고 있다. 네비게이션 바에서 DATA TOOLS &amp;gt; Data Retrieval Tools &amp;gt; BLS Popular Series를 선택하면 다음과 같은 페이지가 표시된다.크게 고용, 생산성, 가격, 급여(보수)라는 4개의 카테고리로 제공하고 있다. Price Indexes 부문 중 CPI-U/Less Food and Energy (Unadjusted)가 우리가 찾는 데이터다.지표에 대한 설명은 월간 리포트를 통해 상세히 확인할 수 있다. 검색창에 cpi 등을 입력하거나 ECONOMIC RELEASES 탭의 Latest Releases를 클릭하면 자료들이 유형별로 리스트업 된다.CPI 2023 10월자 자료 예시2. Core CPI 데이터 추출 및 집계CPI는 기본적으로 전년 동월 대비 성장률을 본다. 특히 올해의 인플레이션이 작년 대비 어느 수준인가?를 확인하려면 먼저 YoY(전년 대비 증감)를 집계하고 이후 분석 필요에 따라 전월 대비 증감을 체크한다.데이터 추출부터 YoY 집계까지의 파이썬 코드는 다음과 같다.def get_time_idx(x): if x[&#39;month&#39;] &amp;amp;lt;= 9: date = f&quot;{x[&#39;year&#39;]}-0{x[&#39;month&#39;]}&quot; else: date = f&quot;{x[&#39;year&#39;]}-{x[&#39;month&#39;]}&quot; return datedef get_data(json_data): df = pd.DataFrame(json_data[&#39;Results&#39;][&#39;series&#39;][0][&#39;data&#39;]).iloc[:,[0,1,2,4]] df[&#39;month&#39;] = df[&#39;period&#39;].apply(lambda x : int(x[1:])) df.drop(columns=[&#39;period&#39;], inplace=True) df[&#39;date&#39;] = df.apply(lambda x : get_time_idx(x), axis=1) df.drop(columns=[&#39;year&#39;,&#39;periodName&#39;,&#39;month&#39;], inplace=True) df = df.sort_values(by=&#39;date&#39;).set_index(&#39;date&#39;) df[&#39;value&#39;] = df[&#39;value&#39;].astype(&#39;float64&#39;) df[&#39;yoy&#39;] = df[&#39;value&#39;].pct_change(periods=12) df = df.dropna() return dfheaders = {&#39;Content-type&#39;: &#39;application/json&#39;}data = json.dumps({&quot;seriesid&quot;: [&#39;CUUR0000SA0L1E&#39;],&quot;startyear&quot;:&quot;2015&quot;, &quot;endyear&quot;:&quot;2023&quot;})p = requests.post(&#39;https://api.bls.gov/publicAPI/v2/timeseries/data/&#39;, data=data, headers=headers)json_data = json.loads(p.text)df = get_data(json_data)컬럼 중 value는 원본 데이터, yoy는 전년 동월 대비 증감이다. 여기서 각각 다시 직전 월 대비 증감을 구할 수 있으며 해석하자면 소비자물가지수가 전월 대비 얼마나 성장했는가? 소비자물가지수의 전월 동월 대비 증감률이 전월 대비 얼마나 성장했는가?이다.코로나 전후 각 데이터의 양상을 보면 그 차이를 가시적으로 확인할 수 있다.fig, axes = plt.subplots(ncols=1, nrows=4, figsize=(6,10))event_idx = df.index.to_list().index(&#39;2020-01&#39;)df[&#39;value&#39;].plot(ax=axes[0])axes[0].axvline(x=event_idx, c=&#39;pink&#39;, linestyle=&#39;--&#39;, linewidth=0.7)df[&#39;value&#39;].diff().plot(ax=axes[1])axes[1].axvline(x=event_idx, c=&#39;pink&#39;, linestyle=&#39;--&#39;, linewidth=0.7)df[&#39;yoy&#39;].plot(ax=axes[2])axes[2].axvline(x=event_idx, c=&#39;pink&#39;, linestyle=&#39;--&#39;, linewidth=0.7)df[&#39;yoy&#39;].diff().plot(ax=axes[3])axes[3].axvline(x=event_idx, c=&#39;pink&#39;, linestyle=&#39;--&#39;, linewidth=0.7)plt.tight_layout()plt.show()상단부터 순서대로 value, value의 변화율, yoy, yoy의 변화율이다. 가운데 수직선이 코로나가 본격적으로 확산하기 시작한 2020년 1월에 해당한다. 연준이 유동성을 공급하기 시작한 시점과 맞닿기 때문에 첫 번째 데이터(value)에서 수직선 우측 그래프의 경사도가 높아지는 것을 확인할 수 있다.두 번째 데이터(value.diff())의 우측은 0보다 상단에 위치하여 물가가 떨어지지 않고 계속 오르는 모양새를 보여주며, 그 아래 yoy는 전년 대비 물가가 크게 올랐고, 오른 상태가 유지되는 모습을 보여준다. 마지막 yoy.diff() 전년 동월 대비 물가 증감의 변동률이 코로나 이후 급격히 확대된 것을 보여준다.이렇게 정성적으로 살펴볼 수 있으나 통계적으로 과연 그 차이가 유의한가?를 확인할 때는 paired t-test를 수행한다. python에서는 scipy 패키지로 간단히 수행할 수 있다. paired t-test의 경우 두 비교 대상의 데이터 길이가 같아야 함에 유의한다.from scipy import statsprint(stats.ttest_rel(df[&#39;value&#39;].diff().dropna().loc[&#39;2019-01&#39;:&#39;2019-12&#39;], df[&#39;value&#39;].diff().dropna().loc[&#39;2021-01&#39;:&#39;2021-12&#39;]))print(stats.ttest_rel(df[&#39;yoy&#39;].loc[&#39;2019-01&#39;:&#39;2019-12&#39;], df[&#39;yoy&#39;].loc[&#39;2021-01&#39;:&#39;2021-12&#39;]))print(stats.ttest_rel(df[&#39;yoy&#39;].diff().dropna().loc[&#39;2019-01&#39;:&#39;2019-12&#39;], df[&#39;yoy&#39;].diff().dropna().loc[&#39;2021-01&#39;:&#39;2021-12&#39;]))Ttest_relResult(statistic=-3.0930454921018695, pvalue=0.010230311851951779)Ttest_relResult(statistic=-3.542850119128582, pvalue=0.004609581345573239)Ttest_relResult(statistic=-2.108403107832839, pvalue=0.05873916489588582)paired의 귀무가설은 “이벤트 전, 후 기간의 평균 차이가 통계적으로 유의하지 않다”다. 평균 검정이므로 선형적인 상승 추세가 존재하는 value를 제외한 나머지 3개 데이터 유형을 확인했고, 그 결과 5% 기각역에 진입한 value.diff()와 yoy는 귀무가설을 기각하고 두 기간의 평균 차이가 유의함을 확인했다.3. 시계열 정상성 검정과거의 Core CPI로 다음 달 Core CPI를 예측하는 것이 목표다. 따라서 과거의 데이터 분포가 일정하여 그 분포를 예측에 활용할 수 있다는 가정이 필요하며, 만약 그 가정이 틀렸다면 성립할 수 있도록 데이터를 ‘정상화’해야 한다.실무적으로 ‘정상 시계열’이라 함은 평균과 표준편차가 과거부터 꾸준히 일정함을 의미한다. 분포가 일정하지 않다면 통계 기반 예측을 수행할 수 없다. 시계열의 가장 대표적인 특징 2가지는 ‘자기상관성’과 ‘평균회귀’다. 평균회귀는 예측에 유용하고 자기상관성은 예측을 방해한다. 자기상관성은 과거의 x가 미래의 x에 영향을 미치는 성질을 말하며 이 정도가 강하면 시간이 지남에 따라 분포가 계속해서 달라지기 때문이다.따라서 이러한 자기상관성을 제거하기 위해 앞서 우리가 사용한 것처럼 차분(diff())을 하거나 로그변환을 수행할 수 있다. 그 횟수에는 제약이 없으나 어느 정도의 설명력을 유지하고 정보손실을 최소화하기 위해 2차 차분까지가 실무적으로 적절하다. 2차 차분을 굳이 해석하면 변화량의 차이, 증감 정도가 되겠다.정상성 검정은 Augmented Dickey-Fuller 방법론(이하 ADF)이 주로 활용된다.from statsmodels.tsa.stattools import adfullerdef adf_test(data, name): result = adfuller(data.values) print(name) print(f&#39;&amp;amp;gt;&amp;gt; statistics: {result[0]:.3f}&#39;) print(f&#39;&amp;gt;&amp;gt; p_value: {result[1]:.3f}&#39;) print(f&#39;&amp;gt;&amp;gt; critical values(5%):{result[4][&quot;5%&quot;]:.3f}&#39;) if result[1] &amp;amp;lt; 0.05: print(&#39;pass!! :)&#39;) else: print(&#39;fail :(&#39;) print(&#39;--------------------------&#39;)adf_test(df[&#39;value&#39;], name=&#39;value&#39;)adf_test(df[&#39;value&#39;].diff().dropna(), name=&#39;value (d:1)&#39;)adf_test(df[&#39;value&#39;].diff().diff().dropna(), name=&#39;value (d:2)&#39;)adf_test(df[&#39;yoy&#39;], name=&#39;yoy&#39;)adf_test(df[&#39;yoy&#39;].diff().dropna(), name=&#39;yoy (d:1)&#39;)adf_test(df[&#39;yoy&#39;].diff().diff().dropna(), name=&#39;yoy (d:2)&#39;)value&amp;amp;gt;&amp;gt; statistics: 2.862&amp;gt;&amp;gt; p_value: 1.000&amp;gt;&amp;gt; critical values(5%):-2.894fail :(--------------------------value (d:1)&amp;gt;&amp;gt; statistics: -1.404&amp;gt;&amp;gt; p_value: 0.580&amp;gt;&amp;gt; critical values(5%):-2.898fail :(--------------------------value (d:2)&amp;gt;&amp;gt; statistics: -3.116&amp;gt;&amp;gt; p_value: 0.025&amp;gt;&amp;gt; critical values(5%):-2.899pass!! :)--------------------------yoy&amp;gt;&amp;gt; statistics: -0.891&amp;gt;&amp;gt; p_value: 0.791&amp;gt;&amp;gt; critical values(5%):-2.898fail :(--------------------------yoy (d:1)&amp;gt;&amp;gt; statistics: -2.107&amp;gt;&amp;gt; p_value: 0.242&amp;gt;&amp;gt; critical values(5%):-2.898fail :(--------------------------yoy (d:2)&amp;gt;&amp;gt; statistics: -2.840&amp;gt;&amp;gt; p_value: 0.053&amp;gt;&amp;gt; critical values(5%):-2.898fail :(--------------------------정상성을 검정하는 여러 가지 방식이 있는데 ADF는 검증을 위해 ‘단위근의 존재’를 확인하는 방식이다.y_t = a*y_t-1 + e_t-1위와 같은 식에서 a의 해(근)를 찾는 상황이다. 이때 a=1이 될 수 있는가?를 확인한다. 만약 a=1이라면 y_t = y_t-1 + e_t-1이고 모든 시점을 전개했을 때 y_t = e_0 + e_1 + … + e_t-1이 된다. 즉, y_t는 예측 불가한 오차의 누적 합이므로 예측이 불가하다.한편 a=0일 때에도 y_t = e_t-1로, 모든 항이 오차가 되는데 이 때는 y_t-1과 y_t가 서로 독립이다. 즉, 자기상관성이 없고 오차 범위 내 일정한 분포(일정한 변동성)를 가지므로 예측이 가능하다.정상시계열은 a&amp;lt;1이다. 직전 값(y_t-1)에 1보다 작은 가중치를 곱하고 외부 충격(e)을 더하여 자기 자신이 되는데, a&amp;lt;1이라면 먼 과거에 발생한 외부 충격은 현재까지 영향을 주는 비율이 적으므로 시간이 지남에 따라 시계열이 ‘발산’하지 않을 수 있다. a&amp;gt;=1이라면 시계열은 시간이 지남에 따라 발산하고 각 데이터포인트는 오차의 합이 되어 random이다.돌아와서, 위에서 수행한 adf 검정 결과 value의 2차 차분 데이터만 단위근이 존재한다는 귀무가설을 기각하고 정상성을 만족함을 확인했다.한편, 정상성은 자기상관함수를 통해서도 시각적으로 발견할 수 있다.from statsmodels.graphics.tsaplots import plot_acf, plot_pacffig, axes = plt.subplots(ncols=2, nrows=4, figsize=(12,10))plot_acf(df[&#39;value&#39;], ax=axes[0,0])plot_acf(df[&#39;value&#39;].diff().dropna(), ax=axes[1,0])plot_acf(df[&#39;yoy&#39;], ax=axes[2,0])plot_acf(df[&#39;yoy&#39;].diff().dropna(), ax=axes[3,0])plot_pacf(df[&#39;value&#39;], ax=axes[0,1])plot_pacf(df[&#39;value&#39;].diff().dropna(), ax=axes[1,1])plot_pacf(df[&#39;yoy&#39;], ax=axes[2,1])plot_pacf(df[&#39;yoy&#39;].diff().dropna(), ax=axes[3,1])plt.tight_layout()plt.show()좌측은 자기상관성, 우측은 부분자기상관성을 각 시차별로 보여주며 상단부터 2개는 각각 value와 value의 1차 차분, 하단 2개는 yoy와 yoy의 1차 차분 데이터를 분석한 결과다.이렇게 1차 차분을 수행하더라도 직전 데이터와의 상관성이 여전히 존재한다. 반면, 아래와 같이 2차 차분을 수행하면 1개 이상의 시차에 대해 모두 상관성이 소거된다.fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8,6))plot_acf(df[&#39;value&#39;].diff().diff().dropna(), ax=axes[0,0])plot_acf(df[&#39;yoy&#39;].diff().diff().dropna(), ax=axes[1,0])plot_pacf(df[&#39;value&#39;].diff().diff().dropna(), ax=axes[0,1])plot_pacf(df[&#39;yoy&#39;].diff().diff().dropna(), ax=axes[1,1])plt.tight_layout()plt.show()adf 검정 결과처럼 yoy의 2차 차분보다 value의 2차 차분이 기각역 내에 충분히 포함되어 있음을 알 수 있다.4. ARIMA 모델링한편, auto_arima 패키지를 통해 자동으로 차분 횟수를 결정할 수 있다. 그뿐만 아니라 ar(p), ma(q) 각 모형의 차수도 함께 결정하여 최적의 arima 계산식을 결정한다. arima는 ar(p), ma(q) 모형을 합친 형태로 차분까지 고려한다. 즉, 시계열의 오차 이동평균과 자기상관성을 모두 계산에 포함하되 d차 차분한 데이터로 계산을 수행한다. ar(p): y를 추정하는 데에 지난 p기간의 y(자기 자신)를 고려하겠다 ma(q): y를 추정하는 데에 지난 q기간의 e(오차)를 고려하겠다 i(d): ar(p), ma(q) 모델링을 d차 차분한 데이터로 수행한다.model_value = pm.auto_arima( y=data_value, d=1, start_p=0, max_p=3, start_q=0, max_q=3, m=1, seasonal=False, stepwise=True, trace=True)Performing stepwise search to minimize aic ARIMA(0,1,0)(0,0,0)[0] intercept : AIC=188.093, Time=0.31 sec ARIMA(1,1,0)(0,0,0)[0] intercept : AIC=144.993, Time=0.11 sec ARIMA(0,1,1)(0,0,0)[0] intercept : AIC=150.317, Time=0.10 sec ARIMA(0,1,0)(0,0,0)[0] : AIC=260.225, Time=0.03 sec ARIMA(2,1,0)(0,0,0)[0] intercept : AIC=143.666, Time=0.07 sec ARIMA(3,1,0)(0,0,0)[0] intercept : AIC=143.753, Time=0.08 sec ARIMA(2,1,1)(0,0,0)[0] intercept : AIC=145.047, Time=0.13 sec ARIMA(1,1,1)(0,0,0)[0] intercept : AIC=143.142, Time=0.07 sec ARIMA(1,1,2)(0,0,0)[0] intercept : AIC=143.547, Time=0.21 sec ARIMA(0,1,2)(0,0,0)[0] intercept : AIC=141.235, Time=0.06 sec ARIMA(0,1,3)(0,0,0)[0] intercept : AIC=143.157, Time=0.07 sec ARIMA(1,1,3)(0,0,0)[0] intercept : AIC=141.837, Time=0.19 sec ARIMA(0,1,2)(0,0,0)[0] : AIC=170.584, Time=0.05 secBest model: ARIMA(0,1,2)(0,0,0)[0] interceptTotal fit time: 1.500 secondsmodel_yoy = pm.auto_arima( y=data_yoy, d=1, start_p=0, max_p=3, start_q=0, max_q=3, m=1, seasonal=False, stepwise=True, trace=True)Performing stepwise search to minimize aic ARIMA(0,1,0)(0,0,0)[0] intercept : AIC=-715.722, Time=0.05 sec ARIMA(1,1,0)(0,0,0)[0] intercept : AIC=-750.544, Time=0.06 sec ARIMA(0,1,1)(0,0,0)[0] intercept : AIC=-745.228, Time=0.10 sec ARIMA(0,1,0)(0,0,0)[0] : AIC=-717.252, Time=0.04 sec ARIMA(2,1,0)(0,0,0)[0] intercept : AIC=-755.869, Time=0.12 sec ARIMA(3,1,0)(0,0,0)[0] intercept : AIC=-757.021, Time=0.15 sec ARIMA(3,1,1)(0,0,0)[0] intercept : AIC=-756.707, Time=0.09 sec ARIMA(2,1,1)(0,0,0)[0] intercept : AIC=-746.786, Time=0.15 sec ARIMA(3,1,0)(0,0,0)[0] : AIC=-758.600, Time=0.04 sec ARIMA(2,1,0)(0,0,0)[0] : AIC=-757.636, Time=0.05 sec ARIMA(3,1,1)(0,0,0)[0] : AIC=-758.310, Time=0.07 sec ARIMA(2,1,1)(0,0,0)[0] : AIC=-749.194, Time=0.06 secBest model: ARIMA(3,1,0)(0,0,0)[0] Total fit time: 0.984 secondsvalue 데이터로는 1차 차분 데이터에 대해서 MA(2)로 최적의 모델링이 가능하다. 다시 말해 정상성을 확보하는 2차 차분이 아닌 1차 차분까지만 수행하고 지난 2개의 error term으로 y를 예측하겠다는 의미다.auto_arima로 적합시킨 모형의 요약통계를 아래와 같이 확인할 수 있다.print(model_value.summary()) SARIMAX Results ==============================================================================Dep. Variable: y No. Observations: 94Model: SARIMAX(0, 1, 2) Log Likelihood -66.618Date: Tue, 05 Dec 2023 AIC 141.235Time: 00:36:30 BIC 151.365Sample: 01-01-2016 HQIC 145.325 - 10-01-2023 Covariance Type: opg ============================================================================== coef std err z P&amp;gt;|z| [0.025 0.975]------------------------------------------------------------------------------intercept 0.7222 0.126 5.725 0.000 0.475 0.969ma.L1 0.7906 0.116 6.837 0.000 0.564 1.017ma.L2 0.3484 0.097 3.576 0.000 0.157 0.539sigma2 0.2435 0.036 6.741 0.000 0.173 0.314===================================================================================Ljung-Box (L1) (Q): 0.01 Jarque-Bera (JB): 7.88Prob(Q): 0.94 Prob(JB): 0.02Heteroskedasticity (H): 2.60 Skew: 0.66Prob(H) (two-sided): 0.01 Kurtosis: 3.54===================================================================================Warnings:[1] Covariance matrix calculated using the outer product of gradients (complex-step).print(model_yoy.summary()) SARIMAX Results ==============================================================================Dep. Variable: y No. Observations: 94Model: SARIMAX(3, 1, 0) Log Likelihood 444.380Date: Tue, 05 Dec 2023 AIC -880.760Time: 00:37:19 BIC -870.630Sample: 01-01-2016 HQIC -876.670 - 10-01-2023 Covariance Type: opg ============================================================================== coef std err z P&amp;gt;|z| [0.025 0.975]------------------------------------------------------------------------------ar.L1 0.7229 0.129 5.585 0.000 0.469 0.977ar.L2 -0.1489 0.123 -1.210 0.226 -0.390 0.092ar.L3 -0.1835 0.084 -2.181 0.029 -0.348 -0.019sigma2 4.104e-06 5.46e-07 7.514 0.000 3.03e-06 5.17e-06===================================================================================Ljung-Box (L1) (Q): 0.05 Jarque-Bera (JB): 118.85Prob(Q): 0.83 Prob(JB): 0.00Heteroskedasticity (H): 5.96 Skew: 1.10Prob(H) (two-sided): 0.00 Kurtosis: 8.08===================================================================================Warnings:[1] Covariance matrix calculated using the outer product of gradients (complex-step).Ljung-Box는 잔차의 정상성을 검정한다. 앞서 확인한 정상성은 모델이 적합할 대상에 대한 정상성이다. 즉, 데이터가 발산하거나 랜덤 워크가 아니라 특정 분포를 유지하는가?를 검정한 것이며, Ljung-Box가 검정하는 것은 모델이 적합하고 남은 오차끼리 서로 자기상관성이 없는가? 에 대한 내용이다.Ljung-Box의 귀무가설은 잔차 자기상관성이 0이다(존재하지 않는다)이며, 위 두 케이스에서 모두 Prob(Q)가 0.05보다 크므로 모델이 잔차에 정상성을 남기지 않고 잘 적합시켰다고 볼 수 있다.Heteroskedasticity는 이분산성, 그러니까 ‘시간 경과에 따라 분산이 달라짐’을 의미한다. 이분산성이 존재한다면 전 시차에 걸쳐 잔차가 일정 분포를 유지하지 못하는 것이며, 온전히 잔차만 걸러진 것이라 확신할 수 없다. 따라서 이분산성이 아닌 동분산성에 대한 잔차 가정을 만족하는가를 검증해야 한다.한편, Jarque-Bera 검정은 잔차의 정규성을 확인한다. 이분산성 검정의 귀무가설은 ‘잔차의 분산이 일정하다’이며, Jarque-Bera 검정 귀무가설은 ‘잔차가 정규성을 만족한다’이다. 두 검정에서 P-value는 0.05보다 작으므로 기각한다.동분산성과 정규성을 만족시키는 여러 방법론이 존재한다. 하지만 단일 시계열로 미래값을 예측하는 것은 외생변수의 존재를 고려하지 못하므로 근본적인 제약이 있다. 따라서 이와 같은 단일 시계열에 대해 동분산성과 정규성을 만족시키더라도 예측의 한계가 뚜렷하기에 그 제약을 인정하고 부가적인 처리 없이 예측을 수행한다.(*auto_arima로 적합한 모형 인스턴스는 잔차 검정 결과에 대해 아래와 같이 시각화 함수를 함께 제공한다.)model_value.plot_diagnostics(figsize=(8,6))plt.tight_layout()plt.show()model_yoy.plot_diagnostics(figsize=(8,6))plt.tight_layout()plt.show()5. ARIMA 예측 결과 시각화value(원본 데이터), yoy(전년 동월 대비 증감) 각각 적합시킨 ARIMA 모형을 통해 다음 달의 yoy를 예측한다. 주의할 점은 value에 적합시킨 모형의 예측값은 value의 분포로, yoy에 적합시킨 예측값은 yoy의 분포로 그 결과를 추정하기 때문에 value의 경우 yoy의 형태로 변환해야 한다. 예시) value 기존 값(2016-01~2023-10)과 예측 값(2023-11-01~2024-03-01)date2016-01 244.5282016-02 245.6802016-03 246.3582016-04 246.9922016-05 247.544 ... 2023-06 308.9102023-07 309.4022023-08 310.1032023-09 310.8172023-10 311.380Name: value, Length: 94, dtype: float642023-11-01 311.9569432023-12-01 312.6252142024-01-01 313.3474252024-02-01 314.0696372024-03-01 314.791848Freq: MS, dtype: float64 예시) yoy 기존 값(2016-01~2023-10)과 예측 값(2023-11-01~2024-03-01)date2016-01 NaN2016-02 NaN2016-03 NaN2016-04 NaN2016-05 NaN ... 2023-06 0.0482902023-07 0.0465292023-08 0.0434922023-09 0.0414652023-10 0.040309Name: yoy, Length: 94, dtype: float642023-11-01 0.0403322023-12-01 0.0408932024-01-01 0.0415072024-02-01 0.0418632024-03-01 0.041926Freq: MS, dtype: float64value에서 yoy로의 변환은 간단히 pct_change(period=12) 함수만 적용해 주면 된다. 예측 결과를 비교하면 아래와 같다.data_value_pred = model_value.predict(n_periods=5)data_value_all = pd.Series(list(data_value.values) + list(data_value_pred.values)).pct_change(periods=12).dropna().reset_index(drop=True)data_value_all.plot()plt.axvline(x=len(data_value_all)-len(data_value_pred), c=&#39;pink&#39;, linewidth=0.7, linestyle=&#39;--&#39;)plt.show()data_yoy_pred = model_yoy.predict(n_periods=5)data_yoy_all = pd.Series(list(data_yoy.values) + list(data_yoy_pred.values)).dropna().reset_index(drop=True)data_yoy_all.plot()plt.axvline(x=len(data_yoy_all)-len(data_yoy_pred), c=&#39;pink&#39;, linewidth=0.7, linestyle=&#39;--&#39;)plt.show()이렇게 같은 데이터에 기반한 같은 모형이라도 집계 방식에 따라 전혀 다른 방향의 예측을 수행할 수 있다. 이 부분이 단변량 시계열의 제약이며, 경제 요인과 같은 연쇄적이고 복잡한 데이터에 취약하다.따라서 2개 이상의 독립변수를 고려할 수 있는 VaR 같은 다변량 모형을 다룰 수 있어야 하며, 특히 기본적인 회귀모형과 인과추론 방법론을 통해 변수를 찾고 설명력을 높이는 것도 반드시 필요하겠다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "FRED 주요 경제 지표 - 신용 스프레드(ICE BofA US High Yield Index Option-Adjusted Spread)", "url": "/posts/FRED-%EC%A3%BC%EC%9A%94-%EA%B2%BD%EC%A0%9C-%EC%A7%80%ED%91%9C-%EC%8B%A0%EC%9A%A9-%EC%8A%A4%ED%94%84%EB%A0%88%EB%93%9C-ICE-BofA-US-High-Yield-Ind/", "categories": "1. 기술, 머신러닝, 딥러닝", "tags": "", "date": "2023-10-17 00:00:00 +0900", "snippet": "하이일드 채권: 고위험/고수익 채권, 정크본드스프레드: 차액, 차이하이일드 채권 스프레드: 하이일드 채권 수익률 - 안전 자산(국채) 수익률참고 지표) ICE BofA US High Yield Index Option-Adjusted Spread2020년 3월 23일 하이일드 채권 스프레드 10.87%로 절정. 시장의 극단적인 위험 기피“낮은 신용의 채권에 대해 얼마나 많은 프리미엄을 지불해야 하는가?”본 지표는 대표적인 신용 스프레드로 하이일드 채권(BB 등급, S&amp;amp;P 신용등급 평가 기준)의 안전자산(국채) 대비 초과 수익률을 보여줌. 채권 발행자) 안전 자산인 국채 대신 위험한 BB 등급(S&amp;amp;P 신용등급 평가 기준) 이하 하이일드 채권을 판매하려면 얼마나 많은 프리미엄을 얹어줘야 하는가 채권 구매자) 안전 자산인 국채 대신 위험한 BB 등급(S&amp;amp;P 신용등급 평가 기준) 이하 하이일드 채권을 구매했을 때 얼마나 많은 초과 수익을 기대할 수 있는가초과 수익이 치솟는다 == 잘 팔리지 않는 채권이다 == 시장에서 위험을 선호하지 않는다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "더 나은 비즈니스 전략을 위한 데이터 활용 방법 4가지", "url": "/posts/%EB%8D%94-%EB%82%98%EC%9D%80-%EB%B9%84%EC%A6%88%EB%8B%88%EC%8A%A4-%EC%A0%84%EB%9E%B5%EC%9D%84-%EC%9C%84%ED%95%9C-%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%99%9C%EC%9A%A9-%EB%B0%A9%EB%B2%95-4%EA%B0%80%EC%A7%80/", "categories": "2. 도메인, 비즈니스", "tags": "", "date": "2023-09-10 00:00:00 +0900", "snippet": " kenway consulting에서 발행한 아티클(4 ways to leverage data for a better business strategy)을 번역하였습니다. kenway consulting은 시카고에 본사를 둔 직원 100명 규모의 IT 경영 컨설팅 펌입니다. 주로 기업의 디저털 전환 및 경영 관리를 위해 데이터 파이프라인과 통합 시스템을 기획, 설계하고 관련 컨설팅을 제공하며 세일즈 포스와 Power BI와 협력하고 있습니다.“기업 혁신은 어떻게 성공하는가? 지속가능한 성장의 뒤편에는 무엇이 있는가? 번성하는 기업 문화는 어떻게 만들 수 있는가?”이러한 질문들을 모두 관통하는 단 하나의 정답은 없을 것이다. 그러나 혁신하고, 성장하고, 번성하는 기업들의 공통점은 있다. 바로 데이터 기반 의사결정 능력이다.최근 태블로 서베이에 따르면, 데이터 분석 및 데이터 기반 의사결정을 효과적으로 활용하는 기업의 경우 매출과 이익이 8% 높고 비용은 4% 낮다. 사실, 데이터가 비즈니스 성공에 직결된다는 것은 이미 잘 알려져 있고 많은 기업들이 데이터 분석 기술을 활용하고 싶어 한다. 문제는 대부분의 기업이 데이터는 상당량의 데이터를 확보하고 있으나 활용하지 못하고 있다는 점이다.데이터를 효과적으로 활용하기 위해서는 데이터 정책과 관리 체계부터 제대로 수립하는 것이 중요하다. 이를 시작으로 포괄적이고 전략적인 접근이 이루어져야만 데이터로부터 무궁무진한 효용을 끄집어낼 수 있다. 성공적인 데이터 활용을 막는 제약들 방대한 데이터를 확보했더라도 이를 비즈니스 목표와 제대로 부합시키는 것은 어렵다. 고객 만족, 시장 경쟁, 규제 준수와 같은 비즈니스에 직결되는 중요한 문제들이 더 시급하기 때문에 데이터 구축 프로젝트 같은 일들은 자연스럽게 우선순위가 낮아진다.이 때문에 데이터 정책이나 관리 체계 수립 등의 문제는 주요 이니셔티브에 밀리고, 데이터 분석가들은 데이터를 보고 활용해야 하는 의사결정자들과 조직적으로 멀어진다. 데이터는 부서별로 모두 분리되어 있고, 통합적으로 관리되지 않아 액셔너블한 인사이트를 찾는 것은 거의 불가능하다. 비즈니스 성과를 높이는 4가지 데이터 역량 앞서 말한 ‘중요해보이는’ 문제들이 데이터 기술 도입보다 더 시급하게 느껴질 수 있으나 데이터를 제대로 활용하는 것은 결국 비즈니스 성과는 물론 ‘중요해 보이는’ 문제들까지 더 효과적으로 풀어내도록 한다.2-1. 고객을 더 잘 이해하기B2B, B2C 두 영역 모두에서 고객은 점점 더 개인화된 경험을 원하고 있다. 데이터를 활용하는 것은 고객을 더 잘 이해하고 세분화된 전략을 세울 수 있도록 한다.- 고객의 감성을 이해하고 인사이트를 얻어라고객의 감성은 수많은 요인에 의해 결정된다. 제품 후기나 설문조사는 매우 표면적인 작은 요인에 불과하다. 데이터 분석은 고객이 신경쓰는 ‘진짜 요인’을 발굴하게 해 준다.- 디테일한 고객 프로필 구축을 통해 수요를 예측하라데이터 기반 고객 프로필은 기업으로 하여금 고객이 관심을 갖고 있는 제품과 서비스를 투명하게 보여주고, 그들에게 도달하기 위한 올바른 마케팅 전략을 제시하도록 한다. 그러니 고객을 잘 분류하고, 그들의 개인화된 경험을 제공할 수 있도록 데이터를 활용하라.- 일관된 옴니채널 경험을 제공하라기업이 데이터 기반으로 고객 경험을 설계하기 시작하면 고객이 웹사이트에 방문하든, 고객 서비스 채널로 소통하든, 직접 영업팀과 대면하든 언제나 그들의 니즈에 맞는 가장 정확하고 최신화된 경험을 제공하게 된다. 여기서 통합 데이터 관리가 강력한 힘을 발휘하는데, 고객과의 최전선에 있는 영업 부서나 상담팀이 고객에 대한 정확한 정보를 데이터 도구를 통해 볼 수 있도록 하라.2-2. 직원들의 니즈를 효율적으로 체크하기고객과 마찬가지로 직원들도 그들이 속한 기업에 대해 개인화된 경험을 필요로 한다. 기업은 보통 직원들의 요구사항을 확인하기 위해 설문조사 같은 방법을 사용하는데, 이는 직원 개개인에 적용하기에 쓸모가 없다. 예를 들어, 개인의 건강 관련 데이터를 안전한 방식으로 상시 측정 및 수집할 수 있다면 이를 통해 개인화된 복지를 제공할 수 있을 것이다.이처럼 데이터를 통합하고 더 나은 분석 기법을 활용한다면 임직원 관련 공통의 문제를 해결하는 데에 도움을 줄 것이다.- 퇴사 사유를 객관적으로 분석하고 관리하라임직원을 신입 사원부터 고성과 인력까지 여러 유형으로 분류하고 각 특성을 분석한다면 각 유형의 직원들이 왜 근속하고, 왜 떠나는지 이해할 수 있다.- 조직별 특성을 파악하고 다양성을 높여라단순 인구통계를 넘어 지역, 부서별 승진 속도 등 통합적인 분석을 수행한다면 차별화된 인사정책을 피하고 좀 더 포용력 있는 조직을 만들어갈 수 있다.- 데이터로 직원의 성과를 높여라분산된 데이터를 모아 성과 KPI를 한 곳에서 관리하여 직원들의 역량 강화를 위한 프로그램을 통합적으로 설계하라- 온보딩 효율을 체크하라신규 직원이 생산성을 갖추는 데에 얼마나 걸리는지, 어떤 요인이 그들로 하여금 성과를 만들어내는지 각 부서와 매니저 역량 별로 추적할 수 있다.2-3. 성공적이고 지속가능한 성장 전략 세우기성숙한 데이터 역량을 갖춘 61%의 기업은 시장에 성공적으로 진입한다. 그 성공에는 데이터 R&amp;amp;D에 대한 투자, 가장 생존가능성이 높은 전략에 대한 몰입이 있었다.- 고객 생애주기를 고려해 성장 전략 수립하라고객 활성화 단계와 생애주기에 따라 그들이 언제, 어떻게 매출에 영향을 미칠지 예측하고 새로운 기회를 창출하라.- 우선순위에 따라 기회를 시스템적으로 관리하라각각의 성장 기회에 대한 비용 효용 분석을 통해 언제, 어떤 것에 가장 먼저 집중할지 판단하라.- 모든 유입 채널의 고객 경험을 최적화하라새로운 제품, 시장, 고객 유형에 따라 가장 최적화된 ROI(투자대비수익)을 만들어내라.- 쉽게 피드백을 남기고 추적하라모든 유형의 피드백을 쉽게 수집하고, 중앙 관리하여 언제든지 그것에 진입, 측정하고 이를 통해 필요에 따라 새로운 전략을 수립하라.2-4. 민첩성 기르기변화에 대응하는 것은 지속적인 도전이다. 때문에 언제나 민첩한 혁신을 추구해야 하며 이를 통해 더 뛰어난 성과를 내고 혼란을 피할 수 있다. 데이터를 효과적으로 활용하는 것은 민첩함의 핵심이다.- 데이터로 기초 의사결정을 내려라KPI, OKR 등 주요 비즈니스 매트릭들은 모두 의사결정을 위한 목표지향적인 가이드를 제공한다. 기업이 속한 산업과 비즈니스를 제대로 모니터 하기 위해 신뢰할 수 있는 올바른 데이터를 추출, 분석하고 변화와 트랜드를 상시 투명하게 바라봐야 한다.- 전략적 내부 파트너십을 구축하라애자일 조직에서 HR, 재무 등 각 부서는 기업의 주요 전략 파트너다. 기업의 전략은 데이터로부터 나오는데, 비즈니스 리더는 전반의 예측, 분석을 위해 재무 상황에 의존하며 HR 부서는 임직원의 성과나 복지를 포함해 점점 늘어나는 이슈에 대응해야 한다. 따라서 통합 데이터로 각 조직이 같은 지향점을 바라보고 함께 연계될 수 있도록 하라.- 임직원의 문제해결 역량을 강화하라각 단계의 임직원들은 매일 새로운 문제를 맞이한다. 포괄적인 데이터 접근 권한을 제공하여 그들이 문제 원인을 찾고, 신속하게 대응할 수 있도록 하라. 데이터 분석 역량을 향상시키는 방법 (솔루션 홍보) 데이터를 효율적으로 활용하는 문제는 단순히 더 많은 양의 데이터를 수집하거나 데이터 분석가를 채용하는 것으로 쉽게 해결되지 않는다. 명확하게 구축된 가이드라인이 없다면 날 것의 데이터와 그것을 분석하는 분석가들은 어떤 역할도 해내기 어렵다. 따라서 데이터 정책과 관리 체계를 명확히 세우고 조직 전반에 걸쳐 데이터 문화가 자리 잡도록 만드는 것, 그리고 임직원의 데이터 활용 역량을 높이는 것이 중요하다.우리는 비즈니스 인텔리전스 로드맵을 구축하고, 표준화된 리포트를 제공하고, 지속가능한 가치실현을 돕는 프레임워크를 설계하여 기업의 분석 역량을 높이는 데에 기여하고 있다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "Activation,퍼널이 아닌 고객 여정의 관점에서", "url": "/posts/Activation-%ED%8D%BC%EB%84%90%EC%9D%B4-%EC%95%84%EB%8B%8C-%EA%B3%A0%EA%B0%9D-%EC%97%AC%EC%A0%95%EC%9D%98-%EA%B4%80%EC%A0%90%EC%97%90%EC%84%9C/", "categories": "1. 기술, 머신러닝, 딥러닝", "tags": "", "date": "2023-05-26 00:00:00 +0900", "snippet": "그로스 조직은 Activation, 그리고 Retention을 높이기 위해 퍼널을 분석하고 개선한다. 커머스 플랫폼을 떠올려보면 광고, 상품페이지, 회원가입, 장바구니, 결제까지의 6개 흐름이 기본적인 퍼널을 구성하고, 각 단계에서의 전환 과정이 얼마나 효과적으로 이루어지고 있는가를 분석하는 것이다.만약 광고 클릭율은 높은데 상품페이지에서 대부분의 유저가 이탈한다면 타겟 세그먼트를 잘못 분류했거나 상품페이지의 흐름이 잘못 구성되었을 것이라 가정해볼 수 있다. 또는 가입 절차가 복잡할 수도 있다. 그리고 그 가정에 따라 기획안을 다시 살펴보고 타겟을 재정의하고, 광고 문구를 수정하거나 페이지를 재구성한다.그러나 많은 경우 이러한 개선이 효과적이지 않다. 혹은 해당 퍼널 지표가 개선되었다 하더라도 최종 목표 지표인 Retention나 GMV(총 매출) 개선으로 이어지는 것은 또 다른 문제다.따라서 특정 지표에 대한 실질적 개선 효용을 파악하기 위해서는 단기적 영향이 아닌 장기적 영향을 바라봐야 한다.토스에서는 ‘AHA Moment’를 추출함으로써 이러한 영향관계를 판단한다. AHA Moment 추출 과정은 다음과 같다.1. 가설 - AHA MomentAHA Moment는 목표 지표(Feature)에 선행하며 ‘XX라는 행동을 YY라는 날짜 안에 ZZ번 한다’라는 문장으로 일반화할 수 있다. 따라서 AHA Moment를 추출하는 것은 XX, YY, ZZ라는 3가지 변수를 찾아내는 것과 같다.2. XX 찾기 - SHAP Value 분석SHAP은 Shapley Additive Explanations의 약자로 ML 모델의 변수 설명력(공헌도, feature importance)을 보여주기 위한 방법론이다. 즉 아래 이미지에서 xx1, xx2, … , xx6까지 6개의 독립변수가 Feature Value(y)에 얼마나, 그리고 어떤 영향을 미치는가를 보여주고 있다.toss - po sessionxx1의 경우 SHAP Value가 음수일 때 Feature Value도 낮게(파란색) 나타나고, 양수일 때는 반대로(빨간색) 나타난다. 또한 SHAP Value가 음~양 전체에 걸쳐 Feature Value에 대해 전반적인 공헌을 하고 있으므로 가장 설명력이 높은, ‘양의 상관성’이 있는 변수라고 할 수 있겠다. 이렇게 AHA Moment 추출 과정의 첫번째, XX는 ‘xx1’이라는 것을 확인했다.*참고로 파이썬에서 shap 라이브러리를 제공하고 있으며, ML 모델을 학습시킨 후 아래와 같이 코드를 작성하면 동일한 그래프를 확인할 수 있다.import shapexpl = shap.Explainer(model)shap_vals = expl(X)shap.plots.beeswarm(shape_vals)3. ZZ 찾기 - 최적해 도출다음으로 우리는 어떤 액션을 해야 Target Action(여기서는 Ratention)이 유의하게 발생하는가를 확인해야 한다. 앞서 찾은 xx1이 가입 후 1주일 이내의 메시지 전송 수라고 가정해보자.(YY는 xx1 발견 과정에서 함께 추출될 수 있고 본 단계에서 YY를 찾을 수도 있다. 여기서는 이전 단계에서 yy=1w로 확인했음을 가정한다.) 개선하고자 하는 지표는 Retention이므로 ‘회원가입 이후 메시지를 N번 보낸 유저가 1주일 이내 다시 접속한다면 Retained 되는데(설명력이 있는데), 그렇다면 Retention을 극대화하는 최적의 N은 무엇인가?를 찾는 것이 목표다.아래 이미지에서 좌측은 메시지를 1번 보낸 유저가 1주일 이내 다시 접속하는 경우의 볼륨(유저 수), 우측은 메시지를 8번 보낸 유저가 1주일 이내 다시 접속하는 볼륨이 되겠다.toss - po session좌측의 경우 재접속자의 다수를 설명해주고 있으나 Action(메시지 전송)을 한 사람들 중 일부만이 Retained 된 것이므로 Action이 너무 약한 것이다. 반면 우측의 경우 Action을 한 사람들 중 다수가 Retained 되었으나 해당 Retained 된 사람들이 해당 Action으로만 설명되지 않는 영역이 크다. 즉 Action이 너무 강한 것이다.따라서 우리는 교집합 영역이 넓고, 대칭차집합(합집합-교집합)이 좁은 N(메시지 전송 수)을 찾아야 한다.toss - po session다만, 교집합 영역의 비율만 확인하는 경우 이를 인과성이 존재한다고 판단하기는 어렵다. 설명되지 않은 다른 변수로 인해 동시에 발생한, 즉 독립변수와 종속변수의 인과관계가 아닌 서로 다른 Y로서 동시에 영향을 받은 종속변수일 가능성도 존재하기 때문이다.(상관성은 있으나 인과성은 없는 경우) 따라서 토스에서는 인과성에 대한 해석력을 높이기 위해 교차 영역과 동시에 Retention 유저에 대한 Action(XX를 ZZ번 하는 것) 발생 확률을 함께 평가한다고 설명한다.toss - po session이렇게 최적의 ZZ(N)를 찾았다면 의도적으로 유저가 XX를 ZZ번 하는 Action을 할 수 있도록 유도하고, 그렇게 Action한 유저는 고래(헤비 유저)가 될 가능성이 높다.이렇게 토스 PO SESSION에서 Retention을 높이기 위한 분석 방법론, 그 과정에서 Activation, 즉 AHA Mement를 정의하는 과정을 간단히 살펴보았다. 토스는 Activation을 The First Happy Experience라 정의할 만큼 유저의 사용 경험을 극대화하는 “주도적”인 장치로 인식하고 있다. Activation의 개선은 단순히 특정 퍼널 Conversion(ex. 회원가입 전환)을 높이는 것에 있는 것이 아니라 Target Action을 쉽게, 빠르게 경험하는 데에 있다. 이를 통해 장기적 목표 지표(여기서는 Retention)를 적극적으로 높여갈 수 있는 것이다. [참고) 토스 리더가 말하는 유저를 끌어당기는 서비스 개선 PO SESSION](https://www.youtube.com/watch?v=D9x7Tln2DI4) 관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "파이썬을 활용한 애플 주가수익률 예측 분석 - (3). 교차 검증 및 모델 선택", "url": "/posts/%ED%8C%8C%EC%9D%B4%EC%8D%AC%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-%EC%95%A0%ED%94%8C-%EC%A3%BC%EA%B0%80%EC%88%98%EC%9D%B5%EB%A5%A0-%EC%98%88%EC%B8%A1-%EB%B6%84%EC%84%9D-3-%EA%B5%90%EC%B0%A8-%EA%B2%80%EC%A6%9D-%EB%B0%8F-%EB%AA%A8%EB%8D%B8-%EC%84%A0%ED%83%9D/", "categories": "1. 기술, 머신러닝, 딥러닝", "tags": "", "date": "2023-05-02 00:00:00 +0900", "snippet": "본 시리즈는 주가 데이터의 자기 상관(Auto-Correlation) 특성을 억제하고, 동시간대 연관 자산(Cross-Sectional) 수익률 및 지연 수익률을 통한 미래 주가 수익률 예측하는 내용을 담고 있다. 본 장에서는 (1). 데이터 확인 및 예측 안정성 확보, (2). 변수간 상관분석 및 예측변수 정상성 검정에 이어 예측 모델을 구현하는 세 번째 실습을 진행한다.실습은 회귀(Regression) 예측을 위한 다양한 모델들을 일괄 구현한 다음, 교차검증(K-Fold Cross Validation)을 수행함으로써 각각의 성능을 비교 분석하는 방식으로 진행된다.Step 1. Train-Test Dataset Split데이터는 이전 장에서 구축한 df_Xy를 사용한다.예측 변수는 y, 애플 주가(AAPL)의 미래수익률이고 설명 변수는 X, 애플 주가의 지연(과거 5일, 20일, 60일 대비) 수익률과 연관 자산의 현재 수익률이 되겠다. sklearn.model_selection의 train_test_split을 사용하면 간단히 분리할 수 있는데, 여기서는 shuffle 같은 무작위 변환 작업을 하지 않기 때문에 굳이 라이브러리를 사용하지 않고 판다스 iloc 함수를 통해 즉시 8:2로 슬라이싱해주는 편이 더 좋겠다.Step 2. Model Selection회귀 모형은 다양한 방식으로 구현할 수 있다. 단순 회귀모형뿐만 아니라 제약(규제)을 받는 회귀, 거리 기반 회귀, 트리 구조의 회귀, 그리고 앙상블과 신경망으로 더 복잡하게 구현한 회귀 모형까지 그 종류는 매우 다양하다. 이 중 어떤 것이 현재의 데이터셋에 가장 예측력이 우수한지, 어떤 것이 가장 Robust(현실에서의 이상치, 특이값에 대해 영향을 덜 받는)한 지 판단하고자 위해 교차검증(Cross-Validation)을 수행해 볼 수 있다.2-1. Modeling교차검증을 수행하기 전에 먼저 리스트를 하나 만들고 비교 분석할 모델을 모두 담아준다. 이 때, 이후 시각화를 위해 (이름, 모델) 쌍의 튜플 형식으로 구조화하자. 참고로 모델 패키지는 1장에서 호출했던 라이브러리에 의존하며, 이름은 자유롭게 지정해주면 되겠다. 단, 이름이 너무 길면 시각화할 때 모양새가 좋지 않으니 간략하게 명명하는 것이 좋다.for문은 전체 모델을 반복 순회하며 res_cv, res_train, res_test에 각각 해당하는 score를 담아준다. 여기서 score는 rmse(root mean squared error)로 지정해 주었으니 낮을수록 좋다.반복문이 돌면 res_cv에는 전체 모델의 Cross-Validation Score가, res_train에는 학습 데이터셋에 대한 예측 Score가, res_test에는 평가 데이터셋에 대한 예측 Score가 차곡 차곡 쌓일 것이다.앞서 언급했듯 교차검증은 해당 모델이 얼마나 Robust한가, 즉 이상값(혹은 특이값)에 대한 민감도를 확인하기 위함이다. 많은 모델을 비교 분석함에 있어서 모델의 복잡도, 예측안정성, 해석가능성 등 다양한 요인을 고려해야 하는데 그 중에서도 가장 중요한 것은 예측안정성이다. 이미 딥러닝 모델이나 SVM처럼 복잡하고 해석력이 떨어지는 딥러닝 모델도 실무에서 빠지지 않고 활용되고 있는 만큼 안정적인 예측 성능 확보를 우선적으로 고려해야 한다. 교차검증은 이를 간편하게 수행하도록 해준다.다만, 시계열 분석에 있어서 교차검증은 주의해서 사용할 필요가 있다. 교차검증은 말 그대로 예측 구간을 교차해 가며 성능을 확인하는데, 시계열 데이터의 경우 데이터간 시간적 선후행 관계가 존재하므로 시간지평이 필요한 추세(모멘텀) 분석이나 인과추론 시에는 활용할 수 없다.해당 실습의 경우 추세와 계절성을 제거하였으며 (약)정상성을 확보한 시계열에 대한 내재된 패턴을 추정하고자 하므로 교차 검증을 사용하는 것이다. 모델이 패턴을 잘 찾아냈다면 이러한 패턴은 모든 시계열 구간에 걸쳐 통용될 수 있어야 한다. 다시 말하면, 2010년의 패턴이 2021년에 적용되는 것 뿐 아니라 2021년의 패턴이 2010년에도 적용되는 것이 자연스럽다는 의미다. 단, 여기서 분석 대상이 되는 전체 시계열 데이터는 거시 국면 전환 등으로 잠재 특성이 변하지 않았다는 가정을 전제한다. *위 교차검증 부분에 대한 이해가 어렵다면 그 개념에 대해서는 자료가 넘쳐나니 적극적으로 탐색해보길 바란다. 아래는 추천 자료다. 데이터사이언스스쿨 - 교차검증 [Youtube]Machine Learning Fundamentals: Cross Validation2-2. Compare Cross Validation Scores2-1의 출력 로그에 이미 결과는 모두 나왔다. 이제 그것을 보기좋게 차트로 구성해보려 한다.먼저 models 리스트는 (이름, 모델) 쌍의 튜플로 구성되어 있으므로 list comprehension 문법을 통해 이름만 추출해 x_labels로 재구조화했다. x_loc는 차트에 모델 이름이 순서대로 들어갈 수 있도록 하는 좌표값이다.차트에서 전체 모델을 평균 순으로 정렬할 것이다. 이를 위해 numpy의 argsort() 함수를 사용했으며 그것을 인덱스로 cv score와 모델 이름을 모두 정렬해주었다.이렇게 boxplot을 통해 차트를 그려주면 전체 모델의 cv score 분포를 살펴볼 수 있다. cv score가 낮을 수록(좌측) 시계열 데이터 전반에 대한 예측 안정성이 높다. 즉, Lasso(L1규제), ElasticNet(L1 + L2 규제 혼합)을 포함하는 기본적인 선형 회귀모형들이 cv score가 낮으며, 의사결정(Tree) 모형이나 거리기반(KNN, SVR) 모형 대비 높은 안정성을 보인다.참고로 위 차트에서 boxplot 내부의 붉은 선은 중앙값(median)이다. 우리는 앞서 평균으로 정렬했기 때문에 그 순서가 동일하지 않다.2-3. Compare Model Accuray앞서 각 모델을 train dataset에 학습시키고, 그렇게 학습된 모델로 train dataset과 test dataset에 대해 각각 예측을 수행했다. 예측 성능은 rmse로 평가했으며, 그것이 낮을 수록 정확도(Accuracy)는 높다.차트 시각화를 위한 모델명, 데이터 정렬 방식은 2-2에서 수행한 것과 동일하다. 다만, 이번에는 cv값이 아닌 단일 예측 값이므로 위와 같이 res_test를 기준으로 정렬해줄 것이다. 즉, test-error가 낮을 수록 차트상 높게 위치하게 된다.cv score를 통해서도 확인했지만 lr을 포함해 lasso, elasticnet과 같은 선형 회귀모형이 성능이 가장 좋았으며, 거리기반 모형(SVR, KNN)과 의사결정(Tree) 모형은 반대로 좋지 않았다. 특히 트리 모형은 앙상블(ExtraTrees)과 함께 Train dataset에 대해 과적합(error=0)된 것을 확인할 수 있었다.또한, 신경망 모형의 경우 Train dataset과 Test dataset에서의 예측오차가 가장 균일(두 예측오차간 차이가 적음)했다. 주가수익률예측모델의 경우 이미 학습한 train dataset에 대해서도 예측 성능이 높지 않은 경향을 보이는 점을 고려하면, 신경망 모형이 단순 선형 회귀모형 대비 train dataset에 더 적합되었다고 생각해볼 수 있다. 미미한 수치지만(사실 주가 예측에서 만큼은 결코 미미하지 않다), 너무 복잡한 모형을 단순한 선형 패턴의 데이터를 학습하는 데에 사용했기에 나타난 결과다.결과적으로, 선형 회귀 모형(lr, lasso, elasticnet)이 상대적으로 좋은 성능을 보인 이유는 2장에서 확인했듯 연관 자산 수익률간의 양의 상관관계, 지연 수익률간의 양의 상관관계가 있었기 때문이다. 지수 추종 ETF 상품에 포함된 대형 연관 자산의 동시적 움직임, 꾸준히 우상향한 애플 주가로 인한 Negative Skew 현상이 이러한 선형 패턴을 형성하는 데에 핵심적인 요소로 작용했을 것이다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "파이썬을 활용한 애플 주가수익률 예측 분석 - (2). 변수간 상관분석 및 예측변수 정상성 검정", "url": "/posts/%ED%8C%8C%EC%9D%B4%EC%8D%AC%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-%EC%95%A0%ED%94%8C-%EC%A3%BC%EA%B0%80%EC%88%98%EC%9D%B5%EB%A5%A0-%EC%98%88%EC%B8%A1-%EB%B6%84%EC%84%9D-2-%EB%B3%80%EC%88%98%EA%B0%84-%EC%83%81%EA%B4%80%EB%B6%84%EC%84%9D-%EB%B0%8F-%EC%98%88%EC%B8%A1%EB%B3%80%EC%88%98-%EC%A0%95%EC%83%81%EC%84%B1/", "categories": "1. 기술, 머신러닝, 딥러닝", "tags": "", "date": "2023-04-30 00:00:00 +0900", "snippet": "본격적으로 모델링을 수행하기 전에 변수간 상관관계가 어떤지, 예측변수의 정상성이 확보되었는지 확인할 필요가 있다. 특정 설명변수가 예측변수와 상관관계가 강하거나 특정 설명변수 간 상관관계가 강하다면 해당 변수를 유심히 살펴야 한다. 전자의 경우 예측변수에 후행하는 것은 아닌지, 후자의 경우 동일한 외생변수를 갖거나 둘 사이에 상호 인과성이 존재하는 것은 아닌지 등을 확인하고, 해당 변수를 소거하거나 집계를 통해 시점 혹은 분포를 변환해줘야 한다. 또한, 예측변수 내에 설명변수들로 하여금 예측변수를 추정하기 어렵도록 하는 특정 분포(추세, 계절성)의 존재 여부 역시 확인해야 한다. 예측변수 자체가 시간에 따라 그 분포(평균, 표준편차)를 달리한다면 단일변수를 통한 회귀분석은 물론 다중변수 모델링은 제대로 된 예측을 수행할 수 없기 때문이다.Step 1. Data Preprocessing이번 장에서는 변수간 상관분석 및 예측변수의 정상성 검정을 수행한다. 이는 앞으로의 모델링 과정을 위한 사전 작업으로, 정상적인(robust) 모델이 구현될 수 있도록 이전 장에서 확보한 데이터셋을 먼저 적절히 처리해보자. 시계열 예측에 가장 선행되어야 하는 중요한 작업은 시점 처리다. 기본적으로 예측변수는 모든 설명 변수로부터 미래에 존재해야 하며 이 미래의 값을 현재로 당겨줘야 하는데, 그 단위가 ‘시’, ‘분’, ‘초’까지 내려가지 않는 이상 하루 간격으로 잡는 것이 일반적이다.예측변수 및 전체 설명변수는 모두 시계열 데이터다. 또한 그 자체로 자기상관성이 높은 금융 상품의 가격 데이터다. 가격(주가 등)은 본질적으로 전날의 가격을 기반으로 한다. 다시 말해 오늘의 시가가 전날의 종가다. 이러한 연결 고리를 끊어주지 않으면 시점이 누적됨에 따라 시계열의 분포는 달라진다. 따라서 단일 가격 데이터를 모두 로그-차분을 통해 각 일자별 수익률로 변환해주었다.동시에, 예측변수 y에 대해서는 shift(-1) 함수를 사용해 하루씩 당겨 미래(내일)의 값을 현재(오늘)로 옮겨왔다. 또한 예측변수 y는 그 자체로 자기상관성이 내재되어있다는 가정을 가지고 지연값(5일, 20일, 60일)을 사용할 것이므로 여타 설명변수가 diff(1)만 수행한 것과 달리 diff(5), diff(20), diff(60)을 수행함으로써 5일 전, 20일 전, 60일 전 대비 수익률을 함께 설명변수로 포함해주었다.Step 2. Correlation Check파이썬에서는 corr() 함수를 사용하면 해당 데이터셋에 존재하는 모든 변수들간의 상호 상관계수를 구할 수 있다. 애플, 아마존, 메타, 마이크로소프트, 테슬라는 모두 나스닥 시총 상위 종목이므로 인덱스 펀드를 통한 유동성 유입에 따라 어느 정도 상관성(상관계수의 분포는 -1부터 1까지며, 높을 수록 상관성이 높다)을 보인다. 이외 우리가 지연값으로 사용한 5일, 20일, 60일 대비 수익률 역시 양의 상관관계가 존재한다.주가 수익률 분포는 정규분포와 닮아 있으며, 정규분포에 비해 네거티브 왜도(skewness, 0보다 우측으로 기운)와 높은 첨도(kurtosis, 꼬리가 두꺼운)를 가진다. AAPL_d5, AAPL,d20, AAPL_d60의 분포를 보면 다른 분포에 비해 꼬리가 두껍고 평균이 우측으로 기운 것을 확인할 수 있는데, 이는 상대적으로 먼 과거에 비해 현재 주가의 분포를 보여주는 것이므로, 애플 주가가 과거 상당 기간동안 꾸준히 우상향했다는 사실을 알 수 있다.참고로, 왜도(skewness)와 첨도(kurtosis)는 위와 같이 매우 간단히 확인할 수 있다.  skewness가 0보다 낮을 수록 좌측 방향으로 슬로프가 생기는 것이며(평균 우측 이동), kurtosis가 0보다 클수록 꼬리가 두껍다. 분포의 꼬리는 불확실성을 내재한다. 즉, 꼬리가 두껍다면 극단적인 변화(국내로 치면 상한가, 하한가)가 상대적으로 빈번히 발생한다는 것이며, d1, d5, d20으로 갈수록 그 경향은 줄어든다. 즉 단기간 내 변화는 장기간 변화에 비해 불확실하며, 장기로 갈수록 보다 확정적이다.Step 3. Correlation by Month앞서 예측변수의 지연값(d5, d20, d60)은 서로 양의 상관관계가 있음을 확인했다. 특히, d5~d20의 경우 0.52, d20~d60의 경우 0.55로 꽤 밀접한 관련이 있었으며 이 중 일부를 소거할 것인가를 판단하기 위해 각 기간별 상관계수를 한번 더 확인하겠다.만약, 전체 기간에 대해 꾸준이 양의 상관계수가 나타나고 그 반대의 움직임을 포착할 수 없다면 일부 변수를 소거할 필요가 있다.상당 기간에서 음의 상관관계가 포착되었으며, 각 월별 상관계수의 움직임도 충분히 상이해보인다. 즉, 일부 변수를 제거했을 때 얻는 효용 대비 발생하는 정보손실이 클 것이다. 따라서 모든 변수를 소거 없이 사용하자. 최종적으로 사용할 모든 설명변수와 예측변수간 상관계수는 다음과 같다.Step 4. Check Stationarity of y먼저, 정상성이란 데이터의 기저 확률분포가 시점에 따라 변하지 않는 것을 말한다. 이를 엄밀하게 ‘강’정상성이라 하며, 확률분포 자체의 동질성보다는 평균과 분산의 동질성에 초점을 옮긴 것이 ‘약’정상성이다. 이전 장에서도 언급했듯 일반적으로 우리가 분석하는 데이터에서 정상성을 확보한다거나 할 때 말하는 정상성은 약정상성을 가리킨다.4-1. Time Series Decomposition우리는 사실 앞에서 로그차분을 수행하였기에 정상성을 확보되었으리라 기대한다. 다만 중요한 것은 위와 같이 시계열 데이터를 trend(추세), seasonal(계절성), residuals(잔차)로 분해했을 때 trend 역시 정상성이 확보되어야 한다는 점이다. 즉, 일간 수익률의 분포에서 뽑아낸 추세에 추세가 존재해서는 안된다. 다르게 말하면 기술적 시계열 분해를 통해 trend를 얻을 수 없어야 한다. 이미 로그 차분을 통해 추세를 제거했기(최소한, 제거했을 것이라 기대하기) 때문이다.4-2. ADF(Augmented Dickey-Fuller) Test이전 장에서 숙제로 남겼던 정상성 검정은 위와 같이 파이썬으로 간단히 실시할 수 있다. 예측값(y, AAPL)과 예측값의 추세(trend)에 대해 ADF 검정을 실시하였고, 둘 다 가장 강한 임계값(critical value, 1%)을 넘어 정상 시계열임을 확인했다.ADF Test는 단위근 검정이라고도 한다. 검정 과정을 살펴보면, 해당 귀무가설(h0)은 ‘단위근이 존재한다’이다. 단위근의 존재 확률이 1% 이내로 희박하다면 단위근이 존재하지 않는다는 사실을 충분히 신뢰할 수 있고, 단위근의 부재로 해당 데이터는 정상 시계열임을 검증한다. 단위근이 존재하게 되면 시계열은 발산하고, 특정 분포로 수렴하는 것이 불가능하기 때문에 단위근의 부재를 확인하는 것이다.본 장에서 우리가 새기고 넘어갈 것은 단위근의 존재가 시계열 데이터로 하여금 특정 분포로 수렴할 수 없도록 한다는 점, 따라서 단위근의 부재를 확인함으로써 시계열의 정상성을 확인한다는 점이다. 이 외 더 깊이 있는 설명은 ritvikmath 유튜브를 참고바란다. ritvikmath, Augmented Dickey-Fuller Test (ADF 검정) ritvikmath, Unit Roots (단위근)관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "파이썬을 활용한 애플 주가수익률 예측 분석 - (1). 데이터 확인 및 예측 안정성 확보", "url": "/posts/%ED%8C%8C%EC%9D%B4%EC%8D%AC%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-%EC%95%A0%ED%94%8C-%EC%A3%BC%EA%B0%80%EC%88%98%EC%9D%B5%EB%A5%A0-%EC%98%88%EC%B8%A1-%EB%B6%84%EC%84%9D-1-%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%99%95%EC%9D%B8-%EB%B0%8F-%EC%98%88%EC%B8%A1-%EC%95%88%EC%A0%95%EC%84%B1-%ED%99%95%EB%B3%B4/", "categories": "1. 기술, 서버, 데이터, 클라우드", "tags": "", "date": "2023-04-24 00:00:00 +0900", "snippet": "본 시리즈는 파이썬으로 시계열 자기상관 특성 및 마켓, 안전자산, 대체자산 등과의 동시간대 연관성을 분석하고 Apple Inc(AAPL) 주가를 예측한다.튜토리얼 작성을 위해 금융전략을 위한 머신러닝(한빛미디어), 실전 시계열 분석(한빛미디어) 외 야후파이낸스 및 FRED API 공식문서 등을 참고하였으며 작업 과정에서 추가로 참고하게 되는 자료들은 이후 각 편 내에 서술하도록 하겠다.1편에서는 간단히 데이터를 불러와 누락된 분포를 살피고, 시계열 기간을 동일하게 맞춘다. 그 다음, 예측에 필요한 데이터를 추출하기 위한 시계열 분석 작업을 간단히 수행하도록 한다.Step 1. Import Packagesnumpy와 pandas를 포함해 seaborn, matplotlib은 데이터 분석을 위해 언제나 기본적으로 설치해둔다. 관련하여 기타 세팅은 자유롭게 설정하면 되는데, ‘# MacOS - 한글폰트’ 부분은 맥 사용자가 Matplotlib 차트를 그렸을 때 한글이 깨지는 현상을 방지하는 부분이니 참고 바란다.예측 분석을 수행할 모델을 선택하기 위해서는 모델링 과정에서 여러 모델을 비교 분석하게 된다. 우리는 회귀 예측을 목표로 하며 이를 위해 선형 모델, 트리 기반 모델, 거리 기반 모델 및 앙상블 모델을 기본적으로 사용할 예정이다. 또 scikit-learn에서 제공하는 신경망 모형도 함께 불러왔으며 이는 이후 신경망에 ‘기억’의 개념을 포함하는 LSTM 모델과 비교하기 위함이다. LSTM은 다분히 시계열스러운 분석기법이므로 대표적인 시계열 모델 ARIMA와 함께 구현해 볼 것이다. 추후 PyTorch 혹은 Keras를 사용해 LSTM을 구현할 것인데, 자주 사용하는 패키지가 있다면 미리 불러와도 좋다.Step 2. Load Datasetstickers_i는 동종 산업 내 연관자산의 티커명(좌측부터 애플, 테슬라, 마이크로소프트, 메타, 아마존), tickers_m은 시장 내 연관 인덱스 티커명(주석으로 표기)이다. 우리는 시계열 예측을 위해 애플 주가의 과거 데이터(자기상관성 활용)뿐만 아니라 동시간 대의 연관 자산 및 시장 움직임을 모두 포함한 횡/종단면 데이터를 통합 활용할 것이다.데이터를 살피면 의도대로 2010년 1월 4일부터 2022년 12월 30일까지 데이터가 동일하게 불러졌으나 META와 TSLA의 경우 앞쪽 값이 누락된 것을 확인할 수 있다. 이와 같이 상장일이 차이 나거나 데이터 공급사의 집계 문제로 각 자산별 시계열이 일치하지 않을 수 있는데, 그렇다면 해당 자산을 제외하거나 전체 자산을 해당 자산에 맞게 시계열을 일치시켜줘야 한다. 그 기준을 잡기 위해서는 누락 값의 분포를 확인할 필요가 있겠다.Step 3. Drop NA’s자산 그룹(df_i)의 수정종가(Adj Close) 데이터로 누락값을 확인해보자. META의 경우 약 18%, TSLA는 약 4%가 부족하다. 앞에서 우리는 2010년부터의 주가정보를 불러왔으나 사실 주가 예측에 10년 전 데이터를 사용하는 경우는 많지 않다. 시장 국면이 빠르게 달라지기 때문이다. 따라서 미리 충분한 데이터를 불러온 만큼 META의 기준에 맞게 18%가량의 과거 데이터를 제거해주자.META의 수정종가 기준 데이터가 발생한 첫 날짜는 2012년 5월 18일이다. 해당 날짜 인덱스를 기점으로 전체 데이터를 조정해주었다.마찬가지로 시장 인덱스(df_m)도 동일하게 시계열을 조정해주면 된다.Step 4. 예측 안정성 확보본 편의 핵심이자 가장 중요한 부분이다. 시계열 분석이 정상적으로 수행되기 위해서는 데이터셋이 안정적이어야 한다. 즉, 시간에 따라 분포(평균, 분산)가 달라지지 않아야 하는데 이는 시계열 데이터만의 특수성이자 분석을 위한 기본 가정이다.예를 들어보자. 만약 이번 주의 통계적 분포와 다음 주의 통계적 분포가 전혀 다르다면 이번 주의 분포를 통해 학습한 예측 모델이 다음 주의 예측에 정상적인 결과를 내놓을 리 없을 것이다. 그런데 주가 데이터의 경우 이러한 성향이 매우 짙다. 소위 주식시장은 우상향 한다고 하지 않는가? 이제 막 상장된 기술 기업은 상장 전 고평가 받은 것을 상장 이후 1~2년 이내 다 뱉어내는 패턴도 존재하고, J커브 같은 스타트업계 용어도 주식시장에서 매우 그럴싸하게 통용된다.이렇게 테슬라의 경우 2020년 이전과 이후는 전혀 다른 움직임을 보인다. 즉, 통계적 분포와 그 분포에 영향을 미치는 각종 내/외생 변수의 분포까지 달라졌을 것이며, 이는 코로나라는 시장충격이 국면을 완전히 바꿔버린 사례다.이번에는 더 미시적으로 살펴보자. 이번 한 주(4.17 ~ 4.22) 간 테슬라 주가 움직임은 어떤가? 이전 3일(4.18~4.20)의 주가와 이후 2일(4.21~4.22)의 주가가 같은 분포를 띈다고 할 수 있을까? 그렇다면 이전 3일(4.18~4.20)의 주가로 모델링한 예측 모델이 향후 2일(4.21~4.22)의 주가를 제대로 예측할 수 있을까? 절대 그렇지 않다. 모델은 말 그대로 데이터를 학습하고, 본 것만 기억한다. 즉, 패턴이 달라지면 모델도 달라져야 한다.그렇기 때문에 데이터의 안정성을 먼저 확보하는 것이 필요하다. 전 시계열에 걸쳐 분포를 동일하게 맞춰 과거에 만든 모델이 앞으로도 (일정 기간 동안) 사용될 수 있어야 하는 것이다.4-1. 기본 수익률 분포(1일 변화량)위 첫 번째 차트는 주가의 일간 변화량(어제 대비 오늘의 주가)을 보여준다. 기본적으로 주가에는 복리 효과가 내재되어 있다. 즉 지수적인 상승이 발생하는데, 이에 따라 단순히 어제와 오늘의 주가 차이를 늘어뜨려놓으면 점점 그 진폭이 커진다. 예를 들어 주가가 20달러일 때와 200달러일 때 1% 변화량은 각각 0.2달러, 2달러로 10배가 차이 난다. 특히 애플의 경우에 정석적으로 주가가 우상향 한 케이스이므로 이런 효과가 여실히 드러나는 것이다. 그래서 파이썬을 사용한다면 pct_change를 통해 단순 변화량이 아닌 상대적 비율로써 변화량(%)을 확보할 수 있으나 편의상 log 처리하는 것이 일반적이다. log 처리를 한다는 것의 기술적 맥락은 주가 데이터 분석을 위한 로그 변환의 의미를 참고하길 바란다.따라서 이러한 데이터를 그대로 사용하긴 어렵다. 두 번째, 세 번째 차트를 보면 각각 월간 평균 및 표준편차 값의 움직임을 확인할 수 있는데, 시간에 따라 이 분포 특성이 크게 달라진다.4-2. 로그 수익률 1차 차분 분포(로그 변환 + 1일 변화량)다음은 로그 변환한 가격 변화량(수익률)의 분포다. 앞서 언급한 것처럼 로그 변환 시 각 시점의 상대적 변화량(%)과 거의 유사한 값을 확보할 수 있으며, 분포 통계가 어느 정도 일정해진 것을 확인할 수 있다.4-3. 로그 수익률 5차 차분 분포(로그 변환 + 5일 변화량)여기서 조금 더 욕심을 낸다면 위와 같이 차분 수를 늘려 분포를 더 안정적으로 만들 수 있다. 차분 횟수는 자기상관성을 제거하기 위한 시계열상 최소 거리와 관련된다. 그렇다고 차분 횟수를 데이터 특성과 무관하게 계속해서 높이면 자연히 정보손실이 발생한다. 내일 주가가 얼마나 변화할지 예측하기 위해 오늘의 주가, 지난주 주가, 지난달 주가 변화량을 살피는 것을 생각해 보자. 뒤로 갈수록 점점 그 관련성이 떨어진다는 것을 쉽게 짐작할 수 있다. 다만, 주가의 경우 요일 특성이 존재하고 매주 금요일의 주가, 그리고 매주 월요일의 주가 변화량은 어느 정도 유사한 분포를 띌 것이다.이러한 것들을 감으로 판단하기보다 ADF Test(단위근 검정)나 자기상관성 검정 등을 통해 실시하면 좋다. 따라서 본 편에 더해 인사이트를 얻고자 한다면 앞서 확인한 3가지 분포에 대해 정상성을 통계적으로 확인해보길 바란다. 우리는 앞서 미리 statsmodels에서 제공하는 adfuller 패키지를 불러왔다. 해당 패키지를 활용해 단위근 검정을 실시하고 각각의 분포가 정상성을 확보하는지 확인해보자.참고로 단위근 검정 결과를 해석하기 위해서는 약정상성, 강정상성에 대해 미리 알아둘 필요가 있는데, ADF Test는 약정상성을 만족하는가에 대한 검정 결과를 보여준다. 주의할 점은 강정상성이 약정상성을 반드시 포함하지도 않고, 약정상성이라 해서 그 정도가 약하다는 의미도 아니다. 다만, “데이터가 약정상성을 확보했다면, 최소한 모델이 그 분포를 학습하고 이해하기에 적절하다.” 정도로 그 의미를 이해하도록 하자.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "콜옵션/풋옵션 기초 개념 + 내가격(ITM), 등가격(ATM), 외가격(OTM)", "url": "/posts/%EC%BD%9C%EC%98%B5%EC%85%98-%ED%92%8B%EC%98%B5%EC%85%98-%EA%B8%B0%EC%B4%88-%EA%B0%9C%EB%85%90-%EB%82%B4%EA%B0%80%EA%B2%A9-ITM-%EB%93%B1%EA%B0%80%EA%B2%A9-ATM-%EC%99%B8%EA%B0%80%EA%B2%A9-OTM/", "categories": "2. 도메인, 금융", "tags": "", "date": "2023-04-20 00:00:00 +0900", "snippet": "콜옵션은 기초자산을 시장가보다 싸게 매수할 수 있는 권리를 의미한다. (*기초자산 : 옵션 등 파생상품의 기초 혹은 근거가 되는 자산으로 주식, 채권, 통화, 금리 등의 금융상품뿐만 아니라 농축산물, 실물상품까지 포함한다.)권리라 함은 추후 행사하거나 행사하지 않을 수 있다는 것이며 권리를 ‘매입’한다는 점에서 매입에 대한 가격, 즉 프리미엄이 붙는다. 이에 따라 콜옵션 매수 혹은 매도에 대한 이익과 손실 범위는 다음과 같이 나타난다.콜옵션 매수자는 x축 기초자산 가격의 변동(상승시 우측 방향 이동)에 따라 옵션 권리(기초자산을 행사가에 매수할 권리)를 행사하거나 행사하지 않을 수 있다. 콜옵션은 매수 포지션상 외가격(좌측)에서 거래가 이루어지고, 자산 가격이 증가하여 행사가와 일치한 상태인 등가격, 이후 내가격(우측)으로 진입할 때 옵션을 행사하게 된다.이 때, 콜옵션 매수 당시 프리미엄을 지불했기 때문에 손익분기점에 도달하기 위해서는 내가격으로 진입한 이후에도 자산가격이 더 상승해야 한다. 일반적으로 우상향 하는 주식시장을 가정하고, 주가지수를 기초자산으로 하는 콜옵션을 매수했다면 자산가격 상승에 따라 무한정 이익은 커지게 된다. 물론, 옵션은 만기가 존재하고 이러한 만기가 다가옴에 따라 프리미엄이 줄어드는데, 이것을 시간가치라 한다. 즉, 옵션의 가격 혹은 프리미엄은 기초자산의 가격과 행사가의 차액에 시간가치가 더해져 계산된다.콜옵션과 반대로 풋옵션은 시장가보다 비싸게 팔 권리를 의미한다. 작동 원리는 콜옵션과 동일하며, 그림을 통해 확인할 수 있다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "자산 내재가치평가 기본 모형 수식 정리 - 채권, 우선주, 보통주, 배당성장모형", "url": "/posts/%EC%9E%90%EC%82%B0-%EB%82%B4%EC%9E%AC%EA%B0%80%EC%B9%98%ED%8F%89%EA%B0%80-%EA%B8%B0%EB%B3%B8-%EB%AA%A8%ED%98%95-%EC%88%98%EC%8B%9D-%EC%A0%95%EB%A6%AC-%EC%B1%84%EA%B6%8C-%EC%9A%B0%EC%84%A0%EC%A3%BC-%EB%B3%B4%ED%86%B5%EC%A3%BC-%EB%B0%B0%EB%8B%B9%EC%84%B1%EC%9E%A5%EB%AA%A8%ED%98%95/", "categories": "1. 기술, 서버, 데이터, 클라우드", "tags": "", "date": "2023-04-19 00:00:00 +0900", "snippet": "먼저, 자산 가치평가를 위한 기본 모형은 다음과 같다. V : 일정 기간의 자산 가치(Value) CF : 일정 기간의 현금 흐름(Cashflow) k : 요구수익률(=할인율)현재 자산의 가치(V)는 미래 시점 전체 현금 흐름에 각 기간별 할인율(복리)을 적용한 값과 같다. 해당 모형을 통해 채권, 주식에 대한 가치평가를 실시할 수 있다.1. 채권 가치평가 모형채권은 만기가 없는 채권인 영구채(Perpetual bond)와 일반 채권으로 구분하여 산식을 적용한다. 기본 채권 가치평가 모형은 다음과 같다.영구채는 만기가 없기 때문에(연장 가능) 이자를 무한히 지급한다는 가정을 내포한다. 이에 따라 채권 산식에서 만기 원금 수령에 대한 항이 소거되고, n은 무한대로 설정한다.그리고 무한등비급수 공식에 따라 P = I/k로 정리된다.참고로, 무한등비급수 공식을 전개하면 다음과 같다. 결과는 동일하다.2. 주식 가치평가 모형우선주의 경우 채권 이자처럼 주가의 일정 % 비율로 배당이 고정되어 있다. 또한 그 특성상 회사 매각 시 청산 우선권이 있는 형태로, 만기가 존재하지 않는다고 가정하는 영구채와 동일한 가치평가 모형을 가진다. (P=D/k, D=배당금, k=요구수익률)보통주는 다르다. 기업이 재투자를 목적으로 배당금 일부를 유보(유보금, 유보율)할 수 있기 때문에 배당금이 고정적이지 않으며, 우선주와 달리 매매차익실현을 목적으로 보유한다는 점에서 자본이득 및 손실(원금 손익)을 함께 고려해야 한다.여기서 미래 자본이득 및 손실(Expected P)을 예측하는 것은 어려우므로 다음과 같이 배당 성장만을 고려해 가치평가를 수행하는 산식을 배당성장모형(=배당할인모형)이라 한다.단, 이렇게 산식을 간단히 하였을 때 보통주는 우선주나 채권의 영구채와 동일하게 평가된다. 따라서 배당성장모형을 크게 2개 유형(무성장 모형, 항상성장모형)으로 나눠 정의한다.여기서 무성장 모형은 우선주와 동일하다. 배당이 성장하지 않고 고정되어 있으며, 자본회수를 고려하지 않기 때문에 산식은 P=D/k가 된다 반면, 항상성장모형(Constant Growth Model)은 미래 배당금이 일정 비율(g)로 성장한다고 가정하고 산식을 다음과 같이 수정한다.항상성장모형은 유동성이 높은 보통주의 적정주가를 추정한다는 점에서 중요한 의의를 지니며, 요구수익률(k), 배당성장률(g) 각각을 구하는 산식도 유용하게 활용된다. 특히 요구수익률(k)은 자본자산가격결정모형(CAPM), 배당성장률(g)은 자기자본이익률(ROE)에 따라 재정의될 수 있다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "기업 상대가치평가를 위한 핵심 주가배수모형 4가지 용어 정리 - PER, PBR, PSR, PCR", "url": "/posts/%EA%B8%B0%EC%97%85-%EC%83%81%EB%8C%80%EA%B0%80%EC%B9%98%ED%8F%89%EA%B0%80%EB%A5%BC-%EC%9C%84%ED%95%9C-%ED%95%B5%EC%8B%AC-%EC%A3%BC%EA%B0%80%EB%B0%B0%EC%88%98%EB%AA%A8%ED%98%95-4%EA%B0%80%EC%A7%80-%EC%9A%A9%EC%96%B4-%EC%A0%95%EB%A6%AC-PER-PBR-PSR/", "categories": "1. 기술, 서버, 데이터, 클라우드", "tags": "", "date": "2023-04-19 00:00:00 +0900", "snippet": " PER(Price to Earning Ratio) : 주가수익비율 PBR(Price to Book Ratio) : 주가순자산비율 PSR(Price to Sales Ratio, Price Selling Ratio) : 주가매출액비율 PCR(Price to Cashflow Ratio) : 주가현금흐름비율여기서 P(Price)는 주가, 즉 시가총액을 발행주식 수로 나눈 한 주당 가격이다. 따라서 붉은 글씨로 표기된 E, B, S, C는 모두 주당 Earning, 주당 Book, 주당 Sales, 주당 Cashflow가 되어야 각각의 비율이 성립한다.먼저 E, B, S, C에 대한 각각의 기본 정의는 다음과 같다. E(Earning) : 당기순이익 B(Book Value) : 순자산(=자기자본=출자지분) S(Sales) : 매출액 C(Cashflow) : 현금흐름이를 발행주식수로 나눈 것을 다음과 같이 부른다. EPS(Earning Per Share) : 주당 순이익 BPS(Book Value Per Share) : 주당 장부가치(*회사 설립 시점의 자기자본 Capital이 아닌, 현시점 기준 부채 청산 후 남은 순자산) SPS(Sales Per Share) : 주당 매출액 CPS(Cashflow Per Share) : 주당 현금흐름이제 다시 주가배수모형으로 돌아와서 수식을 보면,’~~ 대비 주가가 얼마나 비싼가’로 생각해 볼 수 있다. PER(Price to Earning Ratio) : P/EPS(주당 순이익 대비 주가, 한 주로 발생하는 순이익 대비 주가가 얼마나 비싼가) PBR(Price to Book Ratio) : P/BPS(주당 장부가치 대비 주가, 한 주의 실제 장부가치 대비 주가가 얼마나 비싼가) PSR(Price to Sales Ratio) : P/SPS(주당 매출액 대비 주가, 한 주로 발생하는 매출 대비 주가가 얼마나 비싼가) PCR(Price to Cashflow Ratio) : P/CPS(주당 현금흐름 대비 주가, 한 주로 발생하는 현금흐름 대비 주가가 얼마나 비싼가)관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "주식 차트만 보는 기술적 분석의 무용론", "url": "/posts/%EC%A3%BC%EC%8B%9D-%EC%B0%A8%ED%8A%B8%EB%A7%8C-%EB%B3%B4%EB%8A%94-%EA%B8%B0%EC%88%A0%EC%A0%81-%EB%B6%84%EC%84%9D%EC%9D%98-%EB%AC%B4%EC%9A%A9%EB%A1%A0/", "categories": "1. 기술, 서버, 데이터, 클라우드", "tags": "", "date": "2023-04-14 00:00:00 +0900", "snippet": "아래 차트를 보면 어떤 생각이 드는가? 전고점을 뚫으면 우상향 할 것이고, 뚫지 못하면 박스권이 형성될 것이라 믿는가?그렇다면 이 그래프는 어떤가? 완전히 추세가 꺾여 가망이 없어 보이는가?안타깝게도 둘 다 정규분포로부터 발생시킨 완벽한 무작위 데이터다. 즉, 차트만 보고 우상향 한다, 박스권이다, 추세가 꺾였다 등의 기술적 분석은 아무 근거가 없다는 것이다.정규분포로부터 발생시킨 무작위 데이터라 함은 아래와 같이 일간 수익률을 평균이 0, 분산이 1인 랜덤한 숫자를 누적해서 쌓아 올린 것을 의미한다.이를 연결한 것이 총 500일 치의 일간 수익률 분포다.그리고 이것을 누적해서 쌓아 올리면(cumulative sum) 위에서 봤던 500일간의 누적 수익률, 즉 약 2년 간의 주식차트를 로그변환시킨 것과 동일한 모양새가 나온다.(주식차트를 로그변환하면 복리효과를 제거하여 시간에 따라 증가하는 비선형적 추이를 선형적으로 바꿔준다.)이 차트는 아래처럼 무수히 많은 랜덤 시계열 중 하나로 충분히 발생할 수 있으며, 주식시장의 시계열 데이터(로그-차분한 수익률)는 정규분포와 흡사(+약간의 왜도와 첨도)하다는 점에서 과거의 특정한 패턴이 앞으로 발생한다고 해서 이것이 같은 의미나 맥락을 지닌다고 볼 수 없다. 즉, 패턴은 랜덤한 확률 분포로 발생 가능하다.그러니 주식 그래프에 시장 심리, 기업 내재가치, 상대가치, 거시 사이클 등 모든 정보가 담겨 있다고 해서 차트만 봐도 오르내림을 예측할 수 있다고 믿어선 안 된다. 중요한 것은 화려한 차트 기법이 아니라 차트를 만들어내고 있는 수많은 변수와 지표들이다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "주가 데이터 분석을 위한 로그 변환의 의미, ln(1+return)과 return의 실질적 근사(+파이썬 코드)", "url": "/posts/%EC%A3%BC%EA%B0%80-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%B6%84%EC%84%9D%EC%9D%84-%EC%9C%84%ED%95%9C-%EB%A1%9C%EA%B7%B8-%EB%B3%80%ED%99%98%EC%9D%98-%EC%9D%98%EB%AF%B8-ln-1-return-%EA%B3%BC-return%EC%9D%98-%EC%8B%A4%EC%A7%88%EC%A0%81/", "categories": "1. 기술, 서버, 데이터, 클라우드", "tags": "", "date": "2023-04-06 00:00:00 +0900", "snippet": "주가 데이터를 그대로 분석하기에는 여러 가지 제약이 있다. 단일 주가를 분석할 때에는 그 자체로 정상성이 확보되어 있지 않다는 점, 여러 주가를 비교분석할 때에는 각 종목마다 분포(스케일)가 다르다는 점이 대표적이다.따라서 일반적으로 금융 시장 매크로를 분석하거나 머신러닝 모델을 구축할 때 price가 아닌 return을 활용하는 경우가 많다. 더 나아가 논문 수식이나 자료를 찾다 보면 단순히 net return(이하 return)을 쓰기보다 로그를 취한 ln(1+return)이 빈번하게 등장한다.직관적으로 ln(1+return)은 return과 (거의) 같다. 정확히 말하면 로그 수익률 ln(1+return)은 연속복리 개념을 가정하므로 실제로는 수익률 단위가 분, 초, 밀리세컨드 이하로 내려가야만 극한값으로 수렴해 일치하게 된다. 그러나 일반적인 상황에서는 수익률을 최소 일단위 이상으로 계산하므로 현실에서 같을 수 없다. 따라서 여타 경제학 모형이 그렇듯 현실 적용을 위해정밀성보다 편의성을 챙긴 수식이라고 이해하는 편이 좋다. 둘의 차이를 따져서 얻는 효용보다 약간의 오차를 감수하고 로그를 사용함으로써 얻는 실익이 더 크기 때문이다.일반적으로 주가는 한자리 수 단위로 수익률(%)이 움직이고, 더군다나 국내의 경우 +/- 30%까지 상승/하락폭에 제약이 있다. 아래 이미지의 좌측 그래프는 -0.99~+0.99 까지의 x(return)에 대해 1을 더하고 자연로그를 취한 ln(1+x)를 보여주고, 우측 그래프는 두 축에 대해 -0.3~+0.3 구간을 확대한 것이다.-0.3~+0.3 구간을 보면 알 수 있듯이 일반적인 주가 움직임에서 return과 ln(1+return)의 차이는 극히 적다. 특히 절대값 return이 감소할수록 그 차이는 거의 사라진다.앞서 log return이 정밀성보다 편의성을 챙긴 수식이라 하였다. 이는 복리계산과 연율화 등을 위해 숱하게 반복 계산해야 하는 (1+return)에 대한 승법 연산이 경우에 따라 복잡하고 번거롭기 때문인데, 간단히 log를 취해줌으로써 가법 연산으로 변환이 가능하다. 즉, 기간별 수익률을 구한다고 가정했을 때, (1+return_1)x(1+return_2)x…..x(1+return_t-1)x(1+return_t) 방식의 승법 모형을 sum(r_1, r_2,… , r_t) 방식의 가법 모형으로 변환할 수 있다는 의미다.(여기서 r은 로그 리턴 값)실제 x(=return), y(=ln(1+x)) 값과 그 차이를 비교해보면 다음과 같다.이외에도 로그변환은 우측으로 쏠린 수익률에 대한 확률분포를 0%을 중심으로 재조정해주는 효과가 있다. 즉, 10% 상승 후 10% 하락하는 것과 10% 하락 후 10% 상승하는 것을 동일하게 취급하여 복리에 의한 negative 왜도(skewness) 효과를 상쇄시켜주는 것이다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "TF-IDF 행렬의 특이값 분해를 통한 LSA(Latent Semantic Analysis)의 구현과 빈도 기반 토픽 모델의 한계", "url": "/posts/TF-IDF-%ED%96%89%EB%A0%AC%EC%9D%98-%ED%8A%B9%EC%9D%B4%EA%B0%92-%EB%B6%84%ED%95%B4%EB%A5%BC-%ED%86%B5%ED%95%9C-LSA-Latent-Semantic-Analysis/", "categories": "3. 튜토리얼, 자연어 처리 및 텍스트 분석 방법론", "tags": "", "date": "2023-02-17 00:00:00 +0900", "snippet": "Step 1. 토픽모델링이란토픽모델링은 특정 문서의 확률적 카테고리를 나누는 비지도학습 방법론이며 기본적인 컨셉은 다음과 같다. 문서는 단어의 조합이다. 문서 내 함께 등장하는 단어는 서로 연관성이 있다. 모든 문서에서 자주 등장하는 단어는 특수한 의미를 내포하지 않는다. 반면, 그렇지 않은 단어는 특수한 의미를 내포한다. 즉, 모든 문서에서 자주 등장하지 않으면서 특정 단어들과 함께 등장하는 단어들은 서로 유사한 의미를 내포한다. 그러므로 단어의 조합인 문서는 의미가 있는 단어들의 비중에 따라 카테고리(Topic)가 결정된다.우리는 이러한 컨셉의 프로세스에 따라 1.문서를 단어 조합으로 가공하고, 2~5.문서 내 각 단어들의 의미를 부여한 다음, 6.문서의 토픽을 결정해보도록 하자.Step 2. 분석 대상(문서) 확인분석 대상은 HuffPost의 뉴스데이터이며, 아래 캐글 링크에서 다운로드 받을 수 있다. https://www.kaggle.com/datasets/rmisra/news-category-dataset[News Category DatasetIdentify the type of news based on headlines and short descriptionswww.kaggle.com](https://www.kaggle.com/datasets/rmisra/news-category-dataset)데이터(뉴스 정보)는 총 20만개, 카테고리는 42개이며 그 중 Politics가 가장 많은 데이터를 차지한다.우리는 이렇게 이미 잘 나눠진 카테고리를 다시 한번 토픽모델링을 통해 세부 카테고리로 나눠볼 것이다. 분석하고자 하는 뉴스 카테고리를 하나 선정한 다음 요약 내용을 담고 있는 short_description 부분만 추출한다.빈 값이 있다면 지워주자.Step 3. 빈도 측면에서의 단어의 의미 부여(TF-IDF)단어에 의미를 부여하는 방법은 다양하다. 어떤 방법이든 의미를 가지려면 단어를 숫자로 바꿔줘야 하며 이는 각 문서(short description)가 단어 벡터로 표현됨을 의미한다.변환 방법은 대표적으로 Bag of Words, Word Embedding 방식이 있으며 우리는 Bag of Words, 그중에서도 TF-IDF(Term Frequency-Inverse Document Frequency) 방식을 사용한다. TF-IDF는 각 문장에서 출현하는 특정 단어 빈도인 TF(Term Frequency), 그리고 전체 문장에서 출현하는 특정 단어의 빈도인 IDF(Inverse Document Frequency)의 곱연산으로 정의되며, 이를 활용해 Step 1에서 소개한 토픽모델링 프로세스 중 2~5번 항목을 수행한다.Word Embedding 방식이 각 단어별 잠재벡터(의미)를 즉시 부여한다면, TF-IDF는 단어의 출현 빈도와 비중을 토대로 문서의 잠재벡터를 추정한다. 따라서 엄밀히 말하면 TF-IDF는 각 단어가 그 자체로 의미를 지닐 수는 없으며 문장 내에서 혹은 토픽 내에서만 확률적으로 의미를 가진다.3-1. TFTF는 단순히 단어의 빈도를 의미한다. 따라서 CounterVectorizer() 함수 실행 결과값으로 함께 확인할 수 있다.각 행(인덱스)은 문서를, 열(컬럼)은 단어를 의미한다. 각 데이터는 문서에 등장한 단어의 빈도가 되겠다.여기서 연산량을 줄이기 위해 데이터를 일부 제거할텐데, 단어를 너무 적게 가지고 있는 문서(행)와 전체 문서상에 너무 적게 등장하는 단어(열)는 삭제해 주자. 물론, TF-IDF의 목적이 단어의 특수성과 보편성을 계산하기 위함인 것을 고려하면 원데이터로 분석하는 것이 가장 좋다. 단어를 너무 적게 가지고 있는 문서문장의 길이가 긴 것은 정보를 많이 포함하므로 문제되지 않는다. 단어 2개 이하로 구성된 너무 짧은 문장은 삭제한다. 전체 문서상에 너무 적게 등장하는 단어잘 보이지 않으니 x축을 확대해서 살펴보자.문서 1개에만 존재하는 단어 비중이 매우 크기 때문에 제거하기 어렵다. 따라서 최소 1개 문서에는 출현하는 단어는 모두 단어 사전에 포함하는 것이 좋다.(단어는 사실 앞에서 너무 짧은 문장을 제거했을 때 발생된 데이터다. 따라서 이는 제거해주는 것이 맞다.) 그러나 실습의 편의를 위해 최소 2개의 문서에는 출현하는 단어만 단어 사전에 포함하도록 하자. 연산 속도를 높이기 위함이다.마지막으로 컬럼 중에 숫자로만 구성된 것들도 제거해주자.이렇게 TF-IDF 중 TF(문서별 단어 빈도) 구성을 완료했다.3-2. IDFIDF는 DF의 inverse값이다. DF는 특정 단어가 출현하는 문서의 수를 의미한다. 따라서 전체 빈도의 합계와는 차이가 있다.이러한 특정 단어가 출현하는 문장의 수, 즉 DF의 역수를 취해주면 IDF가 되는데 이때의 역수는 단순히 1/DF를 의미하는 것은 아니다.총 문서의 수 + 1을특정 단어를 포함하는 문서의 수 + 1로 나누고 로그를 취한 다음 다시 1을 더해준다.이렇게 하는 이유는 다음과 같다. 분자 혹은 분모가 0이 되지 않도록 총 문서의 수, 특정 단어를 포함하는 문서의 수에 각각 1을 더한다. 바로 위 셀에서 처럼 특정 단어를 포함하는 문서의 수가 총 문서의 수 대비 매우 적을 수 있다. 따라서 로그를 취해 분포가 너무 커지지 않도록 스케일을 줄여준다. 또 반대로 만약 대부분의 문서에 특정 단어가 포함되는 경우 자연로그를 취한 값은 0에 수렴할 것이다. IDF는 TF에 곱해지는 가중치로써 역할을 해줘야 하기 때문에 1을 더해준다.이렇게 해당 문서에 등장한 특정 단어가 희소할 경우 가중치를 크게, 빈번할 경우 가중치를 낮게 반영한다.3-3. TF-IDF앞에서 구한 tf와 idf를 곱해주면 TF-IDF가 된다. 즉, 각 문서별 특정 단어의 빈도가 TF인데 여기에 해당 단어가 얼마나 특수한가에 대한 정보(IDF)를 가중치로 곱해주는 것이다.분포를 보면 대부분의 값이 0으로 최댓값과 차이가 있다. 이는 너무 넓은 범위에 sparse 한 데이터가 찍혀있는 것이므로 데이터를 한번 더 밀집 공간 내 압축해 줄 필요가 있다.따라서 각 단어의 고유속성, 즉 단어 벡터크기(원점으로부터의 길이)를 나눠주자. 예를 들어 첫 번째 단어의 경우 벡터크기는 다음과 같다.이를 전체에 적용하여 모든 단어의 벡터 길이를 구하고, 이렇게 구한 벡터 길이로 단어별 빈도데이터를 나눠주면 분포는 다음과 같이 바뀐다.Step 4. 토픽 추출사실, 토픽모델링의 경우 LDA(Latent Dirichlet Allocation)가 대표적이지만 본 실습에서는 빈도 기반의 TF-IDF 행렬에서 어떻게 잠재의미를 추출해 낼 수 있는가?를 분석해보기 위해 LSA(Latent Semantic Analysis) 방법론을 사용하고자 한다.LSA는 SVD 행렬분해(특이값 분해)를 통해 추출한 대각행렬(특이값 행렬)에서 정보량이 많은(값이 큰) 부분을 추출해 최소한의 차원(벡터)으로 토픽을 구성하는 방법론이다. 따라서 SVD를 1차 수행한 후 정보량이 많은 부분행렬을 추출하는 방식으로 LSA를 구현해보도록 하자.먼저, SVD를 수행하기 전에 np.inf(무한대), 혹은 np.nan(계산불가) 값이 있는지 확인해야 한다. 넘파이 연산 시 실수를 0으로 나누면 inf 값을, 0을 0으로 나누면 NaN값을 반환하게 되는데, 앞서 충분히 많은 0의 극한값들을 서로 연산했기 때문에 문제 가능성이 있다.NaN값들이 확인되었다. 해당 행을 제거한다.4-1. SVDSVD(Singular Value Decomposition)는 A=(M x N) 행렬을 U=(M x M), S=(M x N), VT=(N x N) 3개의 행렬로 분해한다.여기서 U는 직교행렬, VT 직교행렬을 transpose 시킨 전치행렬, S는 직사각 대각행렬이다. 이때, A=U*S*VT에서 양 변에 V를 곱하면 AV=US가 된다. 이 식의 의미는 직교행렬 V를 A행렬로 선형변환했을 때, 각도는 동일하고 길이만 다른 직교행렬 U*S가 된다는 것이다. 이러한 작용의 의미, 혹은 과정이나 맥락을 이해하기 위해서는 먼저 선형대수의 관점에서 직교행렬 및 단위행렬의 성질을 이해할 필요가 있다. 그러나 우리는 결과적으로 이 식을 구함으로써 어떤 이점이 있는가에 초점을 맞추도록 하자.A=USVT의 식으로 다시 돌아와 보면, S가 어떤 값을 가지든 U의 행(M)과 VT의 열(N)이 A의 그것(M x N)과 동일해야 함을 알 수 있다. 만약 S=(1 x 1)이라면, U=(M x 1), VT=(1 x N)이 되어야 하고, S=(2 x 100)이라면, U=(M x 2), VT=(100 x N)이 되어야 한다. 여기서 불변하는 것은 M과 N이다. 그리고 A가 (문서 x 단어)의 행렬이므로 M=문서, N=단어가 된다. 문서는 단어벡터로 이루어져 있었다. 즉, 단어-&amp;gt;문서로 즉시 이어지던 관계를 단어-&amp;gt;[잠재벡터]-&amp;gt;문서로 한 단계를 더 넣은 것으로 이해해 볼 수 있다. 이 잠재벡터가 바로 토픽이며, 단어 조합을 문장으로 만들어주는 어떠한 정보를 내포하는 것이다.결과적으로 S는 토픽 정보를 가지고 있는데 그 크기는 (1 x 1)부터 (M x N)까지의 자유도를 가진다. 이때, 자연스럽게도 S가 (1 x 1) 일 때나 (M x N) 일 때나 모두 U*S*VT는 A의 형태는 갖춰야 하기 때문에 S가 아무리 작아도(S=(1 x 1)) 최소한의 정보는 가지고 있을 것이라 생각해 볼 수 있다. 실제로 S의 대각 원소의 값은 좌상단부터 우하단까지 내려갈수록 작아진다. 즉, S의 대각 원소 첫 번째 값이 가장 중요한 정보를 가지고 있고, 가장 마지막 값이 가장 작은 정보를 가지는 것이다. 따라서 우리는 SVD를 수행해 S를 구함으로써 A가 구성되기 위해 가장 중요한 정보부터 가장 중요하지 않은 정보까지의 값을 확인할 수 있게 되는 것이다.SVD는 넘파이 선형대수 패키지를 통해 간단히 수행할 수 있다.4-1. S잠재벡터 s를 먼저 확인해 보자. svd() 함수를 사용하면 s는 벡터로 추출된다. 이 벡터의 첫 번째 값이 가장 크고, 마지막 값으로 갈수록 가장 작아지는 것을 확인할 수 있다. 이것이 각 토픽의 정보량이 된다.차트에서 붉은 영역에 속한 점들이 가장 설명력이 높은 상위 3개 토픽을 표시하고 있다. 이 3개 토픽만 활용해 문서의 주제를 분석한다.참고로 잠재벡터를 다음과 같이 대각행렬로 표현해 줄 수도 있다.4-2. UU는 문서가 전체 토픽마다 일치하는 확률을 담고 있다. 특이값 s의 크기 3에 맞춰 열을 잘라주면 가장 설명력(정보량)이 높은 3개 토픽에 대한 확률만 추출된다.U를 데이터프레임으로 변환해줄 텐데, index를 기존 문서 인덱스에 맞게 지정해주는 것이 중요하다.아래에서 시각화를 해줄텐데, X, Y, Z 좌표 범위를 0~1로 동일하게 맞추기 위해 스케일링을 미리 수행해주겠다.4-3. V마지막으로 V는 각 단어가 각각의 토픽마다 일치하는 확률을 담고 있다. VT를 특이값 s의 크기 3에 맞춰 행을 잘라주면 가장 설명력(정보량)이 높은 3개 토픽에 대한 확률만 추출되고 이를 Transpose 시키면 V를 구할 수 있다.V도 마찬가지로 시각화를 위해 스케일링을 진행하자.4-4. 각 토픽에 해당하는 확률 시각화docs는 펼쳐져있는 반면, words는 상대적으로 공간상 수렴해 있다. 이 현상을 잠시 짚어보자.Step 1로 돌아가서, 문서는 단어의 조합이라 했다. 각 단어는 동시 출현 빈도에 따라 연관성을 가지며 이러한 연관성이 결과적으로 토픽을 결정한다. 이렇게 결정된 토픽에 단어 조합으로 구성된 각 문서가 일치할 확률을 표현한다. 따라서 U와 V는 모두 토픽에 일치할 확률을 보여주지만 개념적으로 단어가 토픽의 정보량을 결정하고 그렇게 결정된 정보로 문서를 설명하기 때문에 words 대비 docs는 분포가 펼쳐질 수밖에 없다는 것이다.Step 5. 토픽모델링 결과 분석토픽모델링의 결과는 각 토픽별 단어의 확률분포, 문서의 확률분포가 되겠다. 즉, 특정 단어가 topic 1, 2, 3 각각에 해당할 확률이 얼마인지, 또 특정 문서가 topic 1, 2, 3 각각에 해당하는 확률이 얼마인지 확인할 수 있다. 우리가 앞서 뉴스 카테고리 1개를 선정 후, 카테고리 내 세부카테고리를 더 나눌 수 있는가? 에 관심을 가졌으며 토픽모델링 결과에 따라 그것의 여부를 확인하게 된다.5-1. 각 토픽별 단어 분포topic_1은 주로 무슬림계 범죄, 급진주의 테러리즘에 대한 내용으로 보인다. gauchistes(좌파), mais(이슬람 종교 협의회), islamistes(이슬람) 같은 단어들이 상단에 등장하고 있다.topic_2의 경우 남, 여, 아동 등 일반 시민 대상의 살인 범죄에 대한 내용으로 보이며 최상단에 year이라는 단어가 등장함으로 추측건대 연간 범죄율 등이 주축을 이룰 것이다.마지막으로 topic_3의 경우 police, officer, say 등이 상단에 위치한 것으로 보아 경찰의 행동이나 발언에 초점을 맞추고 있음을 알 수 있다.종합적으로 topic_1는 테러 같은 국가적 이슈, topic_2는 소규모 범죄 피해, topic_3은 치안 및 관련 보도를 다루고 있으며, topic_1에 비해 topic_2와 topic_3은 그 성향이 상대적으로 뚜렷하지는 않다. 각 토픽에 대해 단어별 확률 분포를 시각적으로 확인해 보자.topic_1의 경우 각 단어의 확률값이 전체적으로 매우 높은 반면, topic_2와 topic_3은 0.5 근처로 낮다. 이는 추출된 특이값 중 가장 앞에 위치해 있던 topic_1의 정보량이 가장 많기 때문이다.한편으로는 토픽모델링을 수행했을 때 이렇게 모든 단어의 topic_1, topic_2, topic_3에 대한 각 확률값이 유사한 경우 제대로 된 분석 결과가 나오지 않는데, 이는 빈도기반 토픽모델링의 근본적 한계이기도 하다. 마지막 결론에서 이 내용에 대해 다시 언급하겠다.5-2. 각 토픽별 문서 분포다음으로 각 토픽과 가장 일치할 확률이 높은 문서도 확인해 볼 수 있다.앞서 확인했을 때 topic_1은 테러와 같은 극단적인 범죄에 대한 토픽이었다. 위 문장 중 1번째는 밀수, 5번째는 폭탄테러에 대한 내용이며 나머지는 topic_1과 그다지 근접해 보이지는 않는다.이러한 문서들도 포함된 이유는 topic_1과 일치할 단어의 확률 분포가 매우 높기 때문인데, 그러한 단어들이 문장에 조금만 들어가더라도 문장 자체의 토픽 일치 확률이 크게 상승하게 된다. 따라서 topic_1의 경우 확률 분포상 상위에 있는 문서들이 아이러니하게도 topic_1과 맥락상 유사하지 않을 가능성도 높은 것이다.topic_2의 경우 상위 문서은 예상과 동일하게 일상에서 발생가능한 살인 범죄 등의 사건을 다루고 있다.topic_3도 역시 마찬가지로 앞서 토픽에 대해 정의한 것처럼 경찰의 발언 및 보도에 관한 내용을 담고 있다.토픽별로 각각의 문서가 속할 확률을 보여주고 있으며 왼쪽부터 순서대로 topic_1, topic_2, topic_3을 기준으로 확률분포를 정렬시켰다.문서의 기준으로 보면, 각 문서는 모두 3개 토픽에 포함될 확률을 각각 가지고 있는데, 약 2천 개의 문서는 모두 topic_1에 포함될 확률이 가장 높다.(가장 왼쪽 차트)이는 앞에서도 확인했지만 topic_1과 일치하는 단어가 뚜렷한(확률이 큰) 만큼 그 단어가 속한 문서도 topic_1에 속할 확률이 높아지는 것에 원인이 있다.5-3. 각 토픽별 주요 문서의 단어 분포마지막으로 각 토픽별로 일치 확률이 가장 높은 문서들에 어떤 단어들이 포함되어 있는지 확인해 본다. 이때, 단어 역시 각 토픽별 가장 높은 확률을 가진 단어들로 구성하여 동일한 단어들이 문서별로 또 어떻게 다르게 분포되어 있는가를 확인하고자 한다.topic_2와 topic_3의 경우 각각의 문서가 토픽에 대한 확률값이 높은 단어들 위주로 잘 포함하고 있는 반면 topic_1은 그렇지 않다. 모든 단어들이 topic_1에 포함될 확률이 1에 근사하기 때문이다.이렇게 문서별 단어 분포까지 확인해 보았다. 빈도기반 토픽모델링 LSA는 문장 내 단어의 동시 출현 빈도 및 특수성 가중치를 고려한 TF-IDF 행렬에 대해 특이값을 추출하는 방식으로 발전시켜 단어의 잠재 의미를 끌어내고자 시도했다는 점에서 의의가 있었다. 하지만 이 방법론의 결과로, 토픽 내부적으로는 연관관계가 높은 단어들이 서로 밀집할 수 있었으나 단어의 의미를 고려할 수 없다는 점에서 한계 또한 분명했다.사실, 본 분석에서는 연산 속도 제약상 단어 집합이 정규분포를 따라야 한다는 LSA의 중요한 기본 가정을 무시하고 편향이 있는 단어 분포를 사용했다. 그러나 이를 차치하더라도 LSA는 그 원리상 빈도 기반 연관성만 고려하지 각 단어별 잠재된 의미를 고려하지는 않기 때문에 단어별로 각 토픽에 속할 확률이 큰 폭에서 차이가 없을 가능성이 매우 높으며, 분석에서는 그 특성을 다소 극대화해 보였다. 이러한 한계가 문서의 주제 분류에 있어서 현실적인 사용에 많은 제약을 주게 된다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "단어의 의미를 고려한 문장 유사도 측정 방법 - 기저 벡터와 선형 변환", "url": "/posts/%EB%8B%A8%EC%96%B4%EC%9D%98-%EC%9D%98%EB%AF%B8%EB%A5%BC-%EA%B3%A0%EB%A0%A4%ED%95%9C-%EB%AC%B8%EC%9E%A5-%EC%9C%A0%EC%82%AC%EB%8F%84-%EC%B8%A1%EC%A0%95-%EB%B0%A9%EB%B2%95-%EA%B8%B0%EC%A0%80-%EB%B2%A1%ED%84%B0%EC%99%80-%EC%84%A0%ED%98%95-%EB%B3%80%ED%99%98/", "categories": "1. 기술, 서버, 데이터, 클라우드", "tags": "", "date": "2023-02-11 00:00:00 +0900", "snippet": "Step 1. 문장의 유사도를 구하는 방법 두 문서의 유사성은 어떻게 측정할 수 있을까?문장의 길이? 아니면 주어, 동사, 목적어 등의 문법 구조?그것보다는 얼마나 공통 ‘단어’를 많이 포함하고 있는가? 가 더 합리적으로 보인다. 실제로 수많은 전통적인 텍스트마이닝 방법론들은 이러한 단어 기반 유사도 측정 방식을 따르며, 현재 딥러닝, AI 시대에도 역시 문장 구조와 속성을 분석할 때 단어는 핵심 요소다.이렇게 단어를 기준으로 문장 유사도를 구하기 위해서는 단어를 숫자로 변환해 줄 필요가 있다.즉, 유사도 혹은 거리를 수학적으로 계산하기 위해 문장을 일종의 좌표평면 상에 놓을 수 있어야 하고문장이 좌표평면에 놓이기 위해서는 문장을 구성하고 있는 단어들을 스칼라 혹은 벡터값으로 변환해줘야 하는 것이다.따라서 문장의 유사도를 계산하는 과정은 다음과 같다. 단어를 숫자(스칼라 혹은 벡터)로 변환 각 문장을 벡터(단어)의 배열로 변환 문장 벡터간 유사도 계산Step 2. 빈도 기반 유사도 계산2-1. 문장별 단어 카운트먼저 단어를 숫자(스칼라 혹은 벡터)로 변환하는 것부터 시작해보자.단어를 숫자로 변환하려면 어떻게 해야 할까?가장 쉬운 방법은 각 문장에 단어가 몇 번 등장하는지 세는 것이다.먼저 사용할 패키지를 불러온다.샘플 문장은 다음과 같다. doc_1, doc_2, doc_3은 모두 cake라는 단어를 포함한다. doc_1과 doc_2는 먹는 케이크에 관한 내용이다. doc_3은 케이크라는 영화에 관한 내용이다.단어 수를 세기 위해 sklearn의 CounterVectorizer를 활용한다.CounterVectorizer에 인자로 stop_words를 줄 수 있다.stop_words는 제외할 단어들을 설정하는 것이며 ‘english’를 지정해주면 영어 문장에서 빈번하게 등장하는 This, That, What, a, is 등의 단어들을 제거해준다.이것의 효용은 문장의 특성을 강화시켜 주는 데에 있다.CounterVectorizer를 사용해 얻은 단어 메트릭스는 일종의 단어사전인데, 우리의 단어사전이 어떤 문장에서나 보이는 단어들도 모두 포함하게 되면 사전의 크기는 매우 커지고 주요 단어들의 정보량이 희석된다.따라서 공통적으로 어떤 문장에서나 보이는 단어들보다는 특정 문장의 특성을 잘 보여주는 단어들을 중심으로 사전을 구성하려는 것이다.단어사전이 가지게 된 단어는 총 4개다.doc_1, doc_2, doc_3 각각 포함하는 단어 빈도를 표시해주면 다음과 같다.2-2. 문장 벡터 시각화단어 빈도로 구성된 doc_1, doc_2, doc_3는 이제 각각 하나의 벡터로 생각해보자.그렇다면 해당 벡터는 4차원 공간에 표시할 수 있고, 4차원 좌표평면을 그리기는 어렵기 때문에 cake, movie, tasty 3개 단어를 x, y, z 축으로 해서 문장 벡터를 시각화해 보도록 하자.그전에, 각 문장 내용을 인덱싱 해서 쉽게 볼 수 있도록 Dictionary를 하나 만들어주자.이렇게 각 문장을 3차원 좌표평면상에 위치시켜보자.2-3. 문장 벡터 간 유사도 계산벡터간 유사도를 계산하기 위한 방법으로는 주로 점 간의 직경 거리를 계산하는 ‘유클리디안 거리’, 각 벡터간 코사인 각도를 비교하는 ‘코사인 유사도’가 대표적이다. 참고로 벡터변환을 하지 않고 문장 내 공통 단어의 비율(동시출현빈도)을 보는 ‘자카드 유사도’도 가벼운 추천시스템 등에 자주 쓰이나 여기서는 다루지 않겠다. 유클리디안 거리는 numpy에서 제공하는 선형대수를 위한 함수 linarg를 사용해 벡터 길이 즉 노름(norm)을 계산함으로써 얻을 수 있다. 다음으로 코사인 유사도는 두 벡터의 스칼라곱을 각 벡터 길이의 곱으로 나눠줌으로써 계산할 수 있다. np.dot(vector_1, vector_2)를 통해 스칼라곱을 구하고, np.linalg.norm(vector_1) * np.linalg.norm(vector_2)를 통해 각 벡터 길이의 곱을 구한다. 마지막으로 주의할 점은 유클리디언 거리와 동일하게 ‘벡터간 차이’를 기준으로 잡기 위하여 ‘1-유사도’를 계산여 ‘얼마나 유사하지 않은가’를 확인하도록 하겠다. 이를 편의상 이하 ‘코사인 거리’로 명명하도록 하자.doc_1과 doc_2, 그리고 doc_2와 doc_3은 유클리디안 거리가 서로 1.73으로 동일하며 코사인 거리 역시 0.37로 동일하다.다시 말해 doc_1과 doc_2은 케이크에 대해 이야기하고 있고, doc_3은 영화에 대한 문장인데도 불구하고 doc_2이 doc_1, doc_3과의 유사도가 같다는 것이다. 뭔가 문제가 있어 보인다.이 문제를 조금 더 극대화해보자.Step 3. 코사인 유사도의 효용그럼, 거리 기반 유사도(유클리디안)와 각도 기반 유사도(코사인) 방식은 계산 효용에 차이가 없는가?그렇지 않다. 만약 문장의 길이가 길어져 단어 빈도가 늘어난다면 유클리디안 방식은 문장의 유사도를 계산하는 데에 있어서 효용을 급격히 잃게 된다.3-1. 단어가 반복 등장하는 예예로, doc_1을 5번 반복해보자.그리고 앞에서 한 것과 동일하게 각 문장별 단어 빈도 테이블을 생성해주자.각 문장은 다음과 같다. doc_1이 다소 극단적인 예시처럼 보이지만 노래가사, 시, 광고문구 등을 떠올려보면 현실에서 문서 내에 특정 단어나 문장이 반복되는 경우는 매우 흔하다.3-2. 문장 벡터 시각화doc_1은 단어를 반복시켰고, doc_2와 doc_3은 그대로다. 좌표평면상에 3개의 문장을 표시해보자.3-3. 문장 벡터 간 유사도 계산이제 유클리디안 거리, 코사인 거리를 수치로 확인해보자. doc_1과 doc_2의 유클리디안 거리는 5.92, 코사인 거리는 0.37이다. doc_2와 doc_3의 유클리디안 거리는 1.73, 코사인 거리는 0.37이다.doc_1과 doc_2는 먹는 케이크에 대한 내용, doc_3은 영화 제목에 대한 내용임에도 doc_1의 문장이 길어짐으로써(정확히는 단어가 반복되면서) doc_2와의 직경 거리가 더 멀어졌다. 반대로 벡터의 길이가 아무리 길어졌어도 doc_1과 doc_2, 그리고 doc_2와 doc_3의 코사인 거리는 동일하게 보존되고 있다.이를 통해 유클리디안 거리는 문장 크기에 강한 영향을 받으며 코사인 거리는 문장 크기로부터 자유롭다는 것을 확인했다.Step 4. 선형 변환을 통한 의미상 유사도 계산다시 문장의 길이를 늘리기 전 상황으로 돌아가보자.doc_1과 doc_2의 토픽은 케이크, doc_2의 토픽은 영화다. 그렇다면 doc_1과 doc_2의 유사도는 doc_2와 doc_2의 유사도보다 높아야 한다. 즉, 거리가 더 짧아야 한다. 그러나 doc_1과 doc_2, 그리고 doc_2와 doc_3의 거리는 동일하다.(유클리디안, 코사인 방식 모두.)그렇다면 의미가 유사한 doc_1과 doc_2는 더 가깝게, doc_3은 조금 더 멀리 떨어뜨릴 순 없을까?방법은 좌표계를 바꾸는 것이다. 잠깐 생각해보면, 우리는 좌표평면의 축에 대해서 고려하지 않았다. 좌표평면 축은 각각 ‘movie’, ‘cake’, ‘tasty’의 빈도 수를 나타내며 이에 대해 암묵적으로 좌표계상 기저를 (0,0,1), (0,1,0), (1,0,0)으로 고정시킨 것이다.이 기저를 선형변환해서 좌표계를 찌그러뜨린다면 doc_1, doc_2, doc_3의 벡터도 기저에 따라 선형변환하며 doc_1과 doc_2는 가깝게, doc_3은 멀게 위치시킬 수 있다.4-1. 단어에 의미를 부여하는 워드임베딩선형변환을 하기 위해서는 기저벡터를 먼저 구해야 한다. 즉, 문장이 위치할 좌표평면상 축을 구성하는 ‘movie’, ‘cake’, ‘tasty’가 각각 어디에 위치하는지, 축의 위치를 잡는 것이다.이를 위해서는 ‘movie’, ‘cake’, ‘tasty’에 대한 벡터 값이 있어야 하는데, 이를 구하기 위해서 단어임베딩 모델을 사용할 수 있다. 단어임베딩은 위키피디아 같은 방대한 텍스트에서 문장들을 추출하고, 각 문장 내 단어의 위치관계를 학습해 각 단어마다의 의미를 벡터로 정의하는 방식이다. 앞서 우리가 간단히 빈도로 구성한 단어사전이 각 단어마다 정수 형태의 scalar 값을 가졌다면, 단어임베딩의 결과물인 밀집 단어사전은 각 단어가 수백 차원의 밀집벡터(정수가 아닌 -1에서 1 사이로 압축된 실수 형태)라고 생각하면 된다.이미 수많은 언어모델이 공개되어 있고 언어모델은 반드시 텍스트를 벡터로 변환하는 토크나이저를 포함하므로 이러한 토크나이저의 근간이 되는 학습된 단어임베딩 모델을 쉽게 가져다 사용할 수 있다. 참고로 단어임베딩 모델은 각 테스크에 특화되어 있는 것이 일반적이다. 예를 들어 위키피디아를 학습한 단어임베딩 모델이 가지고 있는 단어사전은 뉴스를 학습한 단어임베딩 모델이 가지고 있는 단어사전과 다르다.전통적으로 word2vec 모델이 많이 쓰이는데, word2vec 모델은 쪼갤 수 있는 최소 단위를 말 그대로 ‘단어’로 한다. 반면 페이스북에서 이를 더 작은 글자(text) 단위로 학습할 수 있도록 개량한 모델이 fasttext다. fasttext의 강점은 학습 모델이 ‘truelove’ 같은 단어를 본 적이 없다 하더라도 ‘true’, ‘love’를 각각 경험했다면 ‘truelove’를 ‘love’와 가까운 단어로 인식하고 계산할 수 있다는 것이다.본 실습에서도 fasttext를 사용하며, 영문 단어임베딩에 대표적으로 많이 쓰이는 ‘fasttext-wiki-news-subwords-300’ 모델을 빌려와 사용하도록 하겠다.지금부터 fasttext를 단어사전이라 생각하자. movie를 사전에서 검색해보면 다음과 같은 결과가 나온다.단어사전에서 꺼낸 movie는 300차원짜리 벡터다.이는 앞에서 우리가 movie에 어떠한 의미도 주지 않고 (0,1,0)의 3차원 기저벡터로 설정했던 것과 비교된다.movie와 비슷한 단어 10개를 다음과 같이 사전에서 검색할 수 있다.movie와 가장 비슷한 flim과의 유사도를 보면 0.876~이다. 이는 코사인 유사도인데, 앞에서 우리가 만든 코사인 거리 함수로 같은 값을 도출할 수 있다.코사인 거리 함수의 출력값은 ‘1-유사도’ 이므로 아래 결괏값에서 1-dst를 해주면 0.876~이 나온다는 것을 이해할 수 있다.물론 fasttext를 사용해 거리도 계산할 수 있다. 마찬가지로 결괏값은 1-유사도다.이제 우리 좌표평면상 기저벡터인 cake, tasty, movie의 서로간 거리를 살펴보자.기존에 임의로 설정한 기저를 기준으로 하면 거리는 모두 동일해야 하지만 현재 임베딩된 값으로 비교했을 때는 차이가 있다.cake와 tasty의 코사인 거리는 0.5인 반면 movie와 cake, movie와 tasty는 각각 0.69, 0.67로 의미상 거리가 더 멀다는 것을 확인할 수 있다.그런데 아쉽게도 우리는 300차원짜리 단어 벡터로는 좌표계를 그릴 수 없다. 적어도 3차원으로는 바꿔줘야 한다.4-2. PCA(차원축소)다행히 우리는 비지도학습 기반 차원축소 기법을 사용할 수 있다. 차원축소는 다차원 데이터를 더 작은 데이터로 압축하는 방법이며, 데이터가 가지고 있는 본연의 특성을 최대한 유지하면서 변수를 줄일 수 있다. 물론, 300차원을 3차원으로 축소하게 되면 그만큼 정보손실이 발생한다. 그러나 우리는 머신러닝 모델링이 아니라 단순히 3차원 좌표계로 문서 간 거리를 확인하고자 함이므로 정보손실을 감수하고 차원을 줄이도록 하겠다.차원 축소를 위해서 ‘movie’, ‘cake’, ‘tasty’ 3가지 값을 포함해 관련 단어들을 각 단어당 200개씩 추출해서 데이터를 구성하겠다. 이렇게 하는 이유는 ‘movie’, ‘cake’, ‘tasty’ 각 단어의 유사 군집을 함께 비지도학습 시킴으로써 상대적으로 3개 단어의 위치정보를 인지시키기 위함이다. 이렇게 했을 때 큰 정보손실에도 고유의 군집 특성을 남길 수 있다. 먼저, 연관 단어를 추출한다. 모든 단어에 대한 300차원의 벡터 값을 가져온다. 그러면 메트릭스는 603개 단어에 대한 300차원 벡터로 구성된다. PCA 모델은 sklearn 패키지에서 제공한다. n_components 파라미터를 통해 축소할 차원을 3차원으로 설정하고, 결과값을 데이터프레임으로 생성해준다.압축된 차원의 데이터분포상 범위는 다음과 같다.4-3. 기저 벡터 및 문장 벡터 시각화 이제 기저 벡터에 대한 3차원 값을 확인할 수 있다. 기저 벡터를 선으로 그리고, 기존 문장 벡터는 기저 벡터에 따라 선형 변환시킨 다음 좌표계에 찍어보도록 하자.기저 벡터가 기존에 (1,0,0), (0,1,0), (0,0,1)이었다면 지금은 (-0.49, 0.19, 0.38), (0.59, 0.55, 0.22), (0.40, -0.39, 0.35)로 선형 변환되었다.선형 변환된 기저 벡터에 따라 문서 벡터 역시 선형 변환시켰다. 선형 변환은 다음과 같이 행렬곱을 통해 수행할 수 있다.마지막으로, 기저 벡터의 방향에 따라 3차원 좌표계 축을 다시 그리고 좌표계상 문서 벡터의 끝점 위치를 찍어보자.축의 각도와 각 문서 벡터 끝점(doc_1, doc_2, doc_3)의 위치가 바뀐 것을 더 명확히 확인할 수 있다.4-4. 문장 벡터 간 유사도 계산시각화했던 코드에서 다음과 같이 변수를 만들어 주었다. 위에서도 확인한 값이지만, df_doc_trans_pos는 선형 변환된 문장 벡터의 끝점 위치를 보여준다.이제 선형 변환시킨 문장 벡터의 코사인 거리가 어떻게 바뀌었을지 확인해보자. doc_1과 doc_2의 유클리디안 거리는 0.98, 코사인 거리는 0.18이다. doc_2와 doc_3의 유클리디안 거리는 1.16, 코사인 거리는 0.26이다.다시 정리하면, doc_1과 doc_2는 먹는 케이크에 대한 내용, doc_3은 영화 제목에 대한 내용이었다. 그리고 선형 변환 전에는 doc_1과 doc_2, doc_2와 doc_3간의 유클리디언 거리 및 코사인 거리가 서로 동일했다.그러나 기저 벡터의 선형 변환에 따라 좌표계가 바뀐 뒤에는 doc_1과 doc_2간의 거리가 doc_2와 doc_3간의 거리보다 유클리디안, 코사인 방식 모두에서 더 짧아졌다.지금까지 문장의 유사도를 구하는 방법에 대해 알아보았다.그 과정에서 코사인 유사도의 유클리디안 거리 계산 방식 대비 효용, 그리고 단어와 문장을 벡터로 변환하는 방법들을 확인했고, 더 나아가 기저 벡터의 선형 변환 및 좌표계 수정을 통해 문장의 의미를 고려한 유사도 계산 방법에 대해서도 확인해보았다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "벡터자기회귀 모형(VAR)을 활용한 다변량 예측 모델링 - 국내 주요 기업 주가 및 거래량 예측", "url": "/posts/%EB%B2%A1%ED%84%B0%EC%9E%90%EA%B8%B0%ED%9A%8C%EA%B7%80-%EB%AA%A8%ED%98%95-VAR-%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-%EB%8B%A4%EB%B3%80%EB%9F%89-%EC%98%88%EC%B8%A1-%EB%AA%A8%EB%8D%B8%EB%A7%81-%EA%B5%AD%EB%82%B4-%EC%A3%BC%EC%9A%94-%EA%B8%B0%EC%97%85-%EC%A3%BC%EA%B0%80-%EB%B0%8F-%EA%B1%B0%EB%9E%98%EB%9F%89/", "categories": "3. 튜토리얼, 시계열 예측 및 계량 분석 방법론", "tags": "", "date": "2023-02-08 00:00:00 +0900", "snippet": "Step 1. VAR 이란?VAR이란 Vector Autoregression, 벡터자기회귀 모형을 의미한다. 기본적인 자기회귀모형이 단변량 시계열 예측에서 사용된다면 벡터자기회귀모형은 다변량 예측에 사용된다. 즉, 2개 이상의 같은 기간에 대한 데이터셋이 서로 다른 변수로 서로 영향을 주는 관계인 경우 벡터자기회귀 모형을 사용한다.단변량 예측에 사용되는 자기회귀 모형 AR, ARMA, ARIMA의 경우 특정 시점의 과거가 현재에 영향을 미치는 단방향 모형일 수 밖에 없다. 이와 달리 VAR은 각 시계열 변수가 서로 영향을 주며 이를 고려해 각 변수의 미래값을 전체 시계열 변수의 과거값으로부터 예측하므로 양방향 모형이다. 이러한 방향성은 변수간 관게를 보여주는 것이기도 하다.(VAR의 인자로 주어지는 변수들은 서로 영향을 주고 받는다.)Step 2. 데이터셋 준비import pandas as pdimport numpy as npimport matplotlib.pyplot as plt# MacOS - 한글 폰트from matplotlib import rcrc(&#39;font&#39;, family=&#39;AppleGothic&#39;)plt.rcParams[&#39;axes.unicode_minus&#39;] = False# Graph Styleimport mplcyberpunkplt.style.use(&#39;cyberpunk&#39;)# Ignore Warningsimport warningswarnings.filterwarnings(&#39;ignore&#39;)2010년~2022년까지의 카카오, 네이버, 삼성전자, 하이닉스 주가 정보를 불러온다.카카오, 네이버, 삼성, 하이닉스 주가 정보 중 수정주가와 거래량만 가져온다.df = pd.concat([ yf_kakao[[&#39;Adj Close&#39;, &#39;Volume&#39;]], yf_naver[[&#39;Adj Close&#39;, &#39;Volume&#39;]], yf_samsung[[&#39;Adj Close&#39;, &#39;Volume&#39;]], yf_skhy[[&#39;Adj Close&#39;, &#39;Volume&#39;]],], axis=1)df.columns = [ &#39;kakao_ac&#39;, &#39;kakao_v&#39;, &#39;naver_ac&#39;, &#39;naver_v&#39;, &#39;samsung_ac&#39;, &#39;samsung_v&#39;, &#39;skhy_ac&#39;, &#39;skhy_v&#39;]df가져온 데이터는 다음과 같이 시각화해볼 수 있다.fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(15,6))ax_li = axes.flatten()df_cols = df.columnsfor i, (col, ax) in enumerate(zip(df_cols, ax_li)): ax.plot(df[col], linewidth=0.8) ax.set_title(col)plt.tight_layout()plt.show()우선, 수정주가를 보면 카카오와 네이버의 추세가 비슷하고 삼성전자와 하이닉스의 추세가 비슷하다.또한 각 기업별 주가 상승은 거래량에 영향을 미쳤을 것이고, 치솟는 거래량은 반대로 주가 상승 하락의 요인이 되었을 것이다.즉, 복합적으로 8가지 지표는 서로 연관이 있겠다는 합리적인 가정을 할 수 있겠다.Step 3. Granger 인과검정VAR 예측을 수행하기 전에 해당 변수들이 모두 상호연관성이 있는가를 확인해줄 필요가 있다.Granger 인과검정의 영가설은 X가 Y에 영향을 미치지 않는다 이다. 즉, 검정 결과 p_value가 유의수준 0.05 이하라면 영가설을 기각하고 해당 변수를 예측모델의 인자로 사용하도록 한다.각 변수가 서로 다른 변수에 영향을 주는가?를 검정하고, 검정 값(p-value)을 판다스 데이터프레임으로 한번에 표시할 것이다.이를 위해 값을 저장할 데이터프레임을 만들어주자. 값은 우선 0으로 채운다.df_pairs = pd.DataFrame(data=np.zeros((len(df_cols), len(df_cols))), columns = [&#39;X_&#39;+x for x in df_cols], index = [&#39;Y_&#39;+x for x in df_cols])df_pairs다음으로 샘플을 확인하기 위해 2개 변수만 가지고 granger 인과검정을 실시해보자.from statsmodels.tsa.stattools import grangercausalitytestsdf[&#39;kakao_ac&#39;].diff()sample_outs = grangercausalitytests(df[[&#39;kakao_ac&#39;,&#39;kakao_v&#39;]], maxlag=4)print(sample_outs)type(sample_outs)&amp;gt;&amp;gt; dictsample_outs의 타입은 딕셔너리다. key와 value를 살펴보자.sample_outs.keys()&amp;gt;&amp;gt; dict_keys([1, 2, 3, 4])sample_outs.values()key는 최소 1부터 maxlen으로 지정한 4까지 존재하며, 각 값 내부에는 검정 결과가 들어있다.sample_outs[1]f테스트, 카이제곱 등의 여러 검정통계량 결과값을 보여주고 있다. 우리는 이중 카이제곱 통계량을 사용하겠다.sample_outs[1][0][&#39;ssr_chi2test&#39;]&amp;gt;&amp;gt; (0.04672946904650268, 0.8288552369555231, 1)lag 1~4(day)까지의 p_value를 모두 확인해보면 다음과 값다.[round(sample_outs[x][0][&#39;ssr_chi2test&#39;][1],2) for x in range(1,5)]&amp;gt;&amp;gt;[0.83, 0.83, 0.89, 0.9]여기서 p_value는 2번째 값, 0.82로 연관관계가 있다고 볼 수 없다. 즉, 영가설을 기각할 수 없다.이제 다른 변수들의 상호 인과성을 함께 확인해보자. 단, 이번에는 4일치가 아닌 총 2주간(maxlag) 데이터로 결과값을 도출하도록 한다.maxlag = 14for col_i, col in enumerate(df_cols): for idx_i, idx in enumerate(df_cols): outs = grangercausalitytests(df[[col,idx]], maxlag=maxlag) pval_min = np.min([round(outs[x][0][&#39;ssr_chi2test&#39;][1],2) for x in range(1, maxlag+1)]) df_pairs.iloc[idx_i, col_i] = pval_min(..이하 생략)만들어진 데이터셋은 다음과 같다.df_pairs더 뚜렷하게 보이도록 히트맵을 사용해 시각화를 수행한다.import seaborn as snsplt.figure(figsize=(16,8))ax = sns.heatmap(df_pairs, annot=True, cmap=&#39;Blues&#39;)ax.xaxis.tick_top()plt.xticks(fontsize=12)plt.yticks(fontsize=12)plt.show()앞에서 분석 대상 시차(maxlag)를 4일로 뒀을 때는 카카오 수정종가와 볼륨간 인과관계가 유의하지 않았으나 지금은 p_value가 유의수준 0.05 이하로 들어온다.다른 변수들 역시 상당 수의 변수가 다른 변수로부터 영향을 받거나 주고 있는 것을 알 수 있다.단, X와 Y간 영향관계가 항상 서로 주고받는 것은 아니라는 사실도 위 히트맵을 통해 확인할 수 있다. 예를 들어, 네이버 거래량(X_naver_v)은 카카오 수정종가(Y_kakao_ac)에 영향을 준다고 보기 어려운(p_value==0.49) 반면 카카오 수정종가(X_kakao_ac)는 네이버 거래량(Y_naver_v)에 유의한 영향을 준.우리는 서로 영향을 주고 받는, 즉 상호 연관성이 있는 양방향 변수들만 VAR 모델 인자로 사용할 것이다.이 조건에 만족하는 조합은 다음과 같다. [kakao_ac, kakao_v, naver_ac] [kakao_v, naver_ac, naver_v] [samsung_ac, samsung_v, skhy_ac] …Step 4. Cointegration Test공적분 검정(혹은 요한슨 검정)은 다중 시계열 간 적분상 균형관계가 존재하는가를 판단한다.즉, Granger 검정이 단일 시계열보다 다중 시계열로 설명력을 더 얻을수 있는가? 를 확인해주었다면, Cointegration 검정은 다중 시계열이 장기적 기간을 두고 안정적인 연관성을 보이는가? 를 확인해준다.따라서 Granger 검정을 통해 필터링한 변수 조합 중 최적 조합을 찾기 위해 Cointegration 검정을 한번 더 실시하고자 한다.from statsmodels.tsa.vector_ar.vecm import coint_johansensample_group = [&#39;kakao_ac&#39;,&#39;kakao_v&#39;,&#39;naver_ac&#39;]out = coint_johansen(df[sample_group], 1, 1)out.lr1&amp;gt;&amp;gt; array([453.97917988, 9.88834366, 1.92223596])파이썬을 통해 공적분은 위와 같이 수행할 수 있다. 결과값으로 나온 수치는 각각 kakao_ac, kakao_v, naver_ac에 대한 공적분 통계량에 해당한다.cvt_df = pd.DataFrame(out.cvt)cvt_df.columns = [&#39;90%&#39;, &#39;95%&#39;, &#39;99%&#39;]cvt_df.index = sample_groupcvt_dfout.cvt를 통해 각 유의수준에 해당하는 통계량을 확인할 수 있다. 공적분 결과값이 해당 유의수준 통계량보다 크다면(우측, 단측검정) 유의하다. 즉, 장기적으로 안정적인 연관성이 있다고 판단한다.def calc_sig_dist(cols): out = coint_johansen(df[cols], 1, 1) stats = [round(x,2) for x in out.lr1] sigs = [round(x,2) for x in out.cvt[:, 1]] yns = [x&amp;gt;y for x,y in zip(stats,sigs)] dist = np.mean(np.array(stats) - np.array(sigs)) print(&#39;stats: &#39;,stats) print(&#39;sig-level: &#39;,sigs) print(&#39;significant_yn: &#39;,[x&amp;gt;y for x,y in zip(stats,sigs)]) print(&#39;dist: &#39;, round(dist,2))stats는 공적분 통계량, sig-level은 유의수준 0.05에 해당하는 관측치, significant_yn은 유의 여부(장기 안정성), dist는 안정성의 강도를 의미한다.함수를 통해 앞에서 묶은 그룹 3가지를 확인해보고, significant_yn이 모두 True이면서 dist가 가장 큰 그룹을 찾아보자.결과값을 토대로 samsung_ac, samsung_v, skhy_v 3개 변수 조합을 사용하자.Step 5. Split Datasets (Train / Test)우리는 과거 데이터를 가지고 가장 최근 30일을 예측해보려 한다.위에서 확인한 그룹을 추출해 최근 30일 전후로 데이터를 구분하자.target_lag = 30target_df = df[[&#39;samsung_ac&#39;,&#39;samsung_v&#39;,&#39;skhy_v&#39;]]df_train = target_df.iloc[:-target_lag, :]df_test = target_df.iloc[-target_lag:, :]print(df_train.shape, df_test.shape)&amp;gt;&amp;gt; (3175, 3) (30, 3)Step 6. Stationary Test마지막으로 VAR 모델을 만들기 전에 입력 데이터의 정상성을 확보해야 한다. 특히 수정종가의 경우 주가 특성상 추세를 포함할 것이므로 정상성을 띄지 않을 가능성이 매우 높다.(여기서 정상성이란 시간의 흐름에 관계없이 데이터의 평균 및 분산이 일정함을 의미하며 이러한 특성은 시계열 분석의 기본 전제가 된다.)시계열 정상성은 Augmented Dickey-Fully(ADF) 검정을 통해 확인하도록 한다. 영가설은 정상성을 띄지 않는다 이다.ADF는 추세를 포함하는 시계열 데이터에 대해서도 단위근 검정을 실시해주는 방법론이며, Dickey-Fully(DF) 검정을 더욱 일반화시킨 방식이다.samgsung_ac 데이터로 먼저 ADF 검정을 실시하고, 결과값 형태를 확인해보자.from statsmodels.tsa.stattools import adfulleradf_sample = adfuller(df_train[&#39;samsung_ac&#39;], autolag=&#39;AIC&#39;) # AIC가 가장 낮은 lag(시차)를 자동 선택adf_sample&amp;gt;&amp;gt; (-0.7808633243482267, 0.8246787955943002, 28, 3146, {&#39;1%&#39;: -3.432430306322329, &#39;5%&#39;: -2.862459151265583, &#39;10%&#39;: -2.567259285721503}, 49407.65034169026)보기 편하게 데이터프레임으로 변환해보자.p_value가 0.05보다 작은가, 즉 인덱스 기준 stat이 5% 유의구간 내에 들어가는가를 확인하면 된다.결과값에서 알 수 있듯이 좌측 단측검정이다.adf_df = pd.DataFrame(adf_sample[:4])adf_df.columns = [&#39;samsung_ac&#39;]adf_df.index = [&#39;stat&#39;,&#39;p_value&#39;,&#39;lag&#39;,&#39;observ&#39;]sig = pd.DataFrame(data={&#39;samsung_ac&#39;:adf_sample[4][&#39;5%&#39;]}, index=[&#39;5%&#39;])adf_df = pd.concat([adf_df, sig], axis=0)adf_dfsamsung_ac의 경우 stat이 5% 보다 크다. 따라서 영가설을 기각할 수 없다.마찬가지로 나머지 2개 변수도 ADF 검정을 실시한다.def get_adf_df(data): adf_df = pd.DataFrame(adfuller(data, autolag=&#39;AIC&#39;)[:4]) adf_df.columns = [col] adf_df.index = [&#39;stat&#39;,&#39;p_value&#39;,&#39;lag&#39;,&#39;observ&#39;] sig = pd.DataFrame(data={col:adf_sample[4][&#39;5%&#39;]}, index=[&#39;5%&#39;]) adf_df = pd.concat([adf_df, sig], axis=0) adf_df = adf_df.apply(lambda x : round(x, 2)) return adf_df거래량을 나타내는 samsung_v와 skhy_v는 정상성을 만족한다. 따라서 우리는 VAR 모델링을 위해 samsung_ac만 정상성을 만족시켜주면 되겠다.차분을 통해 간단히 분포를 바꿔주고 정상성을 다시 확인한다.(차분을 수행하면 첫번째 인덱스는 nan값으로 바뀐다. bfill()함수를 통해 두번째 값으로 첫번째 값을 채워주겠다.)get_adf_df(df_train[&#39;samsung_ac&#39;].diff().bfill())1회 차분으로 samsung_ac 변수도 정상성을 확보했다. 이제 모델을 만들어보자.Step 7. VARVAR 모델 역시 파이썬 함수를 통해 간단히 구현가능하다. 앞에서 1회 차분이 정상성을 확보시켜주는 것을 확인했으므로 해당 데이터만 치환한 후 데이터를 인자로 넘겨주겠다.df_train_ = df_train.drop(&#39;samsung_ac&#39;, axis=1).copy()df_train_[&#39;samsung_ac_diff&#39;] = df_train[&#39;samsung_ac&#39;].diff().bfill()df_train_from statsmodels.tsa.api import VARvar = VAR(df_train_)var.select_order(maxlags=30).summary()AIC가 10 이상만 되어도 유의한 예측이라고 보기 어렵다.물론 데이터를 더 가공하고 양질의 연관 변수를 추가해볼 수도 있겠지만 이 경우는 기본 값 자체가 매우 커서 그렇다.데이터 범위가 커서 예측 수준이 떨어지는 것이다.따라서 데이터를 한번 더 처리해주겠다. 이번에는 정규화를 시켜줄텐데, 시계열 데이터의 경우 정규화에 주의해야 한다.예측값이 최근 일자, 관측값이 과거 일자이므로 정규화가 최근 일자 예측에 어떠한 영향도 미쳐선 안된다. 예를 들어, 전체 데이터를 min-max-scailing한다거나 하는 식으로 말이다.우리는 전체 평균이 아니라 첫날 데이터를 0으로 기준을 잡고, 그 이후에는 첫날 대비 변화량을 보려줄 수 있도록 할 것이다.df_train_norm = (df_train_ / df_train_.iloc[0]) - 1df_train_norm데이터를 한번 더 처리했으니 정상성 여부도 체크해준다.참고로, 일반적으로 정제 강도가 커질수록 정보가 희석되고 정상성이 커지므로 정상 데이터가 비정상으로 다시 회귀하는 경우는 잘 없다.print(get_adf_df(df_train_norm[&#39;samsung_ac_diff&#39;]).loc[&#39;p_value&#39;])print(get_adf_df(df_train_norm[&#39;samsung_v&#39;]).loc[&#39;p_value&#39;])print(get_adf_df(df_train_norm[&#39;skhy_v&#39;]).loc[&#39;p_value&#39;])&amp;gt;&amp;gt;skhy_v 0.0Name: p_value, dtype: float64skhy_v 0.0Name: p_value, dtype: float64skhy_v 0.01Name: p_value, dtype: float64이제 다시 VAR 모델에 데이터를 태워준다.var_norm = VAR(df_train_norm)var_norm.select_order(maxlags=20).summary()일반적으로 AIC 기준 2.0 이하는 모델이 타당하다고 할 수 있다.모델은 과거 1주일치 데이터만 가지고도 어느 정도의 성능을 보이고 있으며, 예측 정확도는 2주 데이터가 확보되었을 때 가장 높았다.따라서 모델의 시차(lag)를 2주로 적합시킨다.model = var_norm.fit(14)model.summary()(..중략)summary에서 중요한 부분은 마지막 Correlation matrix다.해당 관측치는 변수 잔차간 상관관계인데, 이 수치가 크다면 아직 고려해야 할 요소들이 남아있는 것이며 모델은 충분한 설명력을 갖추지 못했다고 볼 수 있다.Correlation matrix는 이는 다음과 같이 직접 추출할 수 있다.다만, 이렇게 봤을 때 이것이 유의한 수준인지 그렇지 않은지 확인할 수 없으므로 마찬가지로 검정통계를 실시한다.잔차의 독립성 검정은 Durbin Watson 검정을 통해 수행하도록 하자.from statsmodels.stats.stattools import durbin_watsondurbin_res = pd.DataFrame([model.resid.columns, [round(x,2) for x in durbin_watson(model.resid)]]).Tdurbin_res.set_index([0])Durbin Watson 검정은 다음과 같이 해석한다. 범위 : 0~4 양의 상관관계 : 0에 수렴 음의 상관관계 : 4에 수렴 상관관계 없음 : 2에 수렴위 해석에 따라 본 모델은 잔차간 상관성이 유의하지 않은 수준이며, 충분한 예측 설명력을 갖췄음을 확인할 수 있다.Step 8. Forecast우리가 만든 모델의 lag는 예측을 위해 사용할 데이터 범위가 된다. 즉, 과거 14일치 데이터를 가지고 앞으로의 주가를 예측한다.model.k_ar&amp;gt;&amp;gt; 14ins = df_train_norm.values[-model.k_ar:]ins3개 변수에 대한 최근 14일치의 데이터다. 앞에서 우리는 최근 30일치를 기준으로 train dataset를 나눴기 때문에 30일치보다 앞선 14일치 데이터라고 보면 되겠다.f = model.forecast(y=ins, steps=7)df_f = pd.DataFrame(f, columns=df_train_norm.columns)df_fforecast 함수를 사용해 df_train 기준 마지막 14개 행의 데이터로 다음 7개 행을 예측했다.예측한 값을 실제 값과 비교하기 위해서는 실제 값의 분포로 다시 돌려줘야 한다.먼저 전체 데이터셋에 대해 첫번째 행 데이터 기준 정규화를 해줬으니 반대로 역정규화를 시켜주자.다음 단계는 1차 차분시킨 samsung_ac_diff도 원상 복귀시키는 것이다.학습데이터 기준, 마지막 날부터 누적합을 통해 계산할 수 있다.마지막으로, 테스트 데이터셋과 동일한 시계열상에 올려주자.데이터셋이 준비되었으므로 예측값과 실제값을 시각화를 통해 비교해보도록 하자.def draw_subplot(axes, ax_i, col): d_test = df_test[col].iloc[:7] d_pred = df_f_denorm[col] d_test.diff() &amp;gt; 0 axes[ax_i].plot(d_test, label=&#39;real&#39;) axes[ax_i].plot(d_pred, label=&#39;pred&#39;) hits = (d_test.diff() &amp;lt; 0) == (d_pred.diff() &amp;lt; 0) for i, hit in enumerate(hits): if hit: if i==0: pass if i+1==len(hits): i==None axes[ax_i].fill_between(x=d_test.index[i-1:i+1], y1=d_pred[i-1:i+1], y2=d_test[i-1:i+1], color=&#39;lightpink&#39;, alpha=0.1) axes[ax_i].set_title(col) axes[ax_i].legend(loc=(1,1))fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(10,10))draw_subplot(axes, 0, &#39;samsung_ac&#39;)draw_subplot(axes, 1, &#39;samsung_v&#39;)draw_subplot(axes, 2, &#39;skhy_v&#39;)plt.tight_layout()plt.show()이렇게 7일간의 데이터 예측치를 살펴보았고, 상승 및 하락 여부가 일치하는 구간에는 영역을 채워줬다. 위 결과값만 놓고 보면 VAR 모형이 종가 예측보다는 거래량 예측에 더 나은 성능을 내는 것으로 보이며 추세도 어느 정도 잘 판단해내는 듯 하다.만약 이 모델에 대한 정량적 평가를 실시해보고 싶다면 다음의 순서를 따르면 되겠다. 14일치 데이터셋을 최소 30개 이상 확보한다. 각 데이터셋(배치)에 대한 7일간 예측치과 실제값의 오차를 RMSE 등의 방식으로 계산한다.다만, 주가데이터 특성상 종가의 범위가 주식마다 매우 다르기 때문에 상승 및 하락에 대한 예측정확도를 평가하는 것도 하나의 방법이며 방식은 다음과 같다. 모형 결과로 나온 값을 차분한다. 차분 값을 음수와 양수를 2개 클래스로 나눈다.(위 시각화에서도 상승하락 여부가 같은지 유사한 프로세스로 비교하였다.) 실제 데이터도 동일하게 차분 및 이진화로 라벨링을 수행한 후 f1 스코어 등으로 평가한다.지금까지 이렇게 VAR 모형의 간단한 개념을 알아보고, 모형을 사용하기 위한 통계적 검정들도 수행한 뒤, 모델링 및 예측까지 시도해보았다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "금융 분석을 위한 파이썬 프로그래밍 - 보충자료 02. 파이썬 데이터 시계열 차트 시각화(matplotlib, subplots)", "url": "/posts/%EA%B8%88%EC%9C%B5-%EB%B6%84%EC%84%9D%EC%9D%84-%EC%9C%84%ED%95%9C-%ED%8C%8C%EC%9D%B4%EC%8D%AC-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EB%B3%B4%EC%B6%A9%EC%9E%90%EB%A3%8C-02-%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%8B%9C%EA%B3%84%EC%97%B4-%EC%B0%A8%ED%8A%B8-%EC%8B%9C%EA%B0%81%ED%99%94/", "categories": "3. 튜토리얼, 금융 분석 프로그래밍 기초", "tags": "", "date": "2023-02-04 00:00:00 +0900", "snippet": "데이터를 시각화하는 도구는 여러 가지가 있습니다. 특히 시계열 데이터의 경우 엑셀이나 스프레드시트, PPT만으로도 충분히 의도하는 내용을 차트로 만들어낼 수 있습니다. 따라서 ‘굳이’ 파이썬으로 시각화를 하고자 한다면 단순히 파이썬으로 분석, 시각화까지 이어서 진행할 수 있는 연속성 외에도 파이썬이 주는 자유도와 자동화로 인한 편의를 충분히 활용해야 합니다.파이썬을 사용하면 반복문과 함수를 손쉽게 활용하고, 프로그래밍을 통해 여러 차트를 동시에 그려낼 수 있습니다. 이를 극대화해주는 함수가 subplots()입니다.​Step 1. 레이아웃 설정먼저, matplotlib.pyplot 패키지를 plt라는 이름(alias, 별칭)으로 불러옵니다.plt를 통해 아래와 같이 전체 레이아웃을 subplot으로 분할하고, 특정 subplot에 그래프를 그릴 수 있습니다.코드 흐름은 아래와 같습니다. 먼저 전체 레이아웃을 figsize=(12,12)로 설정해서 가로, 세로가 동일(12)한 사이즈의 공간을 마련합니다. 공간을 3행 2열의 작은 subplot으로 나눠주고, 각 subplot에는 axes 변수로 접근할 수 있도록 합니다. np.linspace(1, 10, 10)으로 1부터 10까지 10(3번째 인자)개 구간으로 나눠 데이터를 생성합니다. 그럼 data_x에는 array([1,2,3,4,5,6,7,8,9,10]) 이 들어가고, 각 요소의 타입은 float입니다. data_y는 data_x의 모든 요소에 대해 2승한 값을 넣어줍니다. data_x, data_y로 첫 번째 행, 첫 번째 열에 위치한 subplot axes[0,0]에 차트를 그려줍니다. 해당 subplot에는 set_xlabel(), set_ylabel() 함수로 각각 x축 이름, y 축 이름을 설정해 주고, set_title() 함수로 subplot 이름을 해당 subplot 상단에 표시해 줍니다. 첫 번째 행, 두 번째 열에 위치한 subplot axes[0,1]에는 위 방식으로 차트를 그리되, data_y는 data_x 요소들을 모두 3승한 값을 넣어줍니다. 마지막으로 plt.show() 함수로 전체 subplot을 함께 그립니다.​​Step 2. 반복문 활용그런데, 이런 방식으로 6개의 subplot을 모두 그리기에는 너무 번거롭습니다. 따라서 우리는 반복문을 사용해 1~10까지의 정수(float type이지만 소수점 이하 0을 생략하여 정수라는 표현을 사용하겠습니다.)를 2승, 3승 ~ 7승한 그래프를 각 subplot에 순서대로 그려보겠습니다.axes.shape이 (3,2)이므로 a*b는 6입니다. 따라서 for i in range(a*b) 반복문을 통해 i는 0부터 5까지 1씩 증가합니다. i가 1씩 증가하면 i를 2로 나눈 몫은 순서대로 0, 0, 1, 1, 2, 2가 되고, i를 2로 나눈 나머지는 0, 1, 0, 1, 0, 1이 됩니다. 둘을 각각 행의 번호, 열의 번호로 지정하고, 첫 번째 subplot부터 마지막 subplot까지 순회하며 차트를 그려나갑니다.​Step 3. 영역 채우기차트에 선 하나만 있을 때는 시각적으로 잘 표현되지만 2개 이상의 선을 그려야 할 때는 선 대신 면을 활용하면 좋습니다. 이때는 plot() 대신 fill_between() 함수를 사용합니다. 영역을 채우는 함수이므로 y 기준선은 y1, y2 두 개를 설정해야 하는데, y2는 default가 0이므로, 아래와 같이 y1만 설정하면 선 아래 영역을 채울 수 있습니다.step 2와 동일한 코드에 plot() 부분만 변경합니다.​Step 4. 흐리게 표현하기matplotlib을 사용해 그래프를 그릴 때는 일반적으로 alpha 옵션을 통해 흐리기를 지정할 수 있습니다. 0에서 1까지의 숫자로, 1에 가까울수록 색상이 짙습니다. fill_between() 함수 역시 alpha 옵션을 지원합니다.​alpha=0.3 정도로 영역을 흐리게 표현해주면 추후 선이나 면을 추가로 겹쳐 그리더라도 시각적으로 잘 나타낼 수 있습니다.​Step 5. 선과 영역을 같이 표현하기선을 강조하고 싶은 경우 fill_between() 함수와 plot() 함수를 함께 사용하면 됩니다.​Step 6. 강조선 표시하기강조선은 axvline(), axhline()을 통해 표시할 수 있습니다. 각각은 ax + vertical + line, ax + horizontal + line의 약자로 수직선, 수평선을 의미합니다.​여기서도 옵션을 지정할 수 있습니다. 색상은 red로, 선 스타일은 ‘–‘로, 흐리기는 0.6으로 설정했습니다.​Step 7. 강조 영역 표시하기강조 영역은 axvspan(), axhspan() 함수를 사용해 표시할 수 있습니다. 각각 함수명에서 v와 h가 의미하는 바는 강조선 함수와 동일하게 vertical, horizontal이므로 기억하기 쉽습니다.​Step 8. 활용앞서 확인한 내용들을 종합 정리하면 다음과 같습니다. subplots() 함수를 통해 전체 레이아웃을 행과 열로 n등분합니다. n등분한 레이아웃이 2차원인 경우 axes[nrow, ncol], 1차원인 경우 axes[n]로 접근하며 plot() 혹은 fill_between() 함수로 선이나 면을 그립니다. 이어서 axvline(), axhline(), axvspan(), axvspan() 함수로 강조선 혹은 강조 영역을 더 그려줄 수 있습니다. 각 그래프에 대해 alpha로 흐리기 정도를 지정하고, linestyle로 선 스타일을 지정할 수 있으며 이외 옵션들도 색상 등 다양하게 설정할 수 있습니다. 각 그래프에 대해 x축 이름, y축 이름, 그래프 이름을 설정할 수 있습니다. 마지막으로 plt.show()로 전체 레이아웃을 한 번에 그려줍니다.​우리는 위 내용을 활용해 각 연도별 테슬라 주가의 평균치 영역을 그려볼 수 있습니다.8-1. 테슬라 주가 가져오기yahoo finance 패키지를 활용해 2010년(상장 연도) ~ 2021년 말까지의 주가 정보를 수집합니다.8-2. 연도별 주가 평균 영역 시각화차트를 그릴 대상은 2010년, 2018년, 2021년 3개 연도의 종가(Close) 데이터입니다. 시계열 데이터의 경우 index의 type이 datetime이라면 간단히 df[‘2010’]와 같이 인덱싱해주면 해당 연도의 데이터를 모두 가져올 수 있습니다. 3개 연도 데이터를 각각의 subplot에 그릴 것이므로 3개 행, 1개 열을 지정해주고, axes[n]으로 접근합니다.이렇게 단순히 연도별 평균만 강조선으로 그리기보다 평균 +/- 표준편차를 표시해 줌으로써 특정 날짜의 주가가 연도별 평균치를 유의미하게 상회하는지, 하회하는지를 확인할 수 있습니다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "금융 분석을 위한 파이썬 프로그래밍 - 보충자료 01. 파이썬 기초 문법(자료형, Data type)", "url": "/posts/%EA%B8%88%EC%9C%B5-%EB%B6%84%EC%84%9D%EC%9D%84-%EC%9C%84%ED%95%9C-%ED%8C%8C%EC%9D%B4%EC%8D%AC-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EB%B3%B4%EC%B6%A9%EC%9E%90%EB%A3%8C-01-%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EA%B8%B0%EC%B4%88-%EB%AC%B8%EB%B2%95-%EC%9E%90%EB%A3%8C%ED%98%95-Data/", "categories": "3. 튜토리얼, 금융 분석 프로그래밍 기초", "tags": "", "date": "2023-02-03 00:00:00 +0900", "snippet": "파이썬에는 여러 가지 자료형, 즉 데이터 타입이 존재합니다. 그중에서도 우리는 단일 데이터를 표현해 주는 int, float, bool, str(엄밀히 말하면 연결된 char입니다만 char 자료형은 현 단계에서 접할 일이 많지 않습니다. 그래서 문자열 str을 단일 데이터 표현형으로 대체합니다.)과 다중 데이터를 표현해 주는 list, dict, set, 그리고 마지막으로 벡터 및 행렬 표현을 위한 numpy, pandas까지 빠르게 다뤄보도록 하겠습니다.​Step 1. 단일 데이터 표현1-1. intint 자료형은 정수를 표현할 수 있습니다. 변수에 1이나 2와 같은 정수를 담아주면 파이썬은 해당 변수의 자료형을 int로 인식합니다.print() 함수로 변수가 가리키는 데이터를 출력할 수 있고, type() 함수로 해당 데이터의 자료형을 확인할 수 있습니다.​정수끼리 더했을 때도 역시 자료형이 정수로 동일해야 하겠죠.​1-2. floatfloat 자료형은 실수를 표현할 수 있습니다. 마찬가지로 print(), type() 함수로 데이터와 자료형을 확인할 수 있습니다.덧셈도 동일합니다.만약 실수를 정수로 바꾸고 싶다면 int()로 감싸주면 됩니다. 파이썬에서는 이렇게 간편하게 자료형을 변경할 수 있습니다.이때, 0. 이하 숫자는 반올림되는 것이 아니라 ‘버림’(소거) 됩니다.또한, 실수를 1.0과 같이 정수로 떨어지게 만든다고 해서 자료형이 바뀌지는 않습니다. 원하는 자료형으로 만들어주려면 형 변환을 직접 수행해 주어야 합니다.​1-3. str위에서 만든 변수 a, b의 자료형은 int였습니다. 이것을 float이 아니라 문자열 str으로도 형 변환할 수도 있습니다.만약 a, b 모두 문자열 타입으로 바꿨다면 둘을 더했을 때 3이 아니라 ‘12’가 됩니다. 문자열끼리 더해주면 서로 연결되기 때문입니다.​공백 문자도 문자열입니다. 추후에 데이터 분석을 직접 수행하게 되면 NA, NULL, NaN 등 다양한 공백 표현을 다루게 됩니다. 그때 이러한 공백 문자들을 특정 숫자로 치환하거나 NaN으로 변환해 주는 작업들을 자주 접할 것입니다.​1-4. bool불리언(bool) 타입은 True or False 이진 분류형 자료형입니다. 엑셀이나 스프레드시트에서 체크박스가 바로 이런 자료형으로 동작합니다.이 자료형은 int로 변경 시 True는 1로, False는 0으로 바뀝니다.또한, 형 변환 없이 덧셈을 수행해도 1로 바뀌고 자료형도 int로 변경됩니다.위에서 공백 문자열 ‘‘를 f 변수에 담았습니다. 이것은 bool형 변환 시 False가 됩니다. 반대로 문자열이 공백이 아니라면 True가 됩니다.bool 자료형을 더했을 때는 int로 형 변환이 되므로 아래와 같은 결과도 자연스럽게 예상해 볼 수 있습니다.​​Step 2. 다중 데이터 표현위에서 본 데이터들을그룹으로 묶어낼 수 있는데 대표적으로 list(리스트), dict(딕셔너리), set(세트, 집합)이 유용하게 쓰입니다.​2-1. list먼저 리스트 자료형은 대괄호를 통해 여러 요소들을 1차원 배열로 묶어줍니다. 이때, 요소들이 꼭 동일한 자료형일 필요는 없습니다. 리스트끼리 더해주면 문자열처럼 연결됩니다.사실, 앞서 언급했듯 문자열 str은 메모리 참조 구조로 볼 때, char의 배열입니다. 따라서 문자열을 list로 형 변환해 주면 각 문자가 리스트 원소가 됩니다. 단, 이때 각 문자의 자료형은 문자열입니다.​2-2. dict다음은 딕셔너리입니다. 딕셔너리는 명칭처럼 키와 값으로 이루어진 사전과 같습니다. 아래 예시에서 ‘a’와 ‘b’는 키, 그리고 각각의 키에 대한 값은 li_a, li_b가 됩니다.사전에서 ‘a’키에 들어있는 값을 조회하고 싶다면 대괄호를 통해 인덱싱할 수 있습니다.두 값은 모두 리스트 자료형이기 때문에 더했을 때 아래와 같이 연결되어야 합니다.​2-3. setset은 집합입니다. 딕셔너리와 동일하게 중괄호로 생성할 수 있습니다.자료형 특성상 교집합, 합집합, 차집합을 수행할 수 있으며 리스트가 아니므로 합집합에 더하기 기호는 사용할 수 없습니다.집합은 중복이 제거된다는 특성이 있습니다. 만약 중복 원소를 포함한 리스트를 집합으로 바꾸면 아래와 같이 중복이 제거됩니다. 이 방식은 간단하지만 unique 한 원소를 추출할 때 유용하게 쓰입니다.​Step 3. 벡터 및 행렬 표현3-1. numpy array파이썬에서는 벡터 및 행렬 연산을 위해 numpy라는 외장 라이브러리를 사용할 수 있습니다. 먼저, numpy를 np라는 별칭으로 불러옵니다.(라이브러리에 대한 설명은 다음 챕터(01)에서 확인하실 수 있습니다.)이제 1차원 배열, 혹은 벡터 array를 생성하고 출력해 보겠습니다.겉보기에 list 자료형과 동일해 보입니다. 하지만 np_a와 np_b를 더하거나 곱해보면 리스트와 달리 벡터 연산이 되는 것을 확인할 수 있습니다.np.dot() 함수를 사용하면 행렬 곱도 수행할 수 있습니다.shape을 출력해 보면 벡터 혹은 행렬 크기를 확인할 수 있습니다. 1차원 벡터라면 (n, ) 형태로 출력되고 2차원 행렬이라면 (n, m) 형태로 출력됩니다. 아래 예시에서는 np_a, np_b 두 벡터를 다시 한번 np.array()로 묶어 2차원 행렬로 변환해 shape을 출력하는 모습을 보여주고 있습니다.​3-2. pandas DataFrame, Series판다스는 1차원 벡터를 Series 자료형으로, 2차원 행렬을 DataFrame 자료형으로 나타내줍니다. 물론 numpy를 사용한 벡터, 행렬이 아닌 list나 dict 자료형을 활용해서 Series, DataFrame을 만들 수도 있습니다.DataFrame은 엑셀이나 스프레드시트처럼 테이블 형태로 데이터를 조합해 주고 해당 툴에서 함수 수식을 통해 group_by, aggregation 등을 수행한 것처럼 다양한 집계 함수를 제공하고 있습니다. 따라서 이러한 테이블 형태의 데이터를 분석한다면 pandas는 언제나 핵심 도구로 활용됩니다.​사용법은 간단합니다. numpy array 혹은 리스트를 마치 딕셔너리 생성하듯 data에 넣고, pd.DataFrame() 함수로 감싸주면 됩니다.따라서 데이터 프레임은 dict 자료형으로도 간편하게 만들 수 있습니다.그리고 dict에서 ‘키’로 ‘값’을 인덱싱했던 것처럼 데이터 프레임은 행 인덱스(a, b)로 열 리스트를 조회할 수 있습니다. 이렇게 나오는 ‘값’이 바로 Series라는 자료형입니다.언급한 것처럼 데이터 프레임을 활용하면 매우 쉽게 데이터를 집계할 수 있습니다. 집계할 때는 항상 축을 생각해야 하는데, 행을 기준으로 집계하려면 axis=0, 열을 기준으로 집계하려면 axis=1을 사용합니다. 헷갈리면 실행 결과를 행 방향으로 만들고 싶은지, 열 방향으로 만들고 싶은지 떠올리면 수월합니다.​이렇게 아주 간단하게만 자료형에 대해서 알아봤습니다. 자료형은 가장 기초적이지만 실수가 매우 잦은 지점입니다. 특히 분석할 때 자료형을 유의 깊게 신경 쓰지 못해서 에러를 마주하는 경우가 많습니다. 그러나 분석을 수행해 보지 않은 상태에서 자료형을 너무 깊게 다루게 되면 암기과목처럼 되어버리니 앞으로 분석을 수행하면서, 다양한 함수를 사용해 보면서 자연스럽게 익혀나가도록 하겠습니다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "금융 분석을 위한 파이썬 프로그래밍 - 04. 머신러닝 예측 모델링", "url": "/posts/%EA%B8%88%EC%9C%B5-%EB%B6%84%EC%84%9D%EC%9D%84-%EC%9C%84%ED%95%9C-%ED%8C%8C%EC%9D%B4%EC%8D%AC-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-04-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%98%88%EC%B8%A1-%EB%AA%A8%EB%8D%B8%EB%A7%81/", "categories": "3. 튜토리얼, 금융 분석 프로그래밍 응용", "tags": "", "date": "2023-02-02 00:00:00 +0900", "snippet": "본 장에서는 기계학습 방법론에 대해 간단히 알아본 다음, 파이썬 패키지로 제공되는 주요 기계학습 알고리즘을 사용해 보도록 하겠습니다.Step 1. 데이터 불러오기이번에도 역시 테슬라 주가를 불러오는 것부터 시작합니다.​Step 2. 예측값 정의2-1. 기계학습기계학습은 함수를 만드는 방식 중 하나입니다. y=ax+b라는 함수가 있습니다. 만약, 출력값이 입력값 대비 항상 2배가 되어야 하는 함수를 구해야 한다면 단순하게 a=2, b=0으로 설정하면 됩니다. 하지만 출력값이 입력값 대비 언제는 2배가 될 수도 있지만, 경우에 따라 1.5배가 되거나 어쩌면 -2배도 될 수도 있다면 함수를 어떻게 정의해야 할까요?이때는 확률적 방법론을 사용해야 합니다. 경우에 따라 확률적으로 1.5배가 되거나 -2배가 될 수 있도록 하는 것입니다. 그런데 이것을 사람이 계산하는 것은 너무나 어려운 일이기 때문에 컴퓨팅 파워를 활용해 최적의 a와 b, 더 나아가 c ~ z까지 찾아내게 됩니다. 확률적 방법론은 오차의 발생 가능성을 내포합니다. 이러한 오차를 최대한으로 줄여나가는 과정을 ‘학습’이라고 하고, 더 줄일 수 없는 지점에 대해 ‘적합’되었다고 합니다. 충분히 줄이지 못한 지점에 대해서는 ‘과소 적합’되었다고 하고, 지나치게 줄인 지점에 대해서는 ‘과대 적합’되었다고 합니다. 적절한 적합 지점을 찾는 행위가 최적화 과정입니다.이러한 기계학습의 유형은 크게 2가지입니다. 하나는 지도학습이고 다른 하나는 비지도 학습입니다. 사실, 앞에서 설명한 기계학습 방식이 지도학습입니다. 지도학습은 학습을 수행할 때 X(X_1, x_2, …,x_n)와 y(정답, label)을 알려주고 둘 사이의 논리 구조를 찾아나갑니다. 반복적으로 예측을 수행하면서 예측값과 실제 값을 비교하고, 그 차이인 오차를 유의한 수준으로 줄이는 것이 학습의 목적입니다.반면, 비지도 학습은 y가 주어지지 않습니다. X만 가지고 적절한 수의 그룹(군집)으로 나누는 테스크를 수행합니다. 극단적인 예로, 철수는 국어가 7점, 영어가 15점, 수학이 70점, 과학이 80점이고 영수는 국어가 100점, 영어가 99점, 수학이 3점, 과학이 2점이라고 하겠습니다. 또한 철수와 같이 국어와 영어보다 수학, 과학을 잘하는 학생들이 절반 있고, 영수와 같이 국어와 영어를 수학, 과학보다 잘하는 학생들이 절반 있다고 하겠습니다. 그러면 이때, 비지도 학습은 철수처럼 수학, 과학을 잘 하는 학생들을 A로 분류하고 영수처럼 국어, 영어를 잘 하는 학생들을 B로 분류할 것입니다. 우리는 A와 B로 나눠진 결과를 보고 “아, 문/이과로 나눴구나” 하고 판단할 수 있습니다. y(label)을 주지 않았기 때문에 모델은 이것이 문과인지 이과인지, 뭘 분류하는 것인지는 알 수 없지만 데이터상 2개 유형으로 나누는 것이 적합하다고 판단하는 것입니다. 이것이 비지도 학습의 대표적인 유형인 군집 분석입니다.​2-2. 지도학습그중에서도 지도학습은 일반적으로 2개 유형의 테스크를 다룹니다. 하나는 예측이고, 다른 하나는 분류입니다. 지난 1주일 간의 주가 움직임을 보고 다음날 시가를 ‘예측’하거나, 다음날 시가가 어제 종가 대비 오를지, 내릴지를 ‘분류’합니다. 뉘앙스에서 알 수 있듯 사실상 분류도 예측의 일종으로 보기도 합니다. 예측의 대상이 이산적(ex. 0,1)이냐 연속적(ex. 0~1)이냐에 따라 분류 혹은 예측이라 정의하는 것이 보편적입니다.좀 더 직관적으로 예를 들면, 대표적인 예측 기법인 회귀분석에 이진 분류 함수인 Sigmoid를 적용하면 로지스틱 회귀라는 분류 분석 알고리즘으로 바뀝니다. 이때, Sigmoid 함수는 모든 입력값에 대해 0과 1사이로 들어오도록 확률 값 변환 처리를 수행해 주는데, 그 확률 값을 기반으로 target이 0이냐 1이냐로 분류해 내는 방식입니다.​물론 모든 알고리즘이 이런 구조로 분류를 수행하는 것은 아닙니다. 본문에서 살펴볼 RandomForest, XGBoost, LightGBM 같은 Tree 기반 모델들은 애초에 분류를 수행하기 위한 목적으로 설계되어 있습니다. Tree 모델은 입력받은 데이터에 대해 yes or no를 질의해가며 가지치기 형태로 정밀하게 분류해나가는 방식입니다. 본 튜토리얼은 파이썬 활용에 초점을 두고 있으므로 구체적인 알고리즘을 다루지는 않으니 구글링을 통해 적극적으로 찾아가며 학습하시길 권장 드립니다.​2-3. 지도학습 예측값 y 정의자, 이제 우리는 지도학습을 수행할 것이고, 지도학습을 수행한다면 예측하고자 하는 타겟(label, y)이 필요하다는 것을 알고 있습니다. 지난 튜토리얼에서 내일의 주가는 예측할 수 없다는 결론을 내렸습니다. 과연 기계학습을 통해 주가를 예측해 보면 어떨까요? 다만, 정확한 주가를 예측하기보다 내일의 주가가 오를지, 내릴지에 대해서만 ‘분류’하는 것으로 테스크 난이도를 낮춰주면 좀 더 의미 있는 성과를 기대해 볼 수 있겠습니다.​파이썬의 list comprehension 문법을 통해 y를 만들어줍니다. y의 이름은 ‘up’입니다.diff() 함수는 바로 직전 row(행)과 현재 row(행)의 차이를 계산합니다. 종가에 대해 diff() 함수를 수행해 주면 아래와 같이 전일 대비 즘 감음 계산할 수 있습니다.이렇게 나온 값들을 위에서부터 아래로 순회하며 만약 값이 0과 같거나 크면 1, 그렇지 않다면 0으로 표시합니다. 이것을 labeling이라고 하며, 일반적으로 이진 분류의 경우 이렇게 1과 0으로 분류 대상을 마킹해줍니다.​결과적으로 오늘 종가가 전날 종가와 같거나 상승했다면 ‘up’이 1로 표기되고 그렇지 않은 경우 0으로 표기됩니다. 여기서 주의해야 할 점이 있습니다. 우리가 2010년 6월 29일의 종가와 2010년 6월 30일의 종가를 보고 y를 마킹했는데, 모델이 2010년 6월 30일의 y(‘up’ 여부)를 예측해야 하는 상황이라면 어떤 날짜의 데이터(Close)를 보도록 해야 할까요? 다시 말해, X(변수)의 시점은 언제일까요?2010년 6월 29일입니다. 만약, 2010년 6월 30일 데이터(X)를 보고 up(y)을 예측하도록 시킨다면 모델은 100% 확률로 모두 정답을 가려낼 것입니다. 우리가 ‘어제 대비 오늘 증가했는가?’를 수식으로 계산해 y를 만들었는데, 우리가 이렇게 수식을 계산할 수 있다면 모델은 너무 간단히 함수를 정의해버릴 것입니다. 사실, 이것은 오늘의 결과를 미리 보고 ‘up’을 예측한 것과 같고, 이를 미래 참조 편향(lookahead bias)이라고 합니다. 따라서 예측하고자 하는 대상과 예측에 사용될 변수를 이러한 오류가 없도록 설정해 주는 것이 매우 중요합니다.​다시 돌아와서, 우리는 모델 학습에 ‘시가, 고가, 저가, 종가, 거래량’ 이렇게 5가지 feature(x, 각각의 변수)만 사용할 것입니다. 일명 캔들 차트를 구성하는 4개 요소와 일별 관심도 혹은 유입/유출량을 보여주는 거래량 정보를 가지고 다음 날의 종가 상승 여부를 예측(분류) 해보고자 합니다. 따라서 아래와 같이 인덱싱해주고, 칼럼은 소문자로 변환해 주겠습니다.그다음, 모델이 학습할 수 있도록 X와 y를 나눠줍니다.​Step 3. 모델 인스턴스 생성 및 파이프라인 정의​3-1. 인스턴스 생성sklearn 및 각종 패키지를 통해 모델 클래스를 불러와 각각 인스턴스를 생성해 줍니다.​3-2. 파이프라인 정의여기서 파이프라인은 모델 학습 및 평가 프로세스를 의미하며, 다음과 같은 흐름으로 진행됩니다. X, y 데이터를 각각 학습(train)용, 평가(test) 용으로 분할합니다. 모델은 학습 데이터로 주어진 X_train, y_train를 학습합니다. 학습된 모델은 X_test를 평가(예측) 합니다. 평가 결과(y_pred)를 실제 값 y_test와 비교해 정확도를 측정합니다. ​이러한 프로세스를 함수로 구현해두고, 간단히 함수를 호출하는 것만으로 데이터 처리, 모델 학습, 평가까지 한 번에 수행될 수 있도록 합니다.sklearn에서 제공하는 train_test_split() 함수는 파이프라인 중 Flow 1을 수행합니다. test_size=0.2, shuffle=True로 설정하면 전체 데이터 중 학습 데이터를 80%, 평가 데이터를 20%로 ‘랜덤하게’(shuffle) 추출해 줍니다. 여기서 random은 완전한 무작위 추출은 아니고, 지정한 random_state에 따라 정해진 로직으로 무작위 추출을 하게 되는데 저와 동일하게 42로 설정하면 내부적으로 동일한 알고리즘에 따라 데이터를 추출할 수 있습니다. 즉, random이라도 재실행했을 때 동일한 결과(분할된 데이터)가 나올 수 있도록 세팅되는 것입니다.​그리고 X.iloc[:-1], y.iloc[1:] 부분은 각각 분할하고자 하는 대상인 X, y를 넣어주는 곳인데, 위에서 설명한 것처럼 우리가 예측하고자 하는 타겟은 ‘다음 날의 종가가 올라있을지 여부’이기 때문에 X는 첫날부터 마지막 날 하루 전까지로 지정하고, y는 둘째 날부터 마지막 날까지로 설정해 주는 것입니다. 이렇게 하면 (X, y)를 (첫날, 둘째 날), (둘째 날, 셋째 날)과 같이 매칭 시킬 수 있습니다.다음으로 if 함수를 통해 diff=True인 경우에는 X.iloc[1:-1], y.iloc[2:]로 설정해서 (첫날, 둘째 날)이 아니라 (둘째 날, 셋째 날)부터 데이터가 시작될 수 있도록 해줬는데, 아래와 같이 해당 함수를 사용하면 첫날의 데이터는 NaN이 되기 때문입니다.​Flow 2는 모델을 학습하는 구간, Flow 3은 평가(예측) 하는 구간입니다. 함수에서는 model.fit(X_train, y_train)과 model.predict(X_test)로 작성돼있습니다. 기계학습 모델은 데이터를 fitting(적합) 시킨 다음 predict(예측) 합니다. 이것이 함수로는 fit()과 predict()로 구현되어 있으며 우리는 이를 순서대로 실행해 주면 됩니다.​그리고 함수 인자에서 볼 수 있듯 학습할 때는 학습 데이터인 X_train, y_train을 사용하고 예측할 때는 평가 데이터인 X_test를 사용합니다. 모델이 이미 학습에 사용한 데이터(X_train)에 대한 label(y_train)를 알고 있기 때문에 예측은 X_test에 대해서만 수행하는 것입니다. 만약 train, test 데이터를 분리하지 않고 모두 학습에 사용했다면 그중 일부(예를 들면 X_test)에 대해 예측을 수행했을 때 정답(y_test)을 100% 확률로 맞춰낼 것입니다. 모델이 답안지를 미리 봤기 때문입니다.​Step 4. 모델링 및 평가우리는 본 실습에서 3가지 형태의 데이터 셋을 학습 및 평가해 볼 것입니다. 이전 튜토리얼에서 시계열, 특히 주가 데이터를 분석할 때 차분 및 로그 차분 데이터를 사용할 필요가 있다는 점을 설명한 바 있습니다. 따라서 여기서도 원본 주가 데이터, 차분한 주가 데이터, 로그 차분한 주가 데이터를 각각 모델에 넣고 예측을 수행해 보고자 합니다.4-1. 기본 데이터앞서 총 4개의 모델 인스턴스를 정의했습니다. 다시 확인하겠습니다.위 모델을 반복문을 통해 순차적으로 학습시키고, 정확도를 확인해 보도록 하겠습니다.acc_li 리스트는 우리가 파이프라인으로 정의한 show_ml_acc()의 return 값인 acc를 받습니다. 모든 모델마다 acc를 받으면 총 4개의 acc가 쌓이고, 이를 평균 내서 마지막 줄에 print() 함수로 출력해 줬습니다. 그러면 해당 데이터로 모델들을 학습시켰을 때, 평균적인 정확도를 확인할 수 있습니다.​여기서 우리는 단순히 ‘모델 정확도가 51% 정도 되네’ 하고 넘어가선 안됩니다. 더 중요한 것은 이 부분입니다.precision은 정밀도입니다. 즉, 모델이 y를 0 혹은 1이라 분류했는데, 이것이 얼마나 정밀하게 잘 들어맞는가?를 보여줍니다. LogisticRegression의 경우 모델이 1이라 예측한 것 중 실제 1인 비율이 51%네요.​recall은 재현율입니다. 실제 1이 308개(support에 표기되어 있습니다.) 있는데, 그중 몇 개나 동일하게 재현해냈는가?(맞췄는가?)를 보여주는 지표입니다. 이 경우, 308개 모두 맞췄네요.그런데 좀 이상합니다. 0에 대해서는 모두 정확도가 0%입니다. 이것은 데이터를 들여다볼 필요가 있습니다.predict_proba() 함수는 각 label(0, 1)에 대해 몇 %의 확률로 실제 정답이라 예측하는지 알려주고, predict() 함수는 그러한 예측 확률에 의해 나온 예측 결과를 1차원 배열로 이어서 보여줍니다. 보시면 모두 1로 예측한 것을 알 수 있습니다.모델 정확도를 대표하는 Accuracy는 0을 0이라 하고, 1을 1이라 예측한 것의 비율을 보여줍니다. 그래서 단순히 Accuracy만 본다면, LogisticRegression의 경우 모두 1로 예측했기 때문에 전체 데이터 606개 중 308개를 맞추고 298개를 틀렸으니 51%의 정확도가 나오는 것입니다. 따라서 우리는 precision과 recall을 잘 살펴봐야 하는 것입니다. 특히 주가 예측의 경우 모델이 예측해서 특정 action을 취했는데 모델이 오답을 낸 경우 매우 큰 리스크를 지게 됩니다. precision이 더 중요한 task라는 의미입니다. 일반적으로 모델이 특정한 균형 상태에 도달했을 때, trade-off 관계인 precision과 recall을 동시에 높이기는 어렵기 때문에 한쪽의 성능을 최대한 끌어올리는 방향으로 최적화를 하게 되는데, 암 진단이나 지금의 주가 예측같이 ‘오답 시 리스크’가 큰 경우는 precision을 높이고, 사기 탐지나 코로나 검사처럼 ‘발견 실패 시 리스크’가 큰 경우는 recall을 높입니다.​그러나 이렇게 한쪽이 극단적으로 높거나 낮게 되면 해당 모델에 적합하지 않은 데이터 분포를 사용했거나, 모델 자체가 현재의 Task에 적합하지 않은지 의심해 볼 필요가 있습니다. LogisticRegression은 선형회귀를 기반으로 하기 때문에 비선형 구조의 복잡한 패턴을 찾는 데에는 어려움이 있습니다. 따라서 데이터의 복잡도를 낮춰주는 방향으로 개선하거나 비선형 모델링이 가능한 다른 모델을 사용해야 합니다.​트리 기반 알고리즘은 복잡한 데이터를 단순한 로직으로 잘 학습하는 대표적인 모델입니다. 아래와 같이 precision과 recall 모두 안정적으로 확인되며 특히 y=1에 대한 정밀도(precision)을 보면, ‘상승하는가?’에 대해서는 ‘찍기’보다 조금 나은 0.51~0.53 정도의 정확도를 보여주고 있습니다.​4-2. 차분 데이터그렇다면, 데이터 복잡도를 낮춘 차분 데이터는 어떨까요? 차분을 수행하면 ‘어제 대비 오늘의 증감’으로 데이터를 단순화시킬 수 있고, 해당 데이터로 내일의 상승 여부를 예측하는 Task가 됩니다.이렇게 성능은 타 모델 대비 좋지 않은 편이지만 차분을 해주는 것만으로도 LogisticRegression의 성능을 높여줄 수 있습니다. 그러나 데이터 복잡도가 낮아지면 트리 모델의 성능은 위와 같이 상대적으로 더 높아지기 때문에 해당 Task에 LogisticRegression를 굳이 사용할 이유는 없겠습니다.4-3. 로그 차분 데이터데이터에 로그를 씌우고 차분해주면 추세나 계절성 등의 규칙적인 움직임은 대부분 사라지고 그러한 움직임에 내재된 불규칙한 패턴만 남게 됩니다. 차분을 해준다는 점에서 복잡도를 덜어낼 수 있지만 로그가 비선형 함수이기 때문에 LogisticRegression은 이번에도 역시 고전을 면치 못할 것입니다.LogisticRegression의 Recall이 기본 데이터를 사용했을 때와 비슷하게 대부분 1(up)로 예측을 해버렸네요. 이렇게 비선형 처리가 들어가게 되면 선형회귀 기반의 함수는 관측치를 제대로 설명하기 어려워집니다.​반면, 나머지 트리 기반 모델들도 이전보다 성능이 좋진 않습니다. 주가에 로그 차분을 수행했을 때, 너무 많은 정보 손실이 발생했기 때문입니다.이처럼, 아주 간단한 모델링에도 신경 쓸 부분이 많습니다. 기계학습이 정말 기계적으로 알아서 분석하고 예측해 주면 좋겠지만, 모델이 어떤 원리로 동작하는지 이해하고 해석해 보면서 적합한 데이터 처리 과정을 수행해 줘야 제대로 된 예측 및 분류 모델을 개발할 수 있습니다. 그러나 동시에 아무리 복잡한 데이터라도 기계학습 알고리즘을 사용하면 유의미한 관계식을 구현할 수 있다는 점은 대단한 가능성을 열어줍니다. 따라서 이러한 알고리즘을 간단히 구현할 수 있게 도와주는 파이썬이라는 도구는 우리가 데이터를 다루고자 한다면 능숙하게 다뤄야 하며, 몸에 익을수록 더 큰 효용을 가져다줄 것입니다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "금융 분석을 위한 파이썬 프로그래밍 - 03. 회귀 분석, 정규성 검정, 상관 분석, 쌍체 검정", "url": "/posts/%EA%B8%88%EC%9C%B5-%EB%B6%84%EC%84%9D%EC%9D%84-%EC%9C%84%ED%95%9C-%ED%8C%8C%EC%9D%B4%EC%8D%AC-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-03-%ED%9A%8C%EA%B7%80-%EB%B6%84%EC%84%9D-%EC%A0%95%EA%B7%9C%EC%84%B1-%EA%B2%80%EC%A0%95-%EC%83%81%EA%B4%80-%EB%B6%84%EC%84%9D-%EC%8C%8D%EC%B2%B4/", "categories": "3. 튜토리얼, 금융 분석 프로그래밍 기초", "tags": "", "date": "2023-02-01 00:00:00 +0900", "snippet": "Step 1. 데이터 불러오기이제 데이터 불러오는 작업은 익숙해졌을 것입니다. 지난 글과 동일하게 야후파이낸스 패키지로 2010년 상장일부터 최근까지의 테슬라 주가를 가져오겠습니다. (작성시점 : 2022-07)​Step 2. 데이터 분할우선, 전체 기간의 데이터(종가)를 확인하겠습니다.데이터를 봤을 때, 2020년 전후로 움직임에 많은 차이가 있는 것 같습니다. 2020년 이전에는 매우 잔잔했던 반면 그 이후는 급등과 급락을 반복하고 있습니다. 최근 2년간 테슬라 주가는 유례없는 움직임을 보이고 있습니다.데이터를 분석할 때는 분석하고자 하는 대상이 분명해야 합니다. 여기서는 ‘2020년 이전과 이후, 테슬라 주가 추세는 통계적으로 차이가 있는가?’를 확인하고자 합니다. 눈으로 봤을 때는 당연한 것이지만 이런 궁금증을 통계적으로 어떻게 확인할 수 있는가?를 배우는 데에 의의를 가져봅니다.​2-1. 시점(2020년) 기준으로 데이터 분할하기2020년 이전과 이후를 비교할 것이므로 해당 시점을 기준으로 데이터셋을 분리합니다. 1장에서 배운 인덱싱을 활용합니다.print() 문으로 값을 출력할 때, f-string이라는 문자열 포멧팅을 사용했습니다. f로 시작해 따옴표(‘)로 감싸면, 그 안에서 중괄호({, })에 넣어준 변수를 변수에 담긴 값으로 문자열을 변환해 주게 됩니다. {close_before.describe}, {close_after.describe}가 그것입니다. 만약 문자열 포멧팅 없이 그냥 변수를 넣어줬다면 아래와 같이 출력되겠죠.참고로 가운데 ‘\\n’(엔터키 바로 위 자판 + n입니다. 윈도우에서는 원화 표시, 맥에서는 역슬래시입니다.) 은 ‘줄바꿈’을 의미합니다.​이제, 분할한 데이터를 한 그래프에 표시하면 아래와 같이 연결되어 그려질 것입니다.​2-2. 데이터 길이 맞추기집단 간 비교 분석을 위해서는 데이터 길이가 일치해야 합니다. 2020년 이전의 데이터가 상대적으로 더 많기 때문에 가까운 시점을 기준으로 2020년 이후 데이터 길이와 동일하게 잘라주겠습니다.close_after의 길이를 변수 l에 담고, iloc 인덱싱을 통해 뒤쪽부터 l 만큼 잘라줬습니다. 잘라준 값은 다시 close_before에 담았습니다.데이터 길이를 맞췄다면, x 축의 기간 정보를 제거하고 하나의 X축 선상에 그려주겠습니다.그려놓고 보니 before의 움직임은 거의 보이지 않습니다. 상대적으로 after의 데이터 분포가 더 넓기 때문입니다. 최근 주가가 많이 상승한 까닭입니다. 이럴 때에는 동일한 스케일의 분포로 맞춰주는 것이 좋습니다. 분포를 동일하게 맞춰야만 두 기간의 주가 차이가 아니라 주가 추세에 주목할 수 있기 때문입니다. 서두에 언급한 것처럼 우리가 분석을 통해 확인하고자 하는 것은 ‘주가 추세에 차이가 있는가?’입니다.​2-3. 데이터 분포 맞추기(스케일링)스케일링 방법은 다양하지만 그중에서도 가장 직관적으로 이해하기 쉬운 Min-Max Scaling 방식을 사용하겠습니다. Min-Max Scaling은 최댓값과 최솟값의 차이를 기준으로 분포 내 각각의 데이터와 최솟값의 차이를 나타내는 방식입니다. 이를 통해 어떤 데이터라도 그 분포를 0과 1사이로 변환할 수 있습니다.y 축을 보면 0과 1사이에 모든 데이터가 들어온 것을 확인할 수 있습니다.Step 3. 통계 분석3-1. 회귀 분석정말 간단하게 회귀선을 그리는 법을 알아보겠습니다. 회귀선은 기본적으로 인과관계를 나타내기 위해 두 변수 간 관계를 1차원 수식으로 정의하는 것입니다. 비선형(2차원 이상) 그래프로 적합시키는 것이 아니기 때문에 모든 형태의 데이터 관계성을 표현하기는 어렵습니다. 또한, 지금의 케이스처럼 서로 다른 두 시점 간 차이를 볼 때에는 사실 회귀 분석은 적합한 방식이 아닙니다. 하지만 파이썬으로도 아주 간단하게 회귀선을 그려보고 회귀계수와 유의수준을 확인해 볼 수 있음을 알고 나면 추후 인과 분석 테스크에서 유용하게 활용할 수 있겠습니다.​먼저 이렇게 두 변수를 2차원 평면의 각 축에 놓고, 변수 간 상대적 데이터를 산점도로 표시합니다. x축은 before, y 축은 after로 두었습니다.그다음, seaborn 패키지의 regplot() 함수를 사용해 regression 그래프를 그려줍니다.seaborn은 matplotlib을 기반으로 통계 그래프를 그릴 수 있게 확장된 시각화 라이브러리입니다. 따라서 그래프 함수 자체에 대한 matplotlib과 기본적인 사용법은 유사하고, matplotlib 함수를 사용해 레이아웃이나 타이틀 등을 동일하게 설정할 수 있습니다.​이렇게 회귀선을 적합할 때는 내부적으로 OLS(최소자승법, Ordinary Least Squares)라고 해서 2차원 평면 내 모든 점들과의 거리가 최소가 되는 하나의 1차원 선을 그려내는 기법을 사용합니다. 최소 자승(제곱) 법이라는 이름이 붙은 것은 점이 선보다 아래에 있으면 음수, 위에 있으면 양수가 나올 텐데 이것을 그대로 합하면 제대로 거리를 계산해낼 수 없으니 점과 선간 거리를 모두 ‘제곱’해서 합하는 방식을 사용하기 때문입니다.이 OLS 기법을 통해 제대로 식이 계산되었는지 확인하고자 한다면 statsmodels 패키지를 사용하면 됩니다.sm.OLS 함수는 endog와 exog라는 2개의 인자를 받습니다. endog는 endogenous의 약자로 내생변수이자 종속변수인 y를 의미합니다. 그리고 exog는 exogenous의 약자로 외생변수이자 독립변수인 x를 의미합니다. sm.add_constant() 함수로 scaled_before.values를 묶어준 것은 회귀식에 상수항을 추가하기 위함입니다.위에서 확인한 것처럼 회귀선이 (0,0) 좌표에서 출발하지 않고 y가 1에 가까운 상단에서 시작됩니다. 따라서 1에 가까운 상수항을 먼저 표현해 주고, 그것에서부터 출발한 기울기를 구할 수 있습니다.​위쪽의 붉은 박스 R-squared(결정 계수)는 0에서 1사이로 회귀선이 관측치들을 얼마나 잘 표현하는가를 알려줍니다. 선으로부터 점들이 멀수록 R-squared 값은 0에 가까워집니다. 현재 산점도로 볼 때, 점의 흩어짐 정도가 강하기 때문에 높지 않은 수치를 보여주고 있습니다.​ 그 아래 박스를 보면 상수항(const), 그리고 독립변수(x1)의 계수와 t 값 등이 표기되어 있습니다. 그중에서 지금 회귀선을 표현하는 변수들, 그러니까 상수항과 독립변수가 각각 유의한가를 알려주는 것이 P&amp;gt; t 부분이 되겠습니다. 이것은 P-value를 나타내는 것으로, 0.05 이내에 들어오면 ‘유의하다’라고 말합니다. 즉, 지금의 회귀선은 신뢰할 수 있는 const와 x1 계수(coef, coefficient)로 표현되었다고 할 수 있습니다. 3-2. 정규성 검정이렇게 간단히 회귀 분석을 알아봤고, 이제부터는 상관 분석, 쌍체 검정을 실시할 것입니다. 그전에 선행되어야 할 것이 바로 정규성 검정입니다. 상관 분석을 하고자 하는 변수들은 각각 정규성을 확보하는 것을 전제로 합니다. 만약 정규성을 확보하지 못했다면 30회(개) 이상의 충분한 관측 표본으로 구성되어야만 합니다. 표본이 정규분포를 띄지 않더라도 표본이 충분하다면, 중심극한정리에 의해 모집단은 정규분포로 수렴할 수 있습니다. 중심극한정리와 정규분포에 대해서는 구글링을 통해 더 자세히 알아보시길 추천드립니다.​일반적으로, 표본이 충분하더라도 정규성 검정을 통해 분포는 확인하고 넘어가는 것이 좋습니다. 표본 자체의 정규성은 표본대 표본 간 비교 분석의 신뢰성을 더해주기 때문입니다. 따라서 이번 분석에서도 정규성 검정을 실시해 봅니다.subplot을 사용해 1행 2열 레이아웃으로 그래프를 그립니다.bins 옵션을 통해 몇 개의 bar로 차트를 구성할 것인가에 대한 정보를 입력해 줬습니다. 각 분포를 눈으로 봤을 때, 둘 다 정규분포로 보이지는 않습니다. 특히 After(2020년 이후)의 경우 그 정도가 더 강합니다. 정규성 검정은 kolmogorov-smirnov 방식을 사용하며 수치/과학 연산을 위한 scipy 패키지에서 제공하는 kstest()로 간단히 수행할 수 있습니다.kolmogorov-smirnov 검정은 ‘정규분포를 따른다’를 귀무가설로 합니다. Before, After 모두 p-value가 0.05보다 작으므로 귀무가설을 기각할 수 있습니다. 즉, 두 기간의 종가(표본) 분포는 정규분포를 따르지 않습니다.​하지만, 표본이 충분하므로 상관 검정과 쌍체검정(T-Test 유형 중 하나)를 실시할 수 있습니다.​3-3. 상관성 검정상관성 검정의 경우 대표적으로 피어슨 방식과 스피어만 방식이 있습니다. 피어슨 검정의 경우 표본이 충분하거나 정규분포 가정을 만족하는 경우 사용 가능하며, 스피어만 검정은 그렇지 않은 케이스에 대해 표본의 순위-비교 방식으로 상관성을 확인할 수 있습니다.여기서는 표본이 충분해야 한다는 조건을 만족하므로 피어슨 검정을 사용합니다. 피어슨 검정 역시 scipy 패키지를 통해 사용할 수 있으며 귀무가설은 ‘두 잡단은 서로 선형적 상관관계가 없다’입니다.p-value &amp;lt; 0.05로, 귀무가설을 기각합니다. 즉, 두 기간에 대한 추세는 서로 상관관계가 있습니다. 그런데 stats를 보면 -0.65로 음의 상관관계를 보입니다. 같은 주가의 다른 기간에 대해 음의 상관관계를 보인다는 것은 추세가 뒤바뀌었다는 것으로 해석해 볼 수 있겠습니다.​3-4. 쌍체 검정사실 엄밀히 말하면 위 상관 분석 역시 전제에 오류가 있습니다. 상관 분석 시 비교 대상 타겟은 서로 다른 두 모집단입니다. 다시 말해, 서로 다른 두 모집단에서 발생한 각각의 표본을 분석해 모집단 간의 상관성이 있냐? 없냐?를 분석하는 것입니다. 그러나 before, after가 서로 다른 모집단에서 발생한 표본이라고 정의할 수 없습니다. 기간이 다를 뿐, 둘 다 테슬라의 주가(종가)라는 동일 집단에서 추출한 샘플이기 때문입니다.따라서 이런 경우 T-Test 중에서 특정 이벤트 발생 전후 차이를 비교할 수 있는 쌍체 검정(Paired-Sample T-Test)을 사용할 수 있습니다. 역시 scipy 패키지를 사용하며, 귀무가설은 ‘두 분포 간 평균에는 차이가 없다’입니다.p-value&amp;lt;0.05로, 두 분포 간 평균에 차이가 있음을 확인했습니다. 단순히 종가 데이터로 비교한 것이 아니라 기간 범위를 맞추고 0과 1 사이로 스케일링했을 때, 이와 같이 각 집단 평균이 서로 유의하게 다르다면, 우리는 2020년을 기점으로 추세가 바뀌었다고 해석할 수 있겠습니다.​이번에는 이렇게 파이썬을 활용해 여러 가지 통계 검정을 실시해 보았습니다. 이외에도 statsmodels와 scipy 패키지는 많은 통계 분석 함수들을 제공하고 있으니 스스로 가설을 세우고 직접 분석해가면서 학습을 이어나가시면 좋겠습니다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "금융 분석을 위한 파이썬 프로그래밍 - 02. 차트 시각화, 시계열 분해, 자기상관분석", "url": "/posts/%EA%B8%88%EC%9C%B5-%EB%B6%84%EC%84%9D%EC%9D%84-%EC%9C%84%ED%95%9C-%ED%8C%8C%EC%9D%B4%EC%8D%AC-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-02-%EC%B0%A8%ED%8A%B8-%EC%8B%9C%EA%B0%81%ED%99%94-%EC%8B%9C%EA%B3%84%EC%97%B4-%EB%B6%84%ED%95%B4-%EC%9E%90%EA%B8%B0%EC%83%81%EA%B4%80%EB%B6%84%EC%84%9D/", "categories": "3. 튜토리얼, 금융 분석 프로그래밍 기초", "tags": "", "date": "2023-01-31 00:00:00 +0900", "snippet": "Step 1. 데이터 불러오기이제 데이터 불러오는 것은 어렵지 않습니다. yfinance 라이브러리를 사용해 테슬라(TSLA) 주가 정보를 가져오겠습니다. stocks 인스턴스를 생성했고, 객체 내에 내장된 history() 함수로 2010년부터 최근 데이터까지 가져왔습니다. (작성 시점: 2022-07)​Step 2. 차트 시각화파이썬에서 사용하는 대표적인 시각화 라이브러리는 matplotlib과 seaborn이 있습니다. 여기서는 matplotlib을 사용해 차트를 그리는 법을 배워보겠습니다. 참고로 seaborn은 matplotlib을 기반으로 더 다양한 색상과 통계 차트를 제공하고 있으니, 검색해 보시길 권장 드립니다.처음 보는 라이브러리를 사용하게 되면 어떻게 구현되어 있는지 깃허브 등을 통해 살펴보면 좋습니다. 모든 라이브러리마다 들여다보는 것은 효용이 좋지 않겠지만, 관심을 가지고 여유로울 때 살펴보면 라이브러리를 유연하게 잘 사용하는 데에 많은 도움이 될 것입니다. matplotlib은 한 번 내부를 먼저 살펴보도록 하겠습니다.​github 주소로 들어가면 lib 폴더가 보입니다. 바로 여기에 matplotlib으로 사용할 수 있는 패키지들이 내장되어 있습니다.여기서 matplotlib 폴더를 한번 더 들어가주면 아래와 같이 pyplot.py 모듈을 확인할 수 있습니다.내부에 들어가 보면 우리가 pyplot을 통해 자주 사용하는 figure()나 subplot() 등의 유용한 함수들이 얼마나 정교하게 잘 개발되어 있는지 살펴볼 수 있습니다.이런 식으로 파이썬 라이브러리를 사용할 때, 라이브러리가 어떻게 구현되어 있는지 관심을 가지고 살펴보면 코드와 조금 더 친해질 수 있습니다.​2-1. 기본 선 그래프다시 돌아와서, matplotlib의 pyplot 모듈을 plt라는 이름으로 불러오겠습니다.plt.style.use() 함수를 사용하면 그래프 스타일을 적용할 수 있습니다. 여기서는 seaborn 스타일을 적용했으나 다른 스타일도 확인해 보고 싶다면 링크를 참고해 주세요.우리는 df 변수에 데이터 프레임을 담았습니다. ‘Close’컬럼만 불러와서 데이터 프레임에 내장된 plot() 함수를 사용해 주면 이렇게 차트가 그려집니다. 이때, x축은 df의 인덱스, y 축은 ‘Close’ 변수가 됩니다.plt.show()는 print() 함수와 비슷한 역할을 한다고 생각해 주세요. “차트를 그려라” 하고 명령하는 것입니다.​데이터 프레임 내장 함수가 아닌 pyplot의 plot() 함수를 사용할 수도 있습니다. 결과는 같습니다.​2-2. 그래프 채우기pyplot()의 fill_between() 함수를 사용하면 plot() 함수를 사용했을 때 생기는 선형 그래프 아래 영역을 채워줄 수 있습니다.alpha 파라미터로 색상 농도를 조절할 수 있습니다. 1이 default이며, 0으로 갈수록 희미해집니다.​2-3. 산점도pyplot은 단순 선형 그래프 외에도 다양한 그래프를 지원합니다. 데이터가 흩어진 정도를 보여주는 산점도는 시계열 데이터 상에서 얼마나 밀집해있는가를 확인하기 위해서도 쓰입니다. 산점도를 그리려면 scatter() 함수를 사용하면 됩니다.s 파라미터를 넣지 않으면 점의 크기가 매우 커서 보기 좋지 않을 것입니다. 위와 같이 s=0.8 정도로 지정해 주면 충분한 양의 데이터를 대부분 보기 좋게 그려줍니다.​fill_between() 함수와 scatter() 함수의 실행 결과를 한 번에 보여줄 수도 있습니다. plt.show()가 ‘그려라’라는 역할을 하기 때문에 순서대로 작성해 주고 마지막에 plt.show() 함수만 실행해 주면 됩니다.​2-4. 기준선 및 구간 표시우리가 프레젠테이션을 하거나 어떤 설명을 할 때, 차트를 놓고 x축 혹은 y 축에 기준선을 표기해 줄 때가 많습니다. 이럴 때에는 axvline(), axhline() 함수를 사용할 수 있습니다. 만약 기준선이 아니라 기준 영역을 그리고 싶다면 axvspan(), axhspan() 함수를 사용하면 됩니다. 색상과 스타일 등은 아래와 같이 파라미터로 적용할 수 있습니다.​2-5. 타이틀 표기y 축과 x축에 단위 표기나 이름(label)을 설정해 줘야 할 수도 있습니다. 그리고 그래프의 제목을 표기하고 싶을 때도 있습니다. ylabel(), xlabel() 함수는 각 축에 해당하는 위치에 텍스트를 작성하게 해주고, title()은 그래프 위에 타이틀을 작성하게 해줍니다.​Step 3. 장/단기 추세3-1. 3일 이동 평균주가 추세를 분석하거나 기술적 지표를 확인할 때 이평선(이동평균선)이라는 용어가 자주 쓰입니다. 지난 n 일 간의 가격 평균을 연속해서 보여주는 것인데, n 일이라는 기간 동안 만들어진 추세를 시간의 흐름에 따라 살펴볼 수 있는 유용한 지표입니다.​이동평균 데이터는 파이썬으로도 간단하게 확인할 수 있습니다. Close(종가)에 대해 rolling(n) 함수를 적용하면 n 일에 대한 통계치를 계산할 수 있는 객체가 됩니다. 이 객체는 n 일간의 데이터를 담고 있으며 min(), std(), median(), mean() 함수를 통해 n 일간의 통계치를 계산할 수 있습니다. 여기서는 mean() 함수를 사용해 간단히 3일 이동평균 데이터를 구해보았습니다.fill_between()과 scatter() 함수로 차트를 겹쳐 그릴 수 있었던 것처럼 이동평균선도 산점도와 함께 그릴 수 있습니다.데이터 범위(기간)가 너무 커서 잘 보이지 않네요. 2022년만 확대해서 살펴보겠습니다. 이때는 인덱싱을 통해 2022년만 추출하면 됩니다. 시계열 데이터를 인덱싱할 때에는 간단히 ‘2022’ 숫자만 넣어주면 2022년에 해당하는 데이터를 모두 가져옵니다. 단, 겉보기에 시계열 데이터라도 데이터 타입이 문자열이거나 실수형일 수 있으니 문제가 생긴다면 인덱스가 datetime64 타입이 맞는지 확인해야 합니다.​3-2. 30일 이동 평균이번에는 30일 이동 평균 그래프를 그려볼 것입니다. rolling(n) 함수에 n만 바꿔주면 됩니다. 30일 치 이동평균이기 때문에 첫 29일까지는 평균값이 없습니다. axvspan() 함수를 통해 평균값이 있는 기간을 같이 표시해 보겠습니다.​​Step 4. 시계열 분해4-1. 장기 주가 데이터시계열 데이터에는 이렇게 이동평균으로 확인할 수 있는 추세만 있는 것이 아니라 계절성과 잔차라는 요소가 있습니다. 계절성은 관측치(observed, 주가)에서 추세(trend)를 제거하고 남은 데이터 중 반복되는 패턴을 잡아서 보여주며 잔차(residual, resid)는 계절성까지 제거되고 남은 값을 보여줍니다. 따라서 잔차에서는 특정 반복 패턴, 커지거나 작아지는 움직임, 급격히 튀는 구간 등이 남아 있으면 안 됩니다. 모든 패턴과 움직임이 추출되고 남은 ‘나머지’는 random, 즉 무작위적 변동성이어야 하기 때문입니다.seasonal_decompose 함수는 이러한 값들을 한 번에 뽑아낼 수 있도록 해줍니다. statsmodels 라이브러리에서 가져와 사용할 수 있습니다.seasonal_decompose 함수에는 관측할 시계열 데이터와 추세를 추출할 기간을 입력해 줍니다. 30일 치 이동평균을 추세 추출 기준으로 잡았기 때문에 freq=30으로 설정해 주었습니다. 이렇게 생성된 인스턴스는 res라는 변수에 담고, res.observed, res.trend, res.seasonal, res.resid를 통해 각각 관측치, 추세, 계절성, 잔차를 뽑아낼 수 있습니다.​다음으로 시각화 부분입니다. pyplot은 figure() 함수를 제공합니다. 여기에 figsize=(x, y)를 지정해 주면 x축 크기, y 축 크기를 설정할 수 있고, subplot() 함수로 설정된 크기를 나눠서 할당해 그래프를 그릴 수 있습니다.subplot(xyi)와 같이 사용하는데, 이는 x 행 y 열만큼의 서브 그래프 공간을 할당하여 i 번째 위치에 그래프를 그리겠다는 의미입니다. 예를 들어 subplot(411)의 경우, 4행 1 열짜리 서브 그래프 공간을 할당하여 맨 위쪽에 그래프를 그립니다. 여기서는 observed 그래프가 되겠습니다.plt.xticks()는 x축을 의미하는데, 빈 리스트([])를 넣어주면 x축을 표시하지 않게 됩니다. subplot()을 총 4개 사용했고, 맨 하단 그래프가 x축을 표시해 주고 있기 때문에 나머지 위쪽 3개는 x축을 표시하지 않도록 plt.xticks([])를 작성해 주었습니다.다음으로, plt.suptitle()은 이렇게 서브 그래프를 그릴 때 통합 타이틀을 기재해 줄 수 있습니다. 여기서 plt.title()을 쓴다면 해당 위치의 서브 그래프에만 타이틀이 기재될 것입니다.여기서 주의 깊게 봐야 하는 부분은 ‘잔차(resid)’인데, 잔차가 2020년 이전과 2022년 이전, 이후 움직임이 크게 다릅니다. 이렇게 잔차의 움직임 변화가 눈에 잘 보이면 제대로 패턴을 추출해 내지 못한 것입니다. 잔차는 앞에서 언급한 것처럼 ‘무작위성’을 보여야 하기 때문입니다. 진폭이 점점 커지는 것도 ‘발견 가능한 패턴’의 일종입니다.4-2. 단기 주가 데이터이때, 우리는 잔차가 일정한 범위 내에서 불규칙성을 보이는 2022년만 보면 어떨까?라고 생각해 볼 수 있습니다.그러나 이렇게 했을 때 데이터가 너무 적어져서 잔차가 정말 랜덤 한지 판단하기 어렵습니다. 따라서 분 단위 데이터를 확보하거나 충분한 기간의 데이터에 적절한 처리를 해줘서 잔차의 랜덤(무작위)을 확보해야 합니다.4-3. 장기 로그-주가 데이터그래서 일반적으로 주가 분석을 할 때에는 로그를 취해줍니다. 로그를 취하면 y 값(Close)의 움직임 폭을 좁혀주는데, 이는 급격한 추세로 인해 가파르게 상승하거나 하락하는 구간을 희석시키는 역할을 합니다.로그-주가 데이터를 구하기 위해 행렬 및 수치 연산에 활용되는 numpy 라이브러리를 np라는 이름으로 불러와 log() 함수를 사용해 주었습니다. observed를 보면 기존 주가 데이터와 움직임은 유사하나 최근의 급격한 가격 상승으로 보이지 않던 과거의 움직임도 잘 표현되고 있는 것을 확인할 수 있습니다.잔차도 전체 기간에 걸쳐 일정한 범위 내에서 무작위로 진동하는 것을 볼 수 있기 때문에 로그를 취한 주가 데이터에서 드러난 추세는 유의미합니다.(정확히는 유의미성을 확인할 근거를 확보했다고 보는 것이 타당합니다. 추세에 대한 정규성 검정과 관측치의 자기상관성 여부를 확인한 다음에야 시계열의 정상성을 확보할 수 있고 그때 추세분석이 유의미한가를 판단할 수 있기 때문입니다. 다만 여기서는 프로그래밍을 배우는 것이기 때문에 더 디테일한 분석까지 진행하지는 않겠습니다.)4-4. 장기 로그차분-주가 데이터로그를 취했음에도 잔차에 패턴이 남아있거나 유의미한 추세를 판단하기 어려운 경우 차분(미분)을 한 번 더 수행합니다. 차분은 ‘변화’를 의미합니다. 즉, 일별 주가 데이터에 미분을 하면 일일 가격 변화량이 됩니다. 여기서는 그전에 로그 처리가 되어 있기 때문에 로그를 취한 주가 데이터의 일일 변화량이 되겠네요.​차분의 효과는 일간 변화량을 보여주는 것 이상으로 중요한 의미가 있습니다. 바로 과거의 움직임을 현재의 움직임과 같은 비중으로 확인할 수 있다는 것입니다. 현재 주가가 과거 주가보다 훨씬 많이 올라와 있는 상태라면 과거의 1%보다 현재의 1%가 더 큰 영향을 미칩니다. 앞서 로그를 취한 것도 어느 정도 이러한 영향을 상쇄하기 위해서지만 로그를 취한 관측치(observed)를 보면 여전히 상승 추세가 있기 때문에 현재의 1%는 과거의 1%보다 더 사이즈가 큽니다. 따라서 여기에 대해 미분을 통한 ‘변화량’을 y 축에 두게 되면 과거와 현재의 움직임을 ‘변화’라는 동일한 기준으로 파악할 수 있게 됩니다. 즉, 과거나 현재나 1%는 동일하게 1%입니다.하지만, 로그를 취했을 때 충분히 random 한 잔차를 확보할 수 있었고 우리가 파악하길 원했던 추세 정보를 얻었다면 로그 차분을 해줄 필요는 없습니다. 목적과 다르게 추세조차 파악하기 어려워졌기 때문입니다. ​여기서 우연히 우리는 ‘일일 변화량’(차분 값)이 ‘무작위성’을 보인다는 것을 발견했습니다. 로그만 취했던 데이터를 보면 장기 추세는 우상향하는 방향으로 나아갔습니다. 그러나 로그 차분 데이터는 랜덤에 가깝습니다. 이는 가격의 움직임은 추세가 있지만 가격 변화의 움직임은 추세가 없다는 것을 의미하고, 추세는 장기적으로만 유의미하다고 해석할 수 있습니다.​투자의 관점에서 보면 우리가 주가라는 정보만 놓고 투자 의사결정을 할 때, 장기적인 추세에 따라 주가가 오를 것이라고 판단할 수는 있겠지만 내일 주가가 오늘보다 오를 것이냐 떨어질 것이냐는 알 수 없다는 것입니다.​이는 자기상관분석을 통해서 검증할 수 있습니다.​Step 5. 자기상관분석자기상관분석은 단일 데이터(주가)가 스스로에게(과거로부터) 영향을 받아 움직이는가?를 판단하기 위한 분석 기법입니다.​일반적으로 시계열 분석을 할 때, 위에서 잠깐 언급한 ‘정상성’을 확보한 다음 회귀분석 등의 예측을 수행할 수 있는데 ‘정상성’의 근거가 되는 요소 중 하나가 ‘자기상관성이 없다’입니다. 그래서 주가만 놓고 회귀분석을 실시하면 마치 미래를 예측하는 것 같지만 이는 사실 정상성을 확보하지 못한 분석으로 유의미한 결과가 아닙니다. 그래서 위와 같이 로그 차분 등의 전처리를 수행하여 정상성을 확보한 다음 분석으로 넘어가는 것이 바람직합니다.​5-1. 기본 종가 데이터파이썬으로 자기상관성을 확인할 때는 시계열 분해 때 봤던 statsmodels를 사용하면 됩니다.함수는 2개를 사용했습니다. 하나는 plot_acf(), 다른 하나는 plot_pacf()입니다.plot_acf()는 auto-correlation을 계산합니다. auto-correlation은 단순 자기상관성으로, 오늘과 어제, 어제와 그제와 같이 1개 단위(1일 간격)의 상관성을 보여줍니다.plot_pacf()는 partial auto-correlation을 계산합니다. partial auto-correlation은 편 자기상관성으로, 오늘과 어제, 오늘과 그제와 같이 오늘을 기준으로 1~n 개 단위(하루~n 일)의 상관성을 보여줍니다. 함수를 사용할 때 lags 파라미터를 지정해 주었는데, 오늘로부터 과거 100일까지의 상관관계를 보여달라고 한 것입니다.둘 다 1에 가까울수록 상관성이 높고 0에 가까울수록 상관성이 낮다는 의미입니다. 0보다 아래에 있고, -1에 가까워지면 음의 상관관계, 즉 완전히 반대로 움직이는 성향이 있다는 것을 의미합니다. 보통 자기상관성에서는 그런 경향 자체가 모순이기 때문에 잘 나타나지 않습니다.auto-correlation 그래프를 보면 지난 100일까지의 상관관계가 하루 단위로 매우 높게 나타나는 반면, partial auto-correlation 그래프에서는 가장 앞의 2개(오늘과 오늘, 오늘과 어제) 상관성만 높게 나타납니다. 이는 주가가 하루하루간의 연관성은 매우 크지만 이틀 이상 차이가 나면 연관성이 사라진다는 것을 뜻합니다.​다시 말하면, 오늘의 가격은 어제 가격으로부터는 영향을 받지만 그 전날 가격과는 무관합니다.5-2. 로그 데이터로그를 취한 종가 데이터의 자기상관성은 더 두드러집니다. 앞에 설명한 것처럼 급격한 변화가 로그 함수로 인해 상쇄되므로 추세가 더 선명하게 나타나는 것입니다.5-3. 로그 차분 데이터그러나 차분을 해주면 그 즉시 자기상관성이 완전히 사라집니다. 어제의 가격 변화로 내일의 가격 변화를 맞출 수 없습니다. 서로 관련이 없기 때문입니다.이렇게 이번에는 데이터 시각화를 배워보면서 간단하게 시계열 분해와 자기상관분석까지 진행해 보았습니다. 데이터 분석을 배우기 어려운 가장 큰 이유는 분석 대상이 없기 때문입니다. 호기심을 가지고 분석 대상을 찾고, 어떤 분석을 할지 스스로 생각한 다음, 어떻게든 방법을 찾아가며 분석을 해보는 것이 좋습니다. 그것이 가장 빠르게, 그리고 효율적으로 파이썬이라는 도구를 배우는 방법입니다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "금융 분석을 위한 파이썬 프로그래밍 - 01. 라이브러리, 데이터프레임, 인덱싱", "url": "/posts/%EA%B8%88%EC%9C%B5-%EB%B6%84%EC%84%9D%EC%9D%84-%EC%9C%84%ED%95%9C-%ED%8C%8C%EC%9D%B4%EC%8D%AC-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-01-%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC-%EB%8D%B0%EC%9D%B4%ED%84%B0%ED%94%84%EB%A0%88%EC%9E%84-%EC%9D%B8%EB%8D%B1%EC%8B%B1/", "categories": "3. 튜토리얼, 금융 분석 프로그래밍 기초", "tags": "", "date": "2023-01-30 00:00:00 +0900", "snippet": "Step 1. 라이브러리우리가 개발을 할 때, 모든 코드를 한 땀 한 땀 장인의 정신으로 구현해야 한다면 실력과 무관하게 금세 지쳐버릴 것입니다.우리가 작성하려고 하는 코드가 매우 정형적이고 일반적으로 자주 쓰이는 것들이라면 누군가는 반드시 함수나 클래스 형태로 이미 구현해두었을 것입니다. 그리고 우리는 이것을 라이브러리라는 형태로 가져와서 쓸 수 있습니다. 심지어 무료로 말입니다.​야후 파이낸스에 있는 주가 정보도 라이브러리라는 것을 사용하면 손쉽게 가져올 수 있습니다. 누군가 이미 그러한 작업을 한 적이 있고, 개발한 코드를 모두가 사용할 수 있게 공개해둔 것이지요. 참고로 아래 이미지, 깃허브라는 코드 저장소에서 실제 구현된 코드를 눈으로 볼 수도 있습니다. yfinance라는 이름의 라이브러리입니다.​ranaroussi/yfinance이 라이브러리를 사용하고 싶다면 아래와 같이 코드를 작성합니다.​그러면 yfinance 라이브러리를 yf라는 변수명으로 사용할 수 있게 된 겁니다. yfinance는 내부적으로 Tickers라는 클래스를 품고 있습니다. 클래스는 주로 붕어빵 틀로 비유합니다. 붕어빵 틀에서는 붕어빵이 나오죠. 붕어빵을 인스턴스라고 합니다. 비슷하게 우리는 클래스로부터 필요한 인스턴스들을 만들어내고 사용하게 됩니다.​클래스로부터 인스턴스를 만들어낼 때는 경우에 따라 재료를 넣어줍니다. Tickers의 경우 어떤 종목들의 정보를 가져올 것인지, 아래와 같이 괄호 안에 종목 이름들을 재료로 넣어주면 되겠습니다.우리는 테슬라(TSLA), S&amp;amp;P 500(SPY), NASDAQ100(QQQ)의 주가 정보를 가져오기 위해 3개 티커를 넣어주었습니다. 대괄호로 3개 티커 이름을 감싼 다음에 넣어주었는데, 참고로 이러한 형태의 자료형을 ‘리스트’라고 부릅니다.이제 우리는 3개 주가 정보를 다룰 수 있는 stocks라는 인스턴스를 생성한 것입니다. 만약 삼성전자나 카카오 주가 정보를 얻고 싶다면 해당하는 티커를 클래스에 넣어서 새로운 인스턴스를 생성해야 할 것입니다.인스턴스는 클래스 내부에 정의되어 있는 함수들을 사용할 수 있습니다. 여기서는 history()라는 함수를 사용해 봅니다. history()는 특정 기간의 주가 데이터를 뽑아주는 함수입니다.history() 함수를 통해 불러온 데이터는 df라는 변수에 담았고, df를 출력해 보면 위와 같이 저장된 데이터 프레임을 보여줍니다. 여기서 데이터 프레임이란 pandas라는 파이썬 데이터 분석 라이브러리에서 제공하는 2차원 자료형의 이름입니다.​왼쪽 날짜(Date) 부분을 데이터 프레임의 인덱스라고 부르고, 위에 Close, Dividends, High, Low 부분을 컬럼이라고 부릅니다. 일반적으로 컬럼과 인덱스는 하나입니다. 그러나 위와 같이 2개가 될 수도 있으며, 이 경우에는 2중 컬럼 형태를 띠는 것입니다. Close라는 칼럼 아래에 QQQ, SPY, TSLA 이렇게 3개의 컬럼이 하나 더 있는 형태입니다.​Step 2. 인덱싱인덱싱이란 앞에서 봤던 1차원 리스트나 2차원 데이터 프레임 같은 데이터 덩어리로부터 특정 데이터만 추출하는 행위를 말합니다. 인덱싱 방법은 매우 다양하며 일반적으로 쓰이는 방법들을 소개해 드리겠습니다.2-1. 열 인덱싱데이터 프레임은 행과 열로 이루어진 2차원 자료형입니다. 만약 특정 열만 추출하고 싶다면 데이터 프레임 뒤에 대괄호로 컬럼 이름을 써주면 됩니다.앞에서 언급했지만 df 변수에 담은 데이터 프레임은 이중 컬럼을 가지고 있습니다. Close 컬럼을 인덱싱하게되면 이처럼 Close 컬럼에 포함되어 있는 하위 컬럼(열)들을 추출하게 됩니다.여기서 다시 한번 열 인덱싱을 수행하면 다음과 같이 SPY 컬럼에 해당하는 데이터만 뽑아낼 수 있습니다.​꼭 하나의 열만 추출할 수 있는 것은 아닙니다. 2개 이상의 컬럼은 대괄호로 감싸서 아래와 같이 추출할 수도 있습니다.​그런데 우리가 종가(Close)만 계속해서 볼 텐데 이렇게 이중으로 인덱싱을 하는 것은 매우 귀찮죠? 이때는 Close 컬럼을 인덱싱한 데이터 프레임을 복사해서 다른 변수에 담아주면 됩니다.​여기서는 copy() 함수를 사용했습니다. 해당 함수를 사용하지 않고 그대로 변수에 넣어줄 수도 있습니다. 그러나 그런 방식은 잘 못 사용하면 이따끔 상당히 난처한 상황이 생길 수도 있습니다. 대입한 변수를 수정했을 때 대입한 대상인 원본 변수가 수정될 수 있기 때문입니다. 데이터 분석을 하다 보면 뭔가 처리를 잘못해서 원본 데이터를 다시 가져다가 써야 할 때가 있는데, 원본이 수정되어버리면 처음부터 데이터를 다시 가져와야 합니다.​한번 코드를 보면서 무슨 말인지 눈으로 확인해 보겠습니다.잠깐 참고로, 주석(#)에 적힌 객체 참조라는 개념에 대해서는 나중에 한 번은 찾아보시길 권장 드립니다. 프로그래밍이 처음이라면 메모리와 데이터 저장에 대한 개념이 와닿지 않을 수 있지만 알아두면 이러한 현상에 대해 더 이상 마법이라 느껴지지 않을 것입니다.​df_close라는 변수는 기존 데이터 프레임에서 Close 컬럼만 인덱싱한 데이터 프레임을 담고 있습니다. 따라서 df_ref는 해당 데이터 프레임을 그대로 가리키고 있는 꼴이라 생각하시면 되겠습니다. 반면 df_copy는 df_close라는 변수를 복제해서 새로운 데이터 프레임을 만든 것입니다.​그래서 df_close와 df_ref는 같은 데이터 프레임을 다른 변수명으로 담고 있으며, df_copy는 같은 모양(같은 데이터 값)이지만 실제로는 모양만 같고 다른 데이터 프레임(다른 메모리에 저장되어 있는 데이터)을 담고 있게 됩니다.​그래서 아래와 같이 df_close와 같은 데이터 프레임을 가리키고 있는 df_ref를 수정하면 df_close도 동일하게 수정됩니다.​반면 다른 메모리 공간에 저장된 다른 데이터 프레임을 가리키고 있는 df_copy의 경우 변하지 않았습니다.​df_copy를 수정하더라도 마찬가지입니다. drop()을 통해 TSLA 컬럼을 제거한 다음 원본 df_close를 확인하면 그대로 유지되고 있는 것을 볼 수 있습니다. drop() 함수에 대해서는 주석을 참고해 주세요.​2-2. 행 인덱싱열을 인덱싱한 것처럼 행도 인덱싱할 수 있습니다.데이터 프레임은 기본적으로 인덱스 순서를 0번부터 인식합니다. 위와 같이 작성하면 0번부터 4번 인덱스(5번째 행)까지 추출하는 것입니다. ​앞에서 5개 행을 뽑았다면 마지막 5개 행도 뽑을 수 있습니다.​이렇게 자주 쓰이고, 정형화된 코드의 경우 반드시 함수가 있다고 말씀드렸습니다. 아래와 같이 head(), tail()을 통해 간단하게 인덱싱해볼 수도 있겠습니다.​2-3. 행, 열 동시 인덱싱위에서 확인한 행 인덱싱, 열 인덱싱을 함께 사용할 수도 있습니다.​그리고 이 케이스 역시 자체 함수를 제공합니다. 함수는 대표적으로 2개가 있습니다.​먼저 인덱스 이름, 컬럼 이름으로 인덱싱하는 loc() 함수입니다.​다음은 n 번째 행, n 번째 열처럼 숫자로 인덱싱하는 iloc() 함수입니다.​지금까지 라이브러리, 데이터 프레임, 그리고 인덱싱까지 간단하게 살펴보았습니다.처음 프로그래밍을 접하고 파이썬 문법을 익힐 때, 알고리즘이나 자료구조에 대해 어려운 개념과 문법부터 시작하시는 경우가 많습니다. 그러나 여러분의 목표가 데이터 분석이라면 이렇게 데이터를 직접 만져보고 분석해 보면서 필요한 문법들을 채워가시는 편을 더 권장 드립니다.파이썬 고급 문법과 심지어 컴퓨터 공학 이론까지 다 배우고 데이터 분석으로 넘어오기에는 사실상 무한에 가까울 만큼 양이 너무 많고, 실제로 실무에 계신 숙련된 개발자분들도 평생 배워야 한다고 말합니다. 또 배우는 과정에서 웹 개발을 배운다거나 하는 식의 주객전도되는 경우도 더러 있습니다. 물론 연계성이 강한 개발 특성상 두루 알면 도움은 되겠지만 꼭 거쳐야 하는 과정이라고 보기는 어렵습니다. 차라리 분석이 목표라면 분석에 집중하고 더 빠르고 효율적인 분석을 위해 파이썬이라는 도구를 어떻게 활용할 수 있을까를 고민하며 채워나가시면 됩니다.​분석 난이도와 단계에 따라 그에 맞는 적절한 문법과 효율적인 코딩 방식들이 있습니다. 서버에 대해 알아야 할 때가 오고, 데이터베이스와 자료구조를 디테일하게 이해해야 할 때가 옵니다. 미리 모든 것들은 준비하기보다 그때그때의 분석 단계마다 필요한 것들을 배워나간다면 결국은 분석 역량은 물론 이를 뒷받침하는 탄탄한 개발 스킬까지 갖출 수 있게 될 것입니다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "Google Firestore로 경제 매크로 데이터 올리기", "url": "/posts/Google-Firestore%EB%A1%9C-%EA%B2%BD%EC%A0%9C-%EB%A7%A4%ED%81%AC%EB%A1%9C-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%98%AC%EB%A6%AC%EA%B8%B0/", "categories": "1. 기술, 서버, 데이터, 클라우드", "tags": "", "date": "2023-01-13 00:00:00 +0900", "snippet": "Step 0. 파이어스토어 서비스 계정 생성파이어스토어는 구글에서 제공하는 클라우드 기반 NoSQL 데이터베이스다. 따라서 구글 클라우드 계정이 있어야 하며, 만약 처음이라면 GCP에 접속 후 IAM 관리자에서 서비스 계정을 생성해 준다.계정을 생성하면 자동으로 인증키 파일(.json)이 다운로드 된다. 해당 파일을 가지고 아래와 같이 인증 후 데이터베이스 객체를 생성해 주자.​Step 1. 데이터 전송하기먼저, 파이어스토어에 전송할 데이터를 확인한다.이제 데이터를 전송할텐데, 파이어스토어에 데이터가 저장되는 방식은 다음과 같다. 데이터베이스 최상단(루트)에 collection을 생성할 수 있고, 각 collection에는 문서를 집어넣는다. 문서 안에는 필드-필드값 쌍으로 이루어진 데이터를 포함한다.즉, “컬렉션 &amp;gt; 문서 &amp;gt; 데이터(필드-필드값)”의 형태로 데이터베이스를 구성하게 되며, 이 형식에 맞춰서 데이터를 전송하면 되겠다.키-값 쌍으로 데이터를 보내줄 것이다. 아래와 같이 실행해 주면 각 컬럼을 순회하며 날짜별 종가 데이터가 들어간다.참고로, db.collection(collection_name).document(document_name)에서 collection_name과 document_name이 DB에 없는 경우 자동으로 생성된다. 해당 코드의 경우 collection_name은 ‘price’, document_name은 각 컬럼명이므로, 아래와 같이 파이어스토어 DB가 생성되면 정상적으로 데이터가 넘어간 것이다.​Step 2. 데이터 불러오기파이어스토어에 저장된 데이터를 다시 불러오는 것도 간단한다. 어떤 collection을 가져올지 정의해 주고, stream() 함수를 통해 해당 collection 내 documents를 순차적으로 가져온다.위 이미지의 하단 cell에서 볼 수 있듯이 stream()을 통해 반환되는 객체는 generator이다. 파이썬에서 모든 generator는 iterator이므로, 해당 제너레이터를 반복 호출하면서 호출되는 복수의 데이터 컨테이너로부터 데이터를 가져올 수 있다. 여기서, 반복 접근하는 복수의 컨테이너는 ‘document’가 되겠다.​각 document는 필드(키)와 필드값 쌍으로 구성되어 있기 때문에 컨테이너(document)를 파이썬 dictionary로 변환해 주면 id 값에 해당하는 데이터를 아래와 같이 불러올 수도 있다.참고로 데이터를 불러올 때 키값에 따라 정렬을 해줬는데, stream()은 처음 저장된 순서로 데이터를 가져오지 않기 때문에 그렇다. 만약 정렬하지 않고 데이터를 불러오게 되면 아래와 같이 날짜가 섞이게 되고 각 document마다 그 순서는 다르다. 따라서 이를 그대로 병합해버리면 문제가 생긴다.그러나 정렬한 데이터로 병합해 주면 아래와 같이 우리가 처음 파이어스토어에 보냈던 소스 데이터(df)와 동일한 DataFrame을 생성할 수 있다.Step 3. 데이터 시각화일회성 분석이라면 굳이 데이터 프레임을 생성하지 않고, 필요한 특정 날짜의 데이터만 메모리에 올린 다음 차트를 그릴 수도 있다.​이외에도 파이어 스토어는 다양한 함수를 제공한다. stream()을 사용하기 전에 order_by 함수로 미리 정렬한 다음 불러올 수도 있고, where 함수로 필요한 데이터만 쿼리 해서 불러올 수도 있다. 단, 이때는 용도가 다르기 때문에 분석 흐름에 맞게 필드-필드 값도 다르게 설정해 줘야 한다.예를 들어 현재는 필드-필드값이 날짜와 해당 날짜의 가격 쌍(ex. ID : SPY, ‘2013-11-06’:151.259232)으로 이루어져 있는데, 이를 date : [날짜 목록], price : [가격 목록] 과같이 바꿔주면 되겠다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "파이썬 주식 시장 동향 분석 - 자연어처리 감성분석", "url": "/posts/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EC%A3%BC%EC%8B%9D-%EC%8B%9C%EC%9E%A5-%EB%8F%99%ED%96%A5-%EB%B6%84%EC%84%9D-%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-%EA%B0%90%EC%84%B1%EB%B6%84%EC%84%9D/", "categories": "2. 도메인, 금융", "tags": "", "date": "2023-01-12 00:00:00 +0900", "snippet": "주식 시장에는 기본적으로 주식을 팔고자 하는 사람과 사고자 하는 사람, 이렇게 크게 두 유형이 있다. 팽팽한 줄다리기 끝에 팔고자 하는 사람이 더 많으면 공급 초과로 가격은 하락하게 된다.​주가에 영향을 주는 요소는 금융 애널리스트의 예측, 기관 예측, 경제 위기, 그리고 이를 전달하는 뉴스 매체 등으로 셀 수 없이 많지만 결국 그 종착지에는 대중의 움직임이 있다. 그러한 대중의 움직임이 파는 쪽에 더 치우치면 가격은 떨어지고, 사는 쪽에 치우치면 가격은 상승한다.그렇다면 우리는 지금 이 시간, 대중의 움직임이 어느 방향으로 기울고 있는가를 분석해 볼 수 있다. 그 대상은 특정 종목이나 특정 국가가 될 수도 있고 주식 시장 자체가 될 수 있다. 본 분석은 후자, 주식 시장 자체에 대해 사람들이 어떻게 평가하고 있는가를 분석해 볼 것이며 분석 대상이 되는 기간은 1주일이다. 즉, ‘이번 주 주식 시장에 대한 사람들의 평가는 어떠한가?’에 대한 분석이 되겠다.(2022-10-12 기준)Step 1. 블로그 정보 수집1-1. 웹사이트 구조 및 데이터 호출 정보 확인파이썬으로 포스팅을 긁어올 것이므로 타깃을 명확히 확인하자. ‘네이버 블로그’(https://section.blog.naver.com/) 사이트에 진입해서 ‘주가 전망’이라는 키워드를 검색 후 개발자 모드를 열어 네트워크 탭을 살펴본다.미리 보기를 통해 반환된 문서들을 살펴보면 그중 SearchList.naver에 우리가 찾는 데이터가 있는 것을 확인할 수 있다. 이제 파이썬으로 해당 자료를 가져올 수 있으면 된다.먼저 해당 데이터가 어떤 방식(GET, POST)으로, 어떤 url 값으로 반환받을 수 있는지 살펴본 다음, 하단 쿼리 문자열 매개변수를 확인함으로써 넘겨줄 요청 값들을 지정해 줄 수 있다.1-2. 단일 페이지 데이터 호출currentPage는 현재 페이지 번호, countPerPage는 페이지당 포함하는 포스팅 수, endDate와 startDate는 지난 7일간을 지정해 주고 keyword는 검색한 값을 보여준다. 이렇게 파라미터 파악이 끝났으면 코드 작성을 위해 필요한 라이브러리를 호출하겠다.endDate, startDate는 직접 문자열로 입력해 줘도 되지만 스크래퍼를 개발할 때에는 항상 자동화를 염두에 두어야 한다. 따라서 기준 날짜로부터 지난 7일을 계산할 수 있도록 다음과 같이 함수를 사용하겠다.이제, url, header, date, params를 정의하고 파라미터가 잘 세팅되는지 확인한다.requests 모듈을 통해 데이터를 불러온다. 결괏값을 확인했을 때 우리가 앞서 봤던 SearchList.naver의 미리 보기 값과 형태가 동일해야 한다. 텍스트 앞부분만 짧게 출력하자.​데이터를 json 형태로 변환해 주면 좋겠다. 그전에 앞부분의 불필요한 문자열을 잘라줘야 정상적으로 변환이 될 것이다.​1-3. 다중 페이지 데이터 호출(함수)이제, 앞서 구현한 코드들을 종합하여 키워드와 페이지 번호만 넣어주면 지난 7일간의 포스팅 정보를 긁어올 수 있는 함수를 정의하겠다.총 10개 페이지, 70개 포스팅 정보가 수집되었다.Step 2. 블로그 내용 수집2-1. HTML 태그 정보 확인이렇게 수집한 포스팅 정보로 각 url을 순회하며 텍스트만 뽑아올 것다. 수집한 url 중 하나에 진입해서 html 태그를 살펴보자.본문 텍스트는 se-main-continer라는 div class가 지정되어 있다.2-2. 데이터 호출 정보 확인데이터 호출을 위해 이번에는 네트워크 탭의 ‘문서’를 살펴보면 PostView.naver에 우리가 찾는 본문 텍스트가 있다. 해당 데이터를 어떻게 호출할 수 있을지 url과 매개변수를 통해 살펴본다.2-3. 단일 페이지 데이터 호출(함수)이번에는 함수로 바로 만들어주겠다. select_one 함수로 본문을 가져오고 select(‘p’)를 해줌으로써 본문 내 모든 문장들을 리스트로 추출한다. 또한, list comprehension 문법을 통해 각 리스트 내 요소에서 텍스트만 추출해 다시 리스트로 저장하고, 제어문자(u200b는 폭 없는 공백이다. 네이버 블로그 포스팅 시 자동으로 문자 사이에 포함됩니다.)는 없애준다.계속해서 list comprehension 문법을 사용하며 줄바꿈은 분리해 주고 공백 요소는 제거해 깔끔한 형태의 문자열만 담은 리스트로 만든다.Step 3. 감성 분류이제, 이렇게 얻은 문자열에 대해 감성 분류를 실시할 수 있다. 텍스트 클렌징 작업은 최대한 정교하게 할수록 좋다. 텍스트 수집에 정도가 없듯 텍스트 전처리 방법에도 역시 정도가 없다.이번 튜토리얼에서는 간단하게 전처리한 문장을 활용해 감성 분류를 해보자.3-1. 허깅페이스 모델 검색우선, 임의의 텍스트 한 줄을 가져다 잘 분류해 줄 수 있는 모델을 찾아보자.언어 모델의 경우 대규모로 사전학습된 모델을 먼저 리서치해 보는 것이 좋다. 간단한 분석을 위해 몇 주간의 학습 시간을 소진하기에는 현실적으로 어렵기 때문에, 언어 모델 사용 시 다운스트림 테스크로 우리의 데이터 셋에 조금 더 확률 값을 높여주고 우리가 목표로 하는 결괏값을 출력해 내도록 구조를 맞춰주는 정도의 파인튜닝이 일반적이다.감성 분류, 그중에서도 금융 텍스트에 특화된 한국어 모델이 이미 허깅페이스에 올라와 있다. 해당 모델을 테스트해 보고 사용 여부를 판단한다.부정적 어감의 문장을 가져다 넣으니 negative로 잘 분류해 준다. 중립은 어떨까?‘엔비디아 전망’이라는 텍스트는 97.6%의 확률로 neutral으로 분류를 해준다. 만약 우리가 사용하는 모델이 긍/부정으로만 분류할 수 있다면 중립에 해당하는 문장은 최대한 걸러내야 할 것이다.여기서는 우리가 수집한 전체 문장에 대해 중립, 부정, 긍정 각각의 비중을 살펴보고 주식시장의 동향을 파악해 보고자 한다.​우리는 특정 블로그 포스팅 자체에 대해 ‘긍정 포스팅이다.’, ‘부정 포스팅이다.’ 하고 분류하기보다 위 방식처럼 하나의 글 내에서도 여러 문장으로 분리하고 각각을 분류 대상으로 삼을 것이다. 한 명이 작성한 하나의 블로그 포스팅 안에서도 긍정적인 문장과 부정적인 문장이 혼재해있다. 해당 블로그 포스팅으로 누군가는 부정적인 정보를 획득해갈 것이고 누군가는 반대로 긍정적인 정보를 획득해간다. 사람마다 받아들이는 정보는 매우 주관적이므로 블로그 포스팅 내 모든 문장을 정보 혹은 대중의 판단 근거라 가정하는 것이다.물론 이러한 분석 방식 역시 주관과 의도가 강하게 개입되어 있으며, 유일한 정답일 수 없다.3-2. 추론 모델 인스턴스 생성 및 테스트위 코드는 허깅페이스를 통해 우리가 사용하려는 모델과 토크나이저를 불러와 빠르게 추론 모델을 만든다.해당 모델은 뉴스 기사 텍스트로 1차 파인튜닝된 모델이다. 그렇기 때문에 추가 학습 없이 우리 테스크에 바로 적용할 수 있다.다만, 경우에 따라 같은 유형의 테스크라 하더라도 모델이 우리 데이터에 완전히 맞지는 않을 수 있다. 따라서 몇 가지 단어를 넣어 적절한 출력이 나오는지 한 번 더 확인하겠다.주가 관련 텍스트를 학습한 탓에 상승이라는 키워드에는 positive를, 하락이라는 키워드에는 negative를, 애플과 같은 고유명사에는 neutral을 출력해 준다. 우리가 기대했던 결과값이다.그럼, 이제 지난 7일간의 주식 시장에 대한 평가를 네이버 블로그에 한정해서 살펴보도록 하겠다.​3-3. 단일 포스팅 내 텍스트 감성 분류우리가 분석하려는 총 포스트 수는 총 70개다. 첫 번째 포스팅을 먼저 분석해 보겠다.3-4. 전체 7일간 포스팅 감성 분류서론, 혹은 이야기를 풀어나가기 위한 담화용 텍스트는 neutral로 분류되고 그 외 주가 판단에 대한 내용은 positive 혹은 negative로 분류되는 모습을 볼 수 있다.이번에는 전체 70개 포스팅에 대해 모두 분류하고 분류 비중을 살펴보도록 하겠다.포스팅을 먼저 모두 수집하고 전체 데이터에 대해 한 번에 추론하는 방식 혹은 매 포스팅마다 수집과 추론을 반복해서 결괏값을 합치는 방식을 취할 수 있다.속도 및 작업 편의를 고려하면 전자가 좋겠지만 각 포스팅에 수십 개의 문장이 있기 때문에 전체 분류 타깃이 메모리에 한 번에 올라가게 되면 중간에 멈출 수도 있다. 따라서 배치 작업으로 안정적인 추론이 이루어질 수 있도록 후자 형태로 코드를 구성하겠다.3-5. 결과결과는 중립 73%, 긍정 18%, 부정 8%로 현재 불안정한 시장 상황을 충분히 반영하고 있다.사실, 이는 해석하기 나름이다. ‘여전히 긍정적인 전망을 하는 사람들이 더 많기 때문에 아직 바닥이 아니다’라고 할 수도 있고, ‘이제 바닥을 찍고 상승하려는 움직임이 보인다’라고 할 수도 있다.따라서 이렇게 하나의 지표나 결과를 보고 전체 주식시장을 판단하는 것은 무리가 있겠지만 이러한 비율을 매주 추적하며 시계열로 분석한다면 더 유의미한 추세적 지표로 활용할 수 있을 것이다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "주식 가격과 뉴스 기사의 상관 관계에 대하여 - NVIDIA 뉴스 감성 분석", "url": "/posts/%EC%A3%BC%EC%8B%9D-%EA%B0%80%EA%B2%A9%EA%B3%BC-%EB%89%B4%EC%8A%A4-%EA%B8%B0%EC%82%AC%EC%9D%98-%EC%83%81%EA%B4%80-%EA%B4%80%EA%B3%84%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC-NVIDIA-%EB%89%B4%EC%8A%A4-%EA%B0%90%EC%84%B1-%EB%B6%84%EC%84%9D/", "categories": "3. 튜토리얼, 자연어 처리 및 텍스트 분석 방법론", "tags": "", "date": "2023-01-10 00:00:00 +0900", "snippet": "Step 1. 데이터 추출1-1. 뉴스 기사 수집먼저 investing.com에서 엔비디아 관련 기사를 수집한다. python requests 패키지를 사용해 post 방식으로 데이터를 가져올 것이다.​엔비디아 관련 뉴스 기사를 검색했을 때, 네트워크 XHR 탭에서 SearchInnerPage가 뉴스 데이터를 반환하는 것을 확인할 수 있다.따라서 SearchInnerPage의 헤더 정보를 확인해 요청 데이터를 넣는 payload와 header를 만들어주고, requests.post() 함수를 통해 데이터를 받는다.실습에서는 최근 300개의 뉴스 기사를 불러와 데이터 프레임으로 만들어줬다.1-2. 주가 정보 수집주가 데이터는 간단하게 yahoofinance 데이터를 가져오는 yfinance 패키지를 사용해 전체 기간 데이터를 불러온다. 여기서 Close(종가)만 사용할 것이다.​Step 2. 감성 분석2-1. 감성 분석 모델 불러오기우리는 사전학습된 감성 분석 모델을 사용할 것이다. 따라서 다양한 transformer 모델을 공유하는 플랫폼 Hugging face에서 적절한 모델을 찾아보자.금융 관련 텍스트의 감성을 분류해주는 모델이 있다. 해당 모델을 사용하도록 한다.Hugging Face를 통해 배포된 모델을 가져오는 가장 단순한 방법은 transformers 패키지의 pipeline 함수를 사용하는 것이다. 아래와 같이 코드를 작성해주면 감성 분류 모델이 각 뉴스 데이터에 대해 평가(분류) 한다.senti_main은 뉴스 기사 헤더에 대한 분류 정보를 담고 있고, senti_sub는 뉴스 기사 텍스트 일부에 대한 분류 정보를 담고 있다.감성 분류 모델은 positive, negative, neutral 3가지 감성으로 분류한다. 또한 각각의 평가(분류)에 대한 확신도(정확도)를 의미하는 score도 위와 같이 가져올 수 있다.모델을 통해 분류한 감성 정보를 기존 데이터와 합쳐주자.여기서 date 컬럼은 시간 정보를 담고 있으니 시계열 분석을 위해 인덱스로 저장해 준다.title(뉴스 기사 제목)과 content(뉴스 기사 본문 일부)는 같은 기사에 대한 정보지만 텍스트 내용이 다르기 때문에 감성 모델이 다르게 평가할 수도 있다. 그러나 뉴스 기사 제목이 내용을 대표하므로 어조나 분위기가 유사할 것이라 짐작할 수 있고, 따라서 분류 결과의 차이가 너무 크면 모델을 신뢰하기 어렵다. title과 content에 대한 감성 분석 결과를 시각화해보자.다행히 금융 텍스트가 잘 학습된 모델이라 우리가 가진 데이터도 비슷한 비중으로 분류해내고 있다.특히 title에 대한 분류가 중립 감성(neutral) 비중이 높은 것을 보아 title 만으로 감성을 분류할 수 없는 케이스가 있고, content까지 확인했을 때 더 정확히 분류된다는 것을 확인할 수 있다. 따라서 우리는 title과 content 분류 결과를 모두 활용해 새로운 감성 score 값을 만들 것이다.2-2. 감성 계량화positive는 3점, neutral은 2점, negative는 1점으로 하고 확신도(score)를 곱해줄 것이다. 이를 title과 content에 각각 적용하여 두 데이터를 산술평균하는 방식으로 감성을 계량화하겠다.먼저 3가지 분류 유형을 수치형 데이터로 변환해주고 위에서 확인한 3가지 분류 빈도와 동일한지 확인한다.이제 앞서 정의한 방식으로 score를 계량화한다.계량화한 score는 산점도를 통해 분포를 확인해 볼 수 있다.​Step 3. 주가 데이터 스케일링주가 데이터는 2가지 시계열상 특징이 있는데, 하나는 최근으로 올수록 스케일이 커진다는 점, 다른 하나는 자기상관성을 띤다는 점이다. 일반적으로 생존 편향이 있는 주가는 우상향하고, 엔비디아의 경우에도 그렇다.최근(2022-6 기준)의 가격이 과거 대비 매우 크게 상승해 있기 때문에 과거의 1%보다 최근의 1%가 그 등락 폭이 훨씬 크다. 따라서 이러한 과거와 현재의 스케일 차이를 줄여주기 위해 log 변환을 해줄 필요가 있다.또한, 오늘의 가격정보는 전날 가격 정보와 가장 가깝고 상관계수가 매우 높다. 시계열 데이터 분석은 정상성 확보를 위해 각 시점 간 데이터가 가능한 서로 독립이 되도록 해야 한다. 따라서 자기상관성을 제거하기 위해 log 변환된 데이터에 차분까지 실행해 줄 것이다.먼저 주가 데이터는 전체 기간에 대해 가져왔기 때문에 뉴스 기사 데이터가 있는 시점만 추출한다.다음으로 기본 주가 데이터, log 변환된 데이터, log 변환 후 차분한 데이터를 확인해 본다.​Step 4. 상관 분석우리는 주가에 대한 로그 차분 데이터와 계량화된 감성 데이터의 상관성을 분석할 것이다.분석을 위해 두 데이터를 한 번 더 스케일링해 주자. 각 데이터를 정규화(min-max-scaling) 하면 0에서 1사이로 데이터 구간을 맞춰줄 수 있다. 여기서 0.5를 빼주면 음/양 표현이 가능하다. 즉 데이터가 -0.5에서 0.5 사이에 들어오도록 해줌으로써 각 시점에 대해 스케일링된 상승, 하락 포지션을 확인할 수 있다.show_target_bins() 함수는 특정 기간(분기, 월, 주, 일) 별로 스케일링된 주가와 스케일링된 감성 score의 시계열 분포를 보여주고, show_target_corr() 함수는 그것의 상관 분포를 보여준다.작성한 함수를 사용해 아래와 같이 각 기간별 시계열 분포, 상관 분포를 확인할 수 있다.​우측 상관 분포 표 타이틀에는 뉴스 기사에 대한 감성 스코어(scaled)와 각 기간(분기, 월, 주, 일) 별 주가(scaled)의 피어슨 상관 검정 결괏값을 표기했다. r은 상관계수, p는 p-value를 의미한다.결론적으로 뉴스 기사는 유일하게 일별 주가와 유의한 수준으로 상관성이 있었으며 상관계수는 0.25로 높지 않은 수준이다. 만약 뉴스 기사가 나온 정확한 시점에 대해 분 혹은 시간별 주가 상관성을 구한다면 일별 주가 상관계수보다 더 높을 것으로 예상할 수 있다.또한, 현재 수집된 뉴스 기사는 엔비디아를 검색했을 때 나오는 기사들이지만 엔비디아 외 다른 기업들에 대한 내용도 다수 포함되어 있다. 그럼에도 불구하고 ‘늘어진 시간 정보’로도 0.25만큼의 상관계수를 확인할 수 있었다. 따라서 엔비디아만 다루는 기사를 필터링하여 감성 스코어에 대한 분, 초 단위 인과성 검정을 시도한다면 선행지표로서의 유의성을 확인해 볼 수 있겠다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "파이썬 이더리움 가격 예측 - 회귀모형", "url": "/posts/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EC%9D%B4%EB%8D%94%EB%A6%AC%EC%9B%80-%EA%B0%80%EA%B2%A9-%EC%98%88%EC%B8%A1-%ED%9A%8C%EA%B7%80%EB%AA%A8%ED%98%95/", "categories": "2. 도메인, 금융", "tags": "", "date": "2023-01-09 00:00:00 +0900", "snippet": "많은 딥러닝 입문 도서, 유튜브 콘텐츠 등에서 주가 예측을 소재로 삼고 있다. 자극적이고, 흥미로울 뿐 아니라 예측 결과는 경이로울 정도로 정확해 보여서 주식에 관심 있는 많은 사람들의 관심을 받게 된다.다음과 같이 ‘AI가 예측한 주가’라고 하는 시뮬레이션 결과를 보신 적이 있지 않은가?위 결과는 실제로 모델을 학습시켜 예측한 결과물이며, 심지어 복잡한 딥러닝이 아닌 기본적인 단순 통계모델인 회귀 모형을 사용했다.딥러닝을 잠깐 접해본 분들은 이 그래프를 보고 ‘에이, over-fitting 됐네’라거나, ‘미래 데이터를 학습에 사용했네’라고 지적할 수 있다. 하지만 해당 모델은 over-fitting이 생길 만큼 복잡한 모델을 쓰거나 loss를 과하게 줄이지 않았으며(단순 회귀모형 사용), 위에 보이는 기간은 학습용으로 사용하지 않은, 모델이 처음 보는 데이터를 사용했다.그럼, 어떻게 저런 결과가 나올 수 있었을까? 지금부터 회귀모형으로 이더리움 가격을 예측하는 모형을 만들어보고, 해석상 유의해야 할 점을 알아보자.Step 1. Intro​시계열 데이터에는 ‘추세’라는 것이 있다. 매매 기술을 관심 있게 찾아본 적이 있다면, ‘추세 추종’같은 기법을 들어본 적이 있을 것이다. 시계열 데이터는 ‘예측 가능한 영역’과 ‘(주어진 데이터로는) 예측 불가능한 영역’으로 나뉜다. 추세나 계절성 같은 것들이 바로 ‘예측 가능한 영역’에 속하고, 그런 것들을 모두 제거하고 남은 것이 ‘예측 불가능한 영역’이 된다. 예측 불가능하다는 것은 이론적으로 그것을 맞힐 확률이 50%에 수렴한다는 것을 의미한다.주가 데이터는 매우 강한 추세를 가지고 있다. 특히 분 단위, 초 단위로 갈수록 주가의 움직임은 무작위성을 띠는 반면 장기적으로 갈수록 그러한 추세는 점점 강해진다. 단순 회귀분석이든 복잡한 딥러닝 신경망이든 결국은 데이터 간 관계를 찾아서 그다음 데이터를 예측하는 원리다. 추세는 주가 데이터의 움직임에 가장 강한 연관성을 담고 있는 핵심 Factor이기 때문에 어떤 모델이든 이 추세를 처리하지 않고 학습시키면 가장 먼저, 그리고 가장 강하게 추세를 학습하게 된다.주가에 추세가 있다는 것은 어제와 오늘, 오늘과 내일 가격에 서로 강한 관련성이 있음을 의미하고 이를 표현하는 용어가 바로 자기 상관성의 개념이다. 따라서 ‘주가를 잘 예측’하는 모델은 ‘추세를 잘 학습한’ 모델이 되고, 예측 성능이 매우 뛰어난 모델은 ‘오늘의 가격’을 ‘어제의 가격’으로 예측하게 된다.​Step 2. 모델 학습​그럼, 지금부터 위에서 ‘추세를 잘 추종하는’ 회귀 모형을 만들어보자.2-1. 데이터 불러오기데이터는 보유하고 있는 이더리움 가격 정보를 가져왔다. 실습을 원하시는 경우 어떤 데이터를 가져와도 상관없으니 네이버나 야후파이낸스 등 다른 사이트를 통해 직접 분석하고 싶은 데이터를 먼저 확보해 보시면 좋겠다.인덱스가 역순으로 정렬되어 있으니 과거부터 현재순으로 재정렬해 주자.​2-2. 학습 및 평가 데이터 생성먼저, window 데이터 셋을 생성해 준다. 단일 데이터로 예측 분석을 수행한다는 것은 과거 며칠의 데이터를 보고 다음날의 데이터를 예측할 것인가? 의 문제를 푸는 것이다. 여기서 ‘과거 며칠’에 해당하는 것이 window size다. 이 window size에 따라서 1일부터 30일까지의 데이터를 보고 31일째 가격을 예측하고, 2일부터 32일까지의 데이터를 보고 32일째 가격을 예측하는 식이다. 이제 이 예측값과 실제 값을 비교해 보면서 모델을 업데이트하는 것이 모델 학습 원리가 된다. 따라서 모델이 학습할 수 있도록 데이터를 window 구간별로 잘라서 맞춰줘야 하는데, ‘1일부터 30일까지, 2일부터 31일까지, … , ‘ 이렇게 겹쳐가며 데이터 셋을 만들어주면 되겠다.하나의 윈도우가 X(과거 30일 치) 라면 예측하고자 하는 다음날 하루는 y다. 따라서 하나의 학습 시퀀스 사이즈는 예측하고자 하는 하루와 과거 30일(윈도우 사이즈)을 합친 31일이 되고, 이러한 조합으로 데이터 셋을 구성하는 코드는 다음과 같다.이제 이렇게 구성한 데이터 셋을 학습 데이터, 검증 데이터, 평가 데이터 셋으로 나눠준다. 학습 데이터로는 모델을 적합시키고, 검증 데이터는 학습 과정에서 모델의 예측도를 평가하며 모델을 잘 적합시킬 수 있도록 기준이 되어주고, 평가 데이터 셋으로 모델 성능을 실제 평가하게 된다. 여기서는 학습 데이터 비중을 80%, 검증 및 평가 데이터 비중을 10%씩 확보했다. (여기서는 성능을 높이지 않고 단순히 학습 데이터로 적합하는 것만으로도 겉보기에 매우 성능이 높아 보일 수 있다는 것을 보여드리기 위해 검증 과정을 생략한다.)​2-3. 모델 적합회귀 모형은 sklearn의 LinearRegression() 함수로 사용할 수 있다. 간단하게 학습 데이터만으로 모델을 적합시킨다.이제 test 데이터 셋에 대해 평가를 진행하고, 오차 수준을 확인한다. 오차는 mean squared 방식을 사용한다.이제 예측 결과를 실제 가격 데이터와 놓고 비교해 본다.어떤가? 매우 잘 예측한 것처럼 보이지 않은가?그러나 예측 결과 일부를 자세히 보면 마치 예측 가격(파란색)이 실제 가격(초록색)을 복사하는 것처럼 전날의 움직임을 따라가고 있다. 지금은 예측 성능이 높지 않은 아주 간단한 모델이라 이 정도지만, 만약 딥러닝으로 다양한 시점의 다양한 주가를 반복학습을 시켜 데이터에 ‘적합’하는 과정(오차를 줄이고 예측 성능을 높이는 과정)을 길게 가져갈수록 이 경향은 점점 더 뚜렷하게 나타날 것이다.​Step 3. 모델 평가두 그래프를 함께 그려놓고 보면 그렇게 보일 수밖에 없다. 오늘의 주가가 떨어졌지만 올랐다고 예측했더라도 전체적으로 놓고 보면 두 차이가 크지 않기 때문에 계속해서 두 선이 겹치며 그려지게 되기 때문이다.지금부터, 모델의 예측 값이 하루 단위로는 완전히 잘못되더라도 장기적으로는 실제 값을 따라 움직이게 되는 모습을 하루 간격의 예측치로 그려가며 확인해 보겠다.위 그림은 지난 30일 치 가격으로 31일째 되는 날 가격을 예측하는 순간을 포착한 그래프다. 붉은 점선은 과거 30일 치 중 30일째(관측 대상인 x=1 기준으로 어제)를 기준선으로 표시해 주고 있다.우리는 x=1을 예측하기 전까지는 실제 값만 가지고 있었지만, x=1부터는 실제 값과 예측값을 가지고 있다. 그리고 실제 값과 예측값은 다소 차이가 있다. 그럼, x=2부터는 어떤 데이터로 예측하게 될까? 시중에 유통되는 정말 많은 도서와 튜토리얼에서는 실제 값을 사용한다. 이것을 두고 ‘미래 데이터를 입력값으로 사용한 오류’라고 지적하는 경우도 가끔 볼 수 있는데, 엄밀히 말하면 그렇지는 않다. x=2 시점에서는 x=1이 과거가 되기 때문이다.미래 데이터를 입력값으로 사용하는 흔한 사례는 백테스팅에서 볼 수 있다. 예측하고자 하는 시점에 아직 발표되지 않은 공시 데이터나 해당 시점이 속한 분기의 per을 사용하는 케이스다. 매우 뛰어난 예측 성능을 보여주지만 현실에서 사용할 수 없다.이와 달리 현재의 예측 방식은 “내가 내일 주가를 예측하고 싶은데, 어제 주가를 예측했지만 조금 빗나갔어. 어제의 실제 주가를 다시 사용해서 내일 주가를 예측해 봐야겠어”와 같이 접근한다. 따라서 관측 시점을 기준으로는 언제나 과거 데이터를 사용하며 이 로직에 오류가 있다고 보기는 어렵다. 다만, 이 결과를 잘못 해석하고 잘못 받아들일 여지는 있다.전날의 실제 값을 가지고 반복해서 예측하면 어떤 그래프가 만들어지는지 이어서 확인해 보자. 붉은 점선 뒤로는 지난 30일 치 데이터가 있고, 해당 데이터를 사용해 다음날 주가, 그다음 날의 주가를 예측해 나간다.각 시점(x=2, x=2)에서 실제 값과 예측값은 꽤 큰 차이가 있다. 그렇게 반복하다 보면 우연히 일치하는 구간(x=3)도 나타날 수 있지만 항상 그렇지는 않을 것이다.​이렇게 계속 ‘전날의 실제 값’을 가지고 예측을 해나가다 보면 장기적으로는 같은 방향으로 그래프가 그려질 수밖에 없다. 각 시점(x)에서는 방향이 엇갈리더라도 그다음 날을 예측할 때에는 실제 값으로 조정된 과거 30일 치를 사용하기 때문에 멀리서 보면 “귀신같이 주가를 맞히는 것처럼” 보인다.예를 들어보자. 과거 5일 치 테슬라 종가가 100만 원, 200만 원, 300만 원, 400만 원, 500만 원이었다면 6일째 되는 날의 가격은 대부분의 모델(가격만 학습한 경우)이 600만 원이라고 예측할 것이다. 하지만 어떤 외부 요인에 의해 주가가 떨어져서 450만 원이 되었다고 하면 내일의 테슬라 가격은 오늘 예측한 600만 원을 포함해 지난 5일 치 가격 데이터인 200만 원, 300만 원, 400만 원, 500만 원, 600만 원을 가지고 예측하는 것이 아니라 오늘의 실제 가격인 450만 원을 포함해 200만 원, 300만 원, 400만 원, 500만 원, 450만 원을 가지고 예측한다.모델의 목적이 오늘 가격을 맞히는 것이라면 과거의 실제 값들을 가지고 예측하는 이러한 방식이 것이 잘못된 설계라 보기는 어렵다. 하지만 모델이 마치 지난 30일 치를 보고 다음 30일 치를 연이어 맞히는 것처럼 보여준다면 해석에 오류가 있다고 할 수 있다. 그래프 두 개를 놓고 “귀신같이 맞힌다”라고 결론을 내게 되는 것이 그러한 예시가 된다.이것이 학습자 입장에서는 과정과 결과를 그대로 받아들이고, 실제 투자까지 이어질 수 있기 때문에 꽤나 위험한 일일 수 있다고 생각한다. 따라서 지난 며칠을 가지고 다음 며칠을 예측하는 모델인지 확인하고, 하루치 예측이라면 타깃 지점인 하루에 대한 결과만 놓고 해석하는 것이 바람직하겠다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "파이썬 비트코인 가격 예측 - 1차, 2차, 3차 지수 평활", "url": "/posts/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EB%B9%84%ED%8A%B8%EC%BD%94%EC%9D%B8-%EA%B0%80%EA%B2%A9-%EC%98%88%EC%B8%A1-1%EC%B0%A8-2%EC%B0%A8-3%EC%B0%A8-%EC%A7%80%EC%88%98-%ED%8F%89%ED%99%9C/", "categories": "1. 기술, 통계, 시계열", "tags": "", "date": "2023-01-08 00:00:00 +0900", "snippet": " 본 내용은 작년 1월에 교육용으로 준비했던 자료입니다. 데이터를 통해 어떻게 미래 동향을 파악할 수 있는지 확인하고, 현재의 비트코인 가격과 비교해 보시면 좋겠습니다. 예측 결과는 실제 결과와 매우 상이할 것입니다. 과거 데이터만으로 미래를 예측하는 것의 한계를 시사합니다.​Step 1. 데이터 전처리데이터는 2016년 12월 8일부터 2022년 1월 8일까지의 비트코인 일별 종가를 사용한다. 본 실습에서는 미리 확보해둔 데이터를 사용했지만, 꼭 동일한 데이터로 진행할 필요는 없다. 웹에 공개된 데이터가 많으니 비트코인이든 주가 데이터든 직접 스크래핑이나 다운로드 방식으로 데이터 확보를 해보시면 좋겠다.여기서 종가 데이터만 가져오고 날짜의 경우 datetime 타입으로 변환한 다음 인덱스로 지정해 준다. datetime을 인덱스로 두게 되면 데이터 프레임이 기본적으로 시계열 데이터로 간주되어 인덱싱을 더 편하게 할 수 있다.​Step 2. 이동평균과 단순 지수 평활​2-1. 일별 비트코인 가격먼저, 가볍게 단순 종가(‘Close’)부터 그려보자.2-2. 일별 비트코인 가격의 이동 평균이동평균은 시계열 형태의 데이터 프레임에서 제공하는 rolling() 함수를 사용해 구할 수 있다. rolling() 함수를 사용하면 아래와 같이 mean() 뿐만 아니라 std(), min(), max() 등 다양한 통계 함수로 원하는 집계를 수행할 수 있다.​2-3. 일별 비트코인 가격의 단순 지수 평활이동평균선은 기본적으로 예측을 위한 지표는 아니다. 단순히 과거의 데이터를 기간별로 평균 낸 수치로, 가격의 흐름에 따라서 움직이는 후행지표다. 그래서 가격 정보가 아직 없는 미래에 대해 이동평균선을 그려보면 추세를 따라 올라갈 것 같지만 가격이 존재했던 과거 기점을 마지막으로 값은 고정된다. 추세에 대한 정보 역시 포함하지 않기 때문이다.​위 그래프는 이동평균은 아니지만 120일 이동평균, 300일 이동평균선에 지수의 움직임을 더 잘 반영할 수 있도록 가중 평균한 값을 그리고 있다. 이것을 단순 지수 평활 혹은 1차 지수 평활이라고 하는데, 단순하게 최근에 가까울수록 지수를 더 잘 반영하는 조정된 이동평균선이라 생각하시면 되겠다. 단순 지수 평활은 statsmodels 라이브러리의 SimpleExpSmoothing 함수를 사용하면 위와 같이 간단히 그릴 수 있다. 함수를 통해 인스턴스(ses 변수에 담았습니다)를 생성하고 모델을 fitting 하는 과정을 보여주고 있다.이렇게 구현한 단순 지수 평활 역시 ‘이동평균’ 방식을 취한다는 점에서 보다시피 미래에 대한 예측은 어렵다. 기술적으로 과거에 대한 정보만 담기 때문다. 그래서 이동평균과 1차 지수 평활은 미래에 대한 예측보다는 살까 팔까에 대한 심리적 저항선 정도로 사용하는 것이 일반적이다.그래서 사실 이 이동평균선을 단기적으로 뚫고 지나가거나 한참 위를 웃돈다 하더라도 결국 장기적으로는 전혀 이상한 일이 아니나 많은 투자자들이 이동평균선을 매매 지표로 활용하는 만큼 장기 이동평균선의 바닥을 찍었을 때 매수세가 강하게 들어와서 주가가 다시 튀어 오르는 경우가 많기 때문에 단순 심리적 저항선 이상의 의미를 지니기도 한다.​2-4. 이동평균과 단순 지수 평활의 비교이동평균과 지수 평활 그래프를 잠시 비교해 보면, 아래 그래프에서 120일 지수 평활(120_ses, *ses : simple exponential smoothing), 300일 지수 평활(300_ses) 선은 각각 120일 이동평균(120_mv), 300일 이동평균(300_mv) 선보다 실제 지수를 더 잘 반영하고 있음을 알 수 있다.2021년 그래프만 확대해 보면 다음과 같다.초록색 선(120일 이동평균, 120_mv)은 보라색 선(120일 지수 평활_120_ ses)보다 하락장을 잘 반영하지 못하는 모습을 보인다. 붉은색 선(300일 이동평균, 300_mv)과 노란색 선(300일 지수 평활, 300_ses)을 봐도 그렇다. 이는 누적된 과거 데이터가 단기간 지수의 상승과 하락을 기민하게 따라갈 수 없도록 하기 때문이다. 그래서 최근 데이터에 가중치를 주는 지수 평활법이 이동평균법보다 현재 가격의 모습을 더 잘 반영하게 된다.하지만 앞서 설명했듯 1차 지수 평활법으로도 여전히 미래를 설명할 수는 없다. 시계열 데이터에는 가격과 같은 단순 1차원 정보뿐만 아니라 단기 혹은 장기적 추세, 추세 주위를 오르내리는 사이클, 그리고 추세와는 별도로 특정 주기로 움직이는 계절성 등이 담겨있다. 따라서 2차 지수 평활, 3차 지수 평활법을 통해 추세나 계절성을 반영해서 앞으로의 주가 움직임을 예측하려는 시도가 있었다.​Step 3. 추세와 계절성을 추가한 2차, 3차 지수 평활3-1. 2차 지수 평활​1차 지수 평활에 추세를 더한 것이 2차 지수 평활, 2차 지수 평활에 계절성까지 더한 것이 3차 지수 평활이다. 먼저 2차 지수 평활부터 보면, 2022년 이후 데이터가 없음에도 불구하고 1차 지수 평활과 달리 약간의 상승하는 움직임을 보여주고 있다. 과거 데이터가 그간 상승 추세에 있음을 고려하여 앞으로의 움직임을 ‘상승’으로 표현하고 있는 것이다.​3-2. 3차 지수 평활​여기에 계절성까지 더한 3차 지수 평활은 어떨까?계절성의 경우 추세와 별도로 등락의 움직임이 특정 주기로 나타나는 것을 의미하는데, 주가나 암호화폐 가격의 경우 계절성이 선명하게 나타나지 않는다. 오션월드 매출 그래프를 상상해 보면 여름에 폭발적으로 증가하고 겨울에는 바닥을 치고 있을 것이나 비트코인은 그렇지 않다. 1년 단위로 보면 항상 오르는 방향으로 가지만 분기에 따라서 내림세를 겪기도 하기 때문에 추세와 계절성이 복합적으로 얽혀있다.물론 이외에도 우리가 가늠할 수 없는 수많은 외생변수들도 포함되어 있을 것이다. 따라서 여기서는 1분기(1q), 2분기(2q), 1년(1y) 주기의 계절성이 있다고 가정하고 그려보자.​계절성이 1년 주기로 나타난다고 가정했을 때(붉은색) 2022년 이후 급격한 상승세를 보인다. 반대로 계절성이 분기 혹은 반기 단위로 나타난다고 가정했을 때(파란색, 초록색)는 약간의 조정 혹은 하락세를 보다.해석하자면 붉은색 선의 경우 1년 단위로 상승하는 성향에 더해 최근 2021년에 급상승한 추세를 고려해서 2022년은 더 가파른 상승장이 나타난다고 예측했으며 파란색, 초록색 선의 경우 분기나 반기로는 하락하는 구간이 상대적으로 잦았기에 추세를 거스르는 하락장을 예측한 것이다.Step 4. 비트코인 가격 전망이렇게 지금까지 대중적으로 잘 알려진 이동평균부터, 간단하지만 시계열 예측 이론의 핵심을 담고 있는 2차, 3차 지수 평활법까지 살펴보았다. 이를 통해 비트코인 가격을 전망해 보면, 전체 기간으로 보면 상승해왔고 여전히 상승 추세에 있다고 볼 수 있으며 이 추세가 계속 간다고 하면 1년 내에 10억까지도 갈 수 있겠다. 그러나 추세를 꺾을만한 변동성이 나온다면 하락세에 접어들 수 있겠다.시계열 예측이 명확한 답을 내놓지 못하고 확률적으로 해석할 수밖에 없는 이유, 그리고 주가나 암호화폐 가격의 예측이 어려운 이유는 하루가 지나면 새로운 가격 정보와 새로운 외생 변수들이 생겨나기 때문이다. 이번 분석 역시 마찬가지로 2022년 다음 한 분기를 전망해 보았지만 시간이 흐르면서 새로운 정보가 추가될 것이고 가격의 움직임과 계절성, 추세 등의 변화에 따라 예측은 달라지게 되어있다.따라서 경제뉴스, 각종 시장 동향 리포트 등 시간을 기준으로 하는 모든 예측 분석은 “현재까지 데이터로 보면 이렇고, 앞으로는 달라질 여지가 충분하다”라고 이해하는 것이 바람직하겠다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "Elastic Stack을 활용한 글로벌 경제 매크로 대시보드 구축", "url": "/posts/Elastic-Stack%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-%EA%B8%80%EB%A1%9C%EB%B2%8C-%EA%B2%BD%EC%A0%9C-%EB%A7%A4%ED%81%AC%EB%A1%9C-%EB%8C%80%EC%8B%9C%EB%B3%B4%EB%93%9C-%EA%B5%AC%EC%B6%95/", "categories": "4. 실전, 글로벌 매크로 분석", "tags": "", "date": "2023-01-07 00:00:00 +0900", "snippet": "Step 0. 파이프라인Elastic Stack은 Elastic Search를 중심으로 Beats, Logstash, Kibana를 활용해 데이터 파이프라인을 구축하는 일련의 설계 방식이다. 파이프라인의 구성은 다음과 같다.구성 : Beats : 데이터 수집 및 전송 Logstash : 데이터 전처리(필터링) Elasticsearch : 데이터 저장 및 관리 Kibana : 데이터 시각화다음은 파이프라인의 동작 방식이다.동작 : Yahoo Finance에서 각 주요 국가의 시장지수와 금, 은, 유가 등의 각종 매크로 데이터를 추출한다. 추출한 데이터를 Beats로 읽어들여 Logstash로 보내 문자열 처리를 거친 다음, Elastic search로 적재하고, Kibana로 대시보드를 그린다.먼저, 현재 작업 환경은 아래와 같다. 무거운 실습이 아니므로 동일하게 세팅할 필요는 없으나 버전 등에 이슈가 있을 수는 있다.작업 환경 : Macbook Pro 2019 - i9, 16GB GCP 가상 머신 4대 할당(e2-small 3대 + e2-medium 1대) 가상 머신 1대에 클러스터 1개 배치(클러스터 당 노드도 1개씩) filebeat 설치(Local) logstash 설치(Local) elastic search 설치(e2-small 3개 각각 설치) kibana 설치(e2-medium 1대에 설치) filebeat, logstash, elastic search, kibana 모두 7.11 버전 통일​Step 1. Yahoo Finance 데이터 수집Yahoo Finance에서 종가 데이터를 수집할 것이다. 한 번의 요청으로 최대한 많은 데이터를 가져오기 위해 브라우저 네트워크 탭의 XHR 정보를 사용한다. 서버에 직접 데이터를 요청하는 것으로, url 파라미터를 수정해가며 최대 기간의 일 단위 가격 정보를 수집할 수 있다. 헤더와 url를 살펴보고, period 등 수정 가능한 구간들을 확인한다.requests 패키지를 통해 데이터를 불러올 것이다. 데이터는 json 형태로 반환되므로 적절한 가공이 필요하다. 아래와 같이 클래스를 구성하면 여러 티커를 순차적으로 요청하면서 하나의 데이터 테이블에 적재할 수 있다.활용할 수 있는 총 15개의 국제 매크로 지수들을 티커 목록에 넣고, 위에서 만든 클래스를 통해 한 번에 데이터를 불러온다.데이터 타입을 확인하고, csv 파일로 저장한다. 이 파일을 beats가 한 줄씩 읽으면서 logstash에 데이터를 전송하게 된다.​Step 2. 엘라스틱 서치, 키바나 실행현재 GCP 가상머신 4개를 사용했다. 가상머신 3개로 엘라스틱 서치 노드 3개를 구성하고 각 노드는 모두 서로 다른 클러스터를 형성한다. 첫 번째 엘라스틱 서치 서버를 메인 클러스터로 나머지 2개 클러스터가 연결되는 방식이다. 지금은 대용량 데이터에 대한 검색 실습이 아니기 때문에 엘라스틱 서치는 개인 로컬에 1대만 설치해서 띄워도 무관하며 동일한 환경을 구성할 필요는 없다.아래 이미지는 키바나를 실행한 모습, 메인 노드에 curl 명령어로 연결된 노드 정보를 검색하는 모습이다.키바나가 실행되면 포트 정보가 출력된다. 키바나 외부 접속 IP와 함께 포트 번호를 브라우저 url로 검색하면 키바나 어드민 페이지가 나온다.​Step 3. Beats 설정이제 수집한 데이터 파일을 beats가 인식할 수 있도록 yml 파일을 수정해 준다. Filebeat inputs 부분의 paths에 데이터 파일이 있는 경로를 작성해 주면 되겠다.다음으로 output을 정의해 준다. beats가 데이터를 받아서 logstash로 전달해야 하므로, logstash가 실행될 로컬 port 번호를 입력해 준다.​Step 4. Logstash 설정logstash도 beats와 마찬가지로 input과 output을 정의해 줘야 한다. input은 beats가 보낼 데이터가 들어올 포트 번호를 작성해 주고, output은 표준 출력(stdout)과 elasticsearch 외부 url 주소를 작성해 준다. 이렇게 하면 beats와 logstash, logstash와 elasticsearch가 하나의 파이프라인으로 연결되고, 데이터가 들어가는 과정을 표준 출력(stdout)으로 확인할 수 있다.input, output 중간에 filter라는 것이 있다. 우리는 csv 파일 정보를 전달하기 때문에 csv 파일이라는 것을 알려주고, 칼럼을 정의해 줘야 한다. 또한 모든 데이터가 문자열 형태로 전송되므로 각 칼럼별 데이터 타입을 실수형(float)으로 변경해 줄 필요가 있다. logstash는 이렇게 데이터 전처리 역할도 수행할 수 있기 때문에 미리 필터를 잘 설계해두면 데이터가 도착하는 지점인 elastic search에서의 수고를 덜 수 있다.​Step 5. 데이터 전송이제 beats를 실행해서 logstash를 거쳐, elastic search로 데이터를 보내보겠습니다. 데이터가 잘 넘어갈 수 있도록 elastic search와 logstash는 미리 실행되어 있어야 한다.elastic search는 ./bin/elasticsearch로 실행해 주고, logstash는 ./bin/logstash -f (conf 파일)로 실행한다.나의 경우, 맨 좌측 3개는 elastic search 원격 서버가 실행되고 있고, 중앙 상단에 kibana 원격 서버가 실행되고 있다. 그리고 맨 우측 하단에 위 명령어를 통해 logstash가 실행되고 있으며, 마지막으로 맨 우측 상단에 filebeat을 실행하는 모습이다. filebeat도 logstash처럼 실행할 때 설정 파일(yml)을 지정해 준다. ​참고로, beats는 데이터를 한번 읽어들일 때마다 로그가 기록되고, 그다음 데이터를 가리킨다. 따라서 전체 데이터를 다시 전달하고 싶다면 저처럼 rm -rf data/registry/* 명령을 실행해 주시면 되겠다.정상적으로 파이프라인이 동작하면, 다음과 같이 logstash 모니터링 화면에 표준 출력으로 데이터가 넘어가는 모습을 볼 수 있다.​Step 6. 키바나 인덱스 패턴 설정키바나에서 대시보드를 만들기 위해서는 인덱스 패턴을 설정해 줘야 한다. 이 패턴을 통해 대시보드가 각 칼럼과 데이터 타입을 인식한다. Stack Management 메뉴에 Index patterns 탭에 들어와서 진행할 수 있다.일반적으로 키바나 대시보드는 서버 로그를 모니터링하는 용도로 사용된다. 따라서 데이터 집계 기준이 되는 타임 테이블을 가장 먼저 정의해 준다.인덱스 패턴을 생성하면 아래와 같이 필드명과 데이터 타입을 확인할 수 있다. 숫자형 필드가 모두 number로 잘 들어왔는지 체크해 준다.참고로, Discover 메뉴로 들어가면 해당 인덱스 패턴에 대한 자세한 필드 정보를 시계열로 확인할 수 있다.​Step 7. 대시보드 구성메뉴에 대시보드에 들어가면 패널을 생성할 수 있다. 대시보드는 패널 조합으로 구성되며, 패널은 하나의 View(그래프, 집계) 단위라고 보시면 되겠다.패널 생성 화면으로 들어가면 이렇게 인덱스 패턴(finance*)을 지정하고, 해당 패턴에 속하는 필드들로 차트를 구성할 수 있다. 차트를 자유롭게 구성한 다음 Save를 눌러준다.​이 과정을 반복하며 패널을 생성해 주면 대시보드가 아래와 같이 구성된다.이렇게 다양한 글로벌 경제 매크로를 확인할 수 있는 대시보드를 간단히 구현해 보았다.엘라스틱 스택은 실시간 모니터링과 검색에 최적화된 파이프라인 조합이다. 그러므로 여기서 더 발전시켜 데이터 파일의 주기적(batch) 업데이트 혹은 실시간 데이터 전송 프로세스 구축을 통해 매일 변화하는 지표와 데이터를 실시간으로 보여주는 모니터링 서비스를 만들어보시길 추천드린다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "Feature Importance", "url": "/posts/Feature-Importance/", "categories": "1. 기술, 통계, 시계열", "tags": "", "date": "2022-07-13 00:00:00 +0900", "snippet": "Feature Importance and Permutation Feature Importancereference : Feature selection : feature importance vs permutation importance Permutation feature importance(sklearn.org) The Mathematics of Decision Trees, Random Forest and Feature Importance in Scikit-learn and Spark 머신러닝의 해석 2편 - 1. 트리 기반 모델의 Feature Importance 속성 랜덤 포레스트에서의 변수 중요도(Variable Importance) 3가지 Feature Importance(변수 중요도) - 트리 기반 모델 선형 회귀 모델에서 ‘선형’이 의미하는 것은 무엇인가? 머신러닝 - 4. 결정 트리(Decision Tree) Random ForestFeature ImportanceTree 기반 머신러닝 모델은 모델 생성 과정에서 각 Node별 Entropy가 발생(계산)한다. Entropy는 ‘불순도’를 표현하며, 각 Node가 연장되며 가지치기(의사결정)해나가는 과정에서 제대로 가지치기를 하고 있다면 ‘불순도’가 낮아지고 그 값(Entroypy)은 0에 가까워진다. Gradient Boosting 계열의 트리 모델은 이 Entropy 값을 낮추는 방향으로 노드를 이어나간다.만약 제대로 가지치기를 하지 못해 정확히 A 와 B로 나누지 못했다면, 즉 A로 분류한 공간에 B가 많이 남아있을 수록 그 값(Entropy)는 점점 올라간다.Tree 모델에서 자체 함수로 제공하는 feature importance는 이렇게 Entropy가 0으로 잘 수렴하고 있는지를 확인하는데, 정보이론 관점에서 보면 정보를 얼마나 잘 획득하고 있는지 확인하는 과정과 같다.다시 말하면 Entropy값이 상하 노드에서 얼마나 차이가 나는가? 즉 제대로 된 정보를 얼마나 취득해가고 있는가(정보이득)를 체크해서 feature별 유의미성을 판단한다.feature importance는 Tree 기반 모델 자체에서 함수로 제공하고 있으며 plot_importance API를 통해 별도로 호출할 수도 있다. 주로 gradient boosting model 계열에서 활용된다.1. 내장 함수 활용from xgboost import XGBClassifierxgbc = XGBClassifier(random_state=111)xgbc.fit(X_train, y_train)fi = pd.Series(xgbc.feature_importances_, index= X_train.columns).sort_values(ascending=False)sns.barplot(x=fi, y=fi.index)plt.show()2. API 활용from xgboost import XGBClassifierfrom xgboost import plot_importancexgbc = XGBClassifier(random_state=111)xgbc.fit(X_train, y_train)plot_importance(xgbc, max_num_features=10)plot_importanceAPI를 활용하면 별도로 series를 구성/정렬하는 작업 필요없이 바로 시각화가 가능하다.Permutation Feature Importancefeature importance가 ‘불순도’를 얼마나 낮추는 feature인지 확인하는 것을 통해 유의미한 feature를 찾는 방식이라면 Permutation feature importance는 학습을 완료한 모델에서 feature 하나씩 row를 shuffle하면서 모델 성능의 변화를 관찰하는 방식이다.즉, shuffle한 feature가 모델에 중요한 feature였다면 row를 무작위로 shuffle했을 때 성능이 크게 떨어질 것이고 중요한 feature가 아니었다면 shuffle하더라도 성능에 큰 변화가 없을 것이다.(우연히 성능이 올라가거나 떨어질 risk도 분명 존재한다.)특히 RandomForest 트리 모델에서 Feature Importance가 제대로 기능하지 못하기 때문에(Gradient Boosting 방식에는 유의미한 지표를 보여주는 편이다) Permutation 방식이 대안으로 유용하게 활용된다. Permutation Feature Importance는 eli5 패키지를 통해 사용하거나 sklearn 자체 패키지로(sklearn.inspection) 제공하는 permutation_importance로 사용할 수 있다. 둘의 사용 방법 및 return 값은 차이가 있다.1. from eli5.sklearn import PermutationImportanceimport eli5from eli5.sklearn import PermutationImportancerfc = RandomForestClassifier(n_estimators=100, random_state=111)rfc.fit(X_train, y_train)perm = PermutationImportance(rfc, random_state=111).fit(X_valid, y_valid)eli5.show_weights(perm, feature_names = X_valid.columns.tolist())2. from sklearn.inspection import permutation_importancefrom sklearn.inspection import permutation_importancerfc = RandomForestClassifier(n_estimators=100, random_state=112)rfc.fit(X_train, y_train)perm = permutation_importance(rfc, X_valid, y_valid, n_repeats=10, random_state=112)print(perm)# print(perm.importances_mean)# print(perm.importances_std)관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "sklearn - classification_report()", "url": "/posts/sklearn-classification-report/", "categories": "1. 기술, 머신러닝, 딥러닝", "tags": "", "date": "2022-07-11 00:00:00 +0900", "snippet": "sklearn.metrics.classification_report reference : scikit-learn.org, sklearn.metrics.classification_report, document scikit-learn.org,sklearn.metrics.f1_score, document stackoverflow, How to interpret classification report of scikit-learn?,LaSul Answered Accuracy(정확도), Recall(재현율), Precision(정밀도), 그리고 F1 Score, eunsukimme Document :sklearn.metrix.classification_report(y_true, y_pred, *, Labels=None, target_name=None, sample_weight=None, digits=2, output_dict=False, zero_division=&#39;warn&#39;) Examples :from sklearn.metrix import classification_reporty_true = [0, 1, 2, 2, 2] # multivariable classification that targets are &#39;0 or 1 or 2&#39;y_pred = [0, 0, 2, 2, 1] # predicted vector (compared with y_true)target_names = [&#39;class 0&#39;, &#39;class 1&#39;, &#39;class 2&#39;] # 0, 1, 2 중에 맞추는 분류 문제print(classification_report(y_true, y_pred, target_names=target_names)) precision recall f1-score support class 0 0.50 1.00 0.67 1 class 1 0.00 0.00 0.00 1 class 2 1.00 0.67 0.80 3 accuracy 0.60 5 macro avg 0.50 0.56 0.49 5weighted avg 0.70 0.60 0.61 51. support y_true(실제값)은 ‘0’, ‘1’, ‘2’로 구성되어 있고 ‘0’은 1개, ‘1’도 1개, ‘2’는 3개다. y_true=[0, 1, 2, 2, 2] support는 실제값의 분류별 분포(수)를 나타낸다.class 0 : 1class 1 : 1class 2 : 32. precision(정확도) y_pred가 예측한 값 중 y_true(실제값)과 일치하는 비율을 나타낸다.class 0 : 0.5 # y_pred는 &#39;0&#39;을 2개 예측했고, 2개 중 1개만 일치class 1 : 0.0 # y_pred는 &#39;1&#39;을 1개 예측했고, 일치하지 않음class 2 : 1.0 # y_pred는 &#39;2&#39;를 2개 예측했고, 2개 모두 일치3. recall(소환, 호출) y_true(실제값) 중 y_pred가 맞춘(불러낸, 호출한) 비율을 나타낸다.class 0 : 1.0 # y_true는 &#39;0&#39;을 1개 가지고 있었고, 해당 값은 y_pred에 의해 예측되었음class 1 : 0.0 # y_true는 &#39;1&#39;을 1개 가지고 있었고, 해당 값은 y_pred가 맞추지(불러내지) 못했음class 2 : 0.67 # y_true는 &#39;2&#39;를 3개 가지고 있었고, 해당 값 중 y_pred가 맞춘(불러낸) 값은 2개. 즉 2/3 recall4. f1-score precision, recall을 모두 고려한 값 2 * (precision * recall)/(precision + recall) 위 수식은 일반적인 평균 계산식이 아닌 조화 평균이다. 조화평균은 불균형이 심할 수록 낮은 값에 가깝게 나온다. 즉, precision과 recall값이 차이가 크다면, 값이 더 낮게 나오며 이를 통해 불균형도를 체크할 수 있다.5. accuracy 전체 값(y_true) 중 맞은 비율, 여기서 총 5개 변수 [0, 1, 2, 2, 2] 중 3개가 일치했으므로 3/5. 즉 정확도는 0.66. macro avg 전체 평균, class 0, class 1, class 2의 precision, recall, f1-score 각각 값에 대해 n(3)으로 나눈 값 ex. f1-score에서, (0.67 + 0.00 + 0.80)/3 = 0.497. weighted avg 변수 가중치가 포함된 값의 평균, 가중치는 support 수(실제값, y_true의 분포 수) 즉, 각 분류별 측정값에 가중치를 곱하고 전체 합산하여 전체 support 수로 평균함 ex. precision에서, (0.501 + 0.001 + 1.00*3)/5 = 0.70관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "pandas - corr()", "url": "/posts/pandas-corr/", "categories": "1. 기술, 통계, 시계열", "tags": "", "date": "2022-07-08 00:00:00 +0900", "snippet": "df.corr(method=’s’)reference document : pandas.DataFrame.corr 비선형 상관관계 : 스피어만 상관계수, 켄달타우pandas 데이터프레임 객체에 대해 corr()함수를 사용할 수 있다.corr()은 누락값을 제외하고 전체(dataframe) 컬럼들 간의 상관도를 계산한다.계산에 사용되는 상관계수의 default값은 pearson 상관계수로, 모든 변수가 연속형이고 정규분포를 띄는 경우 사용할 수 있다.만약 정규분포를 따르지 않는 변수가 포함되어 있다면 보편적으로 spearman 상관계수를 사용한다.spearman 상관계수는 비모수적 방법 (모수를 특정 분포로 가정하여 접근하는 방법론) 으로써 값에 순위를 매기고 순위에 대해 상관계수를 구하는 방식이다. 따라서 분석에 모수와 정규분포 가정이 필요하지 않다.비모수적 방법으로는 kendall Tau 상관계수도 있으며 spearman과 적용 대상은 대체로 유사하다.상관계수를 가장 대표적인 pearson으로 사용한다면 별도의 인자를 주지 않아도 되고,df.corr()pearson이 아닌 다른 상관계수를 사용하는 경우 파이썬 코드는 아래와 같이 작성할 수 있다.df.corr(method=&#39;spearman&#39;)관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "Kolmogorov Smirnov", "url": "/posts/Kolmogorov-Smirnov/", "categories": "1. 기술, 통계, 시계열", "tags": "", "date": "2022-07-07 00:00:00 +0900", "snippet": "Kolmogorov Smirnovreference document : scipy.stats.kstest Youtube : 파이썬을 활용한 통계분석 - (33)콜모고로프-스미르노프 검정(Kolmogorov-Smirnov Test)많은 통계 분석 기법의 경우 표본이 정규분포를 따른다라는 가정을 전제로 한다. 따라서 데이터를 다룰 때 각 컬럼이 정규분포를 띄지 않는 것을 확인했다면 반드시 정규화를 수행해야 한다.또한, 정규화를 수행한 뒤 정규성 검사를 통해 정규성 조건의 충족 여부를 통계적으로 검증해야 하는데, 이 때 보편적으로 사용하는 기법 중 하나가 Kolmogorov Smirnov Test이다.Kolmogorov Smirnov Test는 정규분포 뿐만 아니라 특정 분포를 따르는지 판단할 수 있는 기법이다.Kolmogorov Smirnov Test를 사용하기 위해서는 먼저 scipy의 stats 패키지를 불러와야 한다.import scipy.stats as stats분석 대상 데이터프레임 df에서 특정 컬럼 age의 정규성을 검사한다고 하면 아래와 같이 코드를 작성할 수 있다.stats.kstest(df[&#39;age&#39;], &#39;norm&#39;)(0.11069395395455, 0.0003342)단일 변수의 정규성 검사를 수행했다. 그 결과 첫번째 값은 통계 값, 두번째 값은 p-value을 반환했다.Kolmogorov Smirnov Test는 정규성을 검사하며 p-value가 0.5 미만인 경우 귀무가설을 기각, 즉 정규성이 없다고 판단한다. 결론 : 위 케이스에서 age 변수의 경우 정규분포를 따르지 않는다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "statsmodels - ols", "url": "/posts/statsmodels-ols/", "categories": "1. 기술, 통계, 시계열", "tags": "", "date": "2022-07-05 00:00:00 +0900", "snippet": "OLS - Regressionreference 선형회귀분석 및 OLS document : Ordinary Least Squares Python 기초 통계 - 회귀분석 실시하기회귀분석은 파이썬에서 제공하는 대표적인 통계 분석 패키지 statsmodels를 불러와 간단히 사용할 수 있다.import statsmodels.api as smmodel = sm.OLS(y, X) # y : 종속 변수(시리즈), X : 독립 변수 집합(데이터프레임)result = model.fit()result.summary()혹은, 함수 식 표현을 활용할 수도 있다.import statsmodels.api as smmodel = sm.OLS.from_formula(&#39;y ~ x1+x2+x3&#39;, data=df) result = model.fit()result.summary()두 코드의 결과 값은 동일하다. 회귀 계수와 각각의 p-value, 그리고 통계적 유의미성을 확인해주는 R^2까지 유용한 지표를 한번에 확인할 수 있다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "범주형/연속형 변수 조합에 따른 가설 검정 방법", "url": "/posts/%EB%B2%94%EC%A3%BC%ED%98%95-%EC%97%B0%EC%86%8D%ED%98%95-%EB%B3%80%EC%88%98-%EC%A1%B0%ED%95%A9%EC%97%90-%EB%94%B0%EB%A5%B8-%EA%B0%80%EC%84%A4-%EA%B2%80%EC%A0%95-%EB%B0%A9%EB%B2%95/", "categories": "1. 기술, 통계, 시계열", "tags": "", "date": "2022-07-04 00:00:00 +0900", "snippet": "1. 단일변수1-1. 연속형 정규성 검정 One Sample T-Test(일표본 평균검정)# One Sample T-Test : ## 귀무가설 H0 - 특정 값은 집단의 평균과 다르지 않다. (H0확률 = p-value, 0.05 미만이면 기각)## 연구가설 H1 - 특정 값은 집단의 평균과 다르다.stats.ttest_1samp(Series, value) # 집단의 평균(Series) vs 검증하고자 하는 값(value)1-2. 범주형 비율 검정(ex. 연구가설 H1-집단 내 남녀 비율은 차이가 있다)2. 다변수2-1. 범주형 ‘X’ &amp;amp; 연속형(정규분포) ‘Y’ Two Sample T-Test(이표본 평균검정), 등분산 검정(Levene Test) 등분산 검정 결과에 따라(이분산, 등분산) Two Sample T-Test를 다르게 수행# 등분산 검정## 귀무가설 H0 - 두 집단의 분산은 다르지 않다## 귀무가설 H0 - 두 집단의 분산은 서로 다르다stats.levene(Series1, Series2)stats.fligner(Series1, Series2)stats.bartlett(Series1, Series2)# 등분산 조건 충족시stats.ttest_ind(Series1, Series2, equal_var=True)# 등분산 조건 불충족시stats.ttest_ind(Series1, Series2, equal_var=False)2-2. 범주형 ‘X’ &amp;amp; 연속형(비-정규분포) ‘Y’ 비모수적 검정2-3. 범주형 ‘X’ &amp;amp; 범주형 ‘Y’ Chi Square 검정(두 범주형 집단의 독립성 검정)# 카이제곱 검정## 귀무가설 H0 - 두 집단은 서로 독립이다## 연구가설 H1 - 두 집단은 서로 독립이 아니다(의존적이다)crosstable = pd.crosstab(Series1, Series2) # 빈도 계산 - Contingency Tablestats.chi2_contingency(crosstable)#-&amp;gt; result : (stats, p-value, 자유도, 모수추정값)#-&amp;gt; p-value &amp;amp;lt; 0.05 : 귀무가설 기각 - 독립이 아니다. #-&amp;amp;gt; ex 자동차 구매 - 계약 유형(Series1)에 따라 제품군(Series2)의 차이가 있다2-4. 연속형 ‘X’ &amp;amp; 연속형 ‘Y’ 상관분석, 회귀분석2-5. 연속형 ‘X’ &amp;amp; 범주형 ‘Y’ 로지스틱 회귀분석(분류문제)관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "CycleGAN vs StarGAN", "url": "/posts/CycleGAN-vs-StarGAN/", "categories": "1. 기술, 머신러닝, 딥러닝", "tags": "", "date": "2022-06-29 00:00:00 +0900", "snippet": "background Conditional GAN - latent vector(random)와 함께 Condition(y)을 받을 수 있다(무작위가 아닌 목표로 하는 이미지를 생성) pix2pix - 이미지(X, Condition)를 받아서 이미지를 돌려준다. y(label, condition)를 따로 받는 것은 아니지만 X 자체를 Condition으로 입력 받아 목표로하는 이미지를 생성하기 때문에 Conditional GAN에 기반을 둔다고 볼 수 있다. CycleGAN , StarGAN은 모두 Conditional GAN 기반의 pix2pix 구조를 사용한다. CycleGAN은 이미지를 받아서 이미지를 돌려주고, StarGAN은 이미지와 y(도메인 Vector)를 추가로 받아서 y에 맞는 이미지를 돌려준다.1. Cycle GAN kaggle : 🌈CycleGAN Tutorial : Monet-to-Photo⛵️[모델 컨셉] 문제) 기존 pix2pix 모델들은 X-Y 가 쌍으로 묶여서 학습되어야 했다. 해결) 라벨링 되지 않은(or 서로 쌍으로 연결되어 있지 않은) unpaired 데이터셋을 통한 학습이 가능하다. How) ‘생성된 이미지를 다시 복원하면 원본이 나와야한다’[모델 특징] (Step 3) Resnet : 병목구간(feature 손실)을 없앰 -&amp;gt; 장단) 원본에서 크게 바뀌지 않는다. (Step 6) Generator 2개, Discriminator 2개 (편도가 아닌, 왕복가능한 모델) G(A-&amp;gt;B), G(B-&amp;gt;A) D(A), D(B) (Step 5) loss : 3개 (G, D) GAN-Loss : MSE (기존 GAN에서 주로 사용하는 CrossEntropy 사용하지 않았다.) (G) Cycle-Loss : L1 (F(G(A-&amp;gt;B)) == A, 변형의 변형은 원본) (G) Identity-Loss : L1 (Gen(A-&amp;gt;B) 모델에 B를 넣으면 당연히 B가 나와야한다. (Step 14) Training A(monet), B(Photo) 동시에 학습하므로 generator 두개를 생성하여 gan-loss, id-loss, cycle-loss를 통해 학습 G(A-&amp;gt;B) , G(B-&amp;gt;A)를 둘 다 받아야 하므로 discriminator 두개를 생성하여 gan-loss를 통해 학습 2. Star GAN kaggle : 🌈StarGAN Tutorial : Generate Celeb Images🧔🏻[모델 컨셉] 문제) 하나의 Gan 모델은 하나의 도메인만 표현가능 해결) 하나의 Gan 모델로 여러 도메인 표현 How) 이미지+도메인을 같이 넣고 도메인에 맞는 가짜 이미지를 생성한다.[모델 특징] (Step 3) Resnet : 병목구간(feature 손실)을 없앰 -&amp;gt; 장단) 원본에서 크게 바뀌지 않는다. (Step 6) Generator 1개, Discriminator 1개 (편도) Generator : 이미지에 라벨(One-Hot)을 (이미지사이즈로) 붙여서 gan 모델에 input으로 넣는다. Discriminator : 이미지의 Fake/Real 여부와 힘께 Real이라고 판단한다면, 라벨(도메인)을 함께 반환한다. (Step 5) loss: 3개 (G, D) GAN-Loss : mse가 아닌 WGAN-GP에서 사용한 Robust한 Loss 사용 (+gradient penalty) (G, D) label-Loss : Binary Cross Entropy (G) Cycle-Loss : L1 (Step 15) Training 무작위 레이블(5-size vector) + 이미지로 가짜이미지 생성 wgan처럼 n_critic을 사용해서(discriminator를 더 많이 학습해서) 판별력을 높여준다. Discriminator는 gan-loss(==wgan-gp loss)와 class-loss(label)를 통해 학습 Generator는 gan-loss, class-loss, cycle-loss로 학습 관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "StyleGAN v2-ada", "url": "/posts/StyleGAN-v2-ada/", "categories": "1. 기술, 머신러닝, 딥러닝", "tags": "", "date": "2022-06-27 00:00:00 +0900", "snippet": "StyleGAN v2-ada - Basic Conceptreference : StyleGAN2를 이용한 성격 유형별 얼굴 생성 모델 연구 개발 과정 공개! [StyleGan2-ada 실습] AFHQ 데이터 셋 이용해서 stylegan2-ada 학습하기 2 [Paper review]StyleGAN2 github/NVlabs/stylegan2-ada-pytorchstyleGAN2-ada는 기존 styleGAN의 일명 ‘물방울 현상’을 발생시키는 AdaIN(Adaptive Instance Normalization)을 정규화부분을 수정하고(StyleGAN2) ADA(Adaptive Discriminator Augmentation, 데이터 증강기법)을 추가로 활용하여 데이터가 부족한 상황에서도 좋은 성능을 낼 수 있도록 개선되었다.추가로, latent code(style)를 얻기 위해 별도의 네트워크를 구성할 필요 없이 자체적으로 구현된 projection method를 활용할 수 있도록 하였으며, pretrained model로 test해 볼 수 있다.python projector.py --outdir=out \\ --target=~/mytargetimg.png \\ --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "StarGAN v2", "url": "/posts/StarGAN-v2/", "categories": "1. 기술, 머신러닝, 딥러닝", "tags": "", "date": "2022-06-27 00:00:00 +0900", "snippet": "StarGAN v2 - Basic Conceptreference : github/clovaai/stargan-v2 github/yunjey/stargan StarGAN v2: Diverse Image Synthesis for Multiple Domains 도메인과 스타일, 모두 잡았다! StarGAN v2기존 StarGAN는 하나의 모델로 하나의 도메인에 대해 학습, Generate하는 것이 아닌, 하나의 모델로 다양한 도메인을 표현해낼 수 있었다면 StarGAN v2는 이에 더해 여러 도메인을 결합하여 동시에 표현해낼 수 있다.예를 들어, 기존의 StarGAN은 입력받은 하나의 이미지가 서로 다른 다양한 도메인을 표현하도록 가짜 이미지를 생성해낸다. 출처 : StarGAN - Official/README반면, StarGAN v2는 입력 받은 하나의 이미지가 몇가지 도메인을 동시에 표현하도록 가짜 이미지를 생성해낸다. 출처 : StarGAN v2 - Official/README이처럼 StarGAN v2에서는 입력 받은 이미지에서 표정(눈,코,입 위치)을 가져와 그러한 표정을 지닌채로 남성(하나의 도메인)이면서 수염(또 다른 도메인)이 있는 이미지를 만들어낼 수 있다.모델의 컨셉은 아래와 같다.(a) GeneratorThe generator translates an input image into an output image reflecting the domain-specific style code. G(Generator)는 이미지(X-metrics)와 스타일(s-vector)을 input으로 받고 목표로 하는 도메인이 반영된 이미지를 반환한다.(b) Mapping networkThe mapping network transforms a latent code into style codes for multiple domains, one of which is randomly selected during training. F(Mapping Network)는 랜덤으로 생성되는 latent vacter ‘z’를 input으로 받고 주어진 도메인을 모두 커버하는 style vector ‘s’를 반환한다.(c) Style encoderThe style encoder extracts the style code of an image, allowing the generator to perform reference-guided image synthesis. E(Style Encoder)는 이미지(X-metrics)를 input으로 받고 주어진 도메인에 해당하는 style vector ‘s’를 반환한다.(d) DiscriminatorThe discriminator distinguishes between real and fake images from multiple domains. D(Multiple output Discriminator / Multitask Discriminator)는 각 도메인을 담당하는 branch들로 구성되어 있으며 각 branch는 G(Generator)가 생성해낸 G(x,s) 이미지를 input으로 받고 이진분류(sigmoid)를 통해 1 혹은 0을 돌려준다. 1이 반환되었다면 G(x,s)가 진짜이미지처럼 잘 생성된 것을 의미한다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "GAN - Unpaired Image to Image Translation", "url": "/posts/GAN-Unpaired-Image-to-Image-Translation/", "categories": "1. 기술, 머신러닝, 딥러닝", "tags": "", "date": "2022-06-24 00:00:00 +0900", "snippet": "GAN : Unpaired Image to Image Translationreference: [Youtube [MODUCON 2019] 보다 유연한 이미지 변환을 하려면 - 이광희](https://www.youtube.com/watch?v=rHDa_fEd_0Q) [PPT 보다 유연한 이미지 변환을 하려면 - 이광희](http://moducon.kr/wp/wp-content/uploads/2020/02/%E1%84%8B%E1%85%B5%E1%84%80%E1%85%AA%E1%86%BC%E1%84%92%E1%85%B4_ModuCon.pdf) 1. Unimodal(one-to-one) Translation 하나의 도메인, 하나의 스타일에 대한 변환 Type A : Cycle Consistency Loss CycleGAN (ICCV, 2017) DiscoGAN (ICCV, 2017) Type B : Shared Latent space Assumption UNIT (NIPS,2017) 2. Multi-modal Tranlation 하나의 도메인 내에서 다양한 스타일을 표현Partially Shared Latent SpaceFeature Disentanglement MUNIT (ECCV, 2018) DRIT (ECCV, 2018)3. Multi-domain Tlanslation 다양한 도메인을 표현Domain LabelUnified Structure StarGAN (CVPR, 2018)4. Multi-mapping Translation (Multi-modal + Multi-domain) 다양한 도메인을 표현하고 동시에 각 도메인마다 다양한 스타일까지 함께 표현Unified Style Encoder DRIT++ (arXiv, 2019) DMIT (Neurips, 2019) SDIT (ICM, 2019) StarGAN v2 (arXiv, 2019)관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "Scheduling", "url": "/posts/Scheduling/", "categories": "1. 기술, 통계, 시계열", "tags": "", "date": "2022-06-23 00:00:00 +0900", "snippet": "Reference : 스케줄링 - 배치처리, 시분할시스템, 멀티테스킹, 멀티 프로그래밍 운영체제의 발전사 - 배치 처리 시스템, 시분할 시스템, 멀티 테스킹 운영체제 - 스케줄링 (배치 처리, 시분할 시스템, 멀티 프로그래밍)Scheduling서버의 실시간 시스템이 일관된 처리 속도를 갖추도록 하는 최적화 알고리즘Batch시스템 개발자의 설계에 따라 프로그램 흐름을 결정 First In, Fist Out 순차적 실행 방식 (&amp;lt;-&amp;gt;병렬 처리)Event-Driven이벤트 발생시 프로그램 흐름 결정Time Sharing동시에 다중 사용자 지원을 위해 컴퓨터 응답 시간을 최소화 하여 동시에 여러 사용자를 수용할 수 있도록 하는 방식Multi-tasking단일 CPU에서 여러 응용 프로그램이 동시에 실행되는 것처럼 보이는 방식으로, 실제로는 10~20ms 단위로 응용 프로그램이 변경되며 실행됨.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "StarGAN", "url": "/posts/StarGAN/", "categories": "1. 기술, 머신러닝, 딥러닝", "tags": "", "date": "2022-06-22 00:00:00 +0900", "snippet": "StarGAN - Basic Conceptreference : github/clovaai/stargan-v2 github/yunjey/stargan [코드리뷰]StarGAN 도메인과 스타일, 모두 잡았다! StarGAN v2 [논문리뷰]StarGAN:Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation StarGAN:Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation하나의 모델(GAN)이 하나의 도메인(특정 스타일)에 대해서만 학습하는 것이 아닌 여러 도메인을 한번에 학습하여 결과적으로 단일 모델만으로 다양한 스타일의 이미지를 생성해내도록 하는 컨셉이다.(a) Training the discriminatorD learns to distinguish between real and fake images and classify the real images to its corresponding domain. 기존 GAN의 D(Discriminator)가 1차적으로 input 데이터를 받고 학습하는 방식에 더해 도메인을 동시에 분류한다.(b) Original-to-target domainG takes in as input both the image and target domain label and generates an fake image. The target domain label is spatially replicated and concatenated with the input image. G(Generator)는 모방할 이미지(real image)와 One-Hot-Encoding 된 Target 도메인(목표로 하는 도메인)의 Vector를 input 값으로 받아 가짜 이미지(fake image)를 생성한다.(c) Target-to-original domainG tries to reconstruct the original image from the fake image given the original domain label. (b)에서 생성한 가짜 이미지(fake image)와 Original 도메인(진짜 이미지의 도메인)의 Vector를 input 값으로 받아 기존 이미지(Original image, 진짜 이미지)를 재구성한다(생성한다).(d) Fooling the discriminatorG tries to generate images indistinguishable from real images and classifiable as target domain by D. G(Generator)는 진짜 이미지와 구별하기 어려운 이미지를 생성(generate, reconstruct)한다. 또한 생성해낸 이미지를 D(Discriminator)가 목표로하는 target domain으로 분류해내도록 한다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "GAN", "url": "/posts/GAN/", "categories": "1. 기술, 머신러닝, 딥러닝", "tags": "", "date": "2022-06-20 00:00:00 +0900", "snippet": "GAN - Basic Conceptreference : GAN의 활용 사례와 발전 방향 ConditionalGAN(CGAN) 생성적 적대 신경망 논문으로 본 적대적 생성 신경망 GAN [NEVER Engineering 1시간만에 GAN(Generative Adversarial Network) 완전 정복하기](https://tv.naver.com/v/1947034) GAN(Generative Adversarial Network)은 특정 정보를 생성(Generative)하는 항과 이와 대립(Adversarial)하는 항으로 이루어진 망(Network)을 뜻한다.Training 생성자 모델 G(Generative)는 훈련 데이터 X 분포를 흉내내도록 훈련한다 판별자 모델 D(Discriminator, Adversarial)는 생성자 모델 G가 생성한 데이터가 진짜인지 여부를 판별하도록 훈련한다.Two-Player MiniMax Game 판별자 모델의 훈련데이터 예측 값, 즉 D(x)의 목표치는 1이다. 반면, 생성자 모델이 만들어낸 샘플에 대한 예측 값, 즉 D(G(x))의 목표치는 0이다. 여기서 판별자 모델 D는 목표치 0에 맞게 가짜 데이터(훈련 데이터가 아닌)를 진짜가 아니다(0)라고 판별하도록 훈련하는 동안 생성자 모델 G는 판별자 모델 D가 가짜 데이터를 진짜가 맞다(1)로 판별하도록 계속해서 학습한다. 판별자 모델 D는 가짜 데이터(G가 생성한 이미지)를 진짜로 판별할 확률을 최소화하고, 생성자 모델 G는 판별자 모델 D가 가짜 데이터를 진짜로 판별할 확률을 최대화하도록 하는 것이다. 이를 두 행위자 최소최대 게임(two-player minimax game)이라 한다. 학습이 거듭되면서 D(G(z))가 0에서 1까지 올라갈 수 있으며, 1에 가까울 수록 가짜 여부를 판별하기 어려워진다.Concept Code - Naver/최윤제님import torchimport torch.nn as nnD = nn.Sequential( nn.Linear(784, 128), # flattend image (28*28), hidden layer 128 nn.ReLU(), nn.Linear(128, 1), nn.Sigmoid() # classification - 0 or 1 (50%))G = nn.Sequential( nn.Linear(100, 128), # Latent vector (100), hidden layer 128 nn.ReLU(), nn.Linear(128, 784), # output == flattend image (28*28) nn.Tenh())criterion = nn.BCELoss() # loss functiond_optimizer = torch.optim.Adam(D.parameters(), lr=0.01)g_optimizer = torch.optim.Adam(G.parameters(), lr=0.01)# Assume x be real images of shape (batch_size, 784)# Assume z be random noise of shape (batch_size, 100)while True: # train D loss = criterion(D(x), 1) + criterion(D(G(z), 0) loss.backward() d_optimizer.step() # train G loss = criterion(D(G(z)), 1) loss.backward() g_optimizer.step()관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "ROC Curve", "url": "/posts/ROC-Curve/", "categories": "1. 기술, 머신러닝, 딥러닝", "tags": "", "date": "2022-06-19 00:00:00 +0900", "snippet": "Precision and Recall in an anomaly detection situation reference : document : sklearn.metrics.plot_roc_curve Classification - ROC &amp;amp; AUC ROC Curve, devopedia Measuring Performance: AUC(AUROC) ROC 커브는 False Positive Rate(FPR)을 기준으로 True Positive Rate(TPR)의 변화도를 표현한다. 아래 첫번째 분포곡선에서 임계값(beta)이 이동함에 따라 TPR과 FPR이 변화한다는 것을 예상해볼 수 있다.임계값이 우측으로 이동하면 FPR과 TPR 모두 줄어든다. 반대로 임계값이 좌측으로 이동하면 FPR과 TPR모두 커진다.즉, 확률상 0이라는 지점은 사실상 없다고 생각하기 때문에(-무한대) 임계값이 완전 좌측으로 이동했을 때 TP와 FP 수 모두 0으로 수렴하고 확률(R)도 0에 다다른다.확률(R)이 0인 지점(-무한대)에서 임계값 축이 다시 우측으로 출발한다고 가정하면, FPR과 TPR 모두 즉시 줄어들지만 실제 값 음성의 분포가 좌측에 있으므로 FPR이 더 빠르게 감소하게 되고, TPR은 서서히 줄어들게 된다.이를 그 아래 좌표평면으로 ROC Curve로 표현할 수 있다.ROC Curve plotted when threshold β is varied. Source: Wikimedia Commons 2015.만약 실제 값 음성 분포와 실제 값 양성 분포의 중심축(데이터가 충분히 많아서 정규분포에 근사한다고 가정할 때 평균에 해당)이 충분히 서로 멀어서 FP(Positive로 오판)와 FN(Negative로 오판)이 크게 감소한다면, 즉 겹치는 영역이 0에 가깝다면 ROC 커브는 좌상향으로 볼록하게 솟아오른다. 좋은 이진분류 모델은 아래 이미지에서 보라색 ROC 커브를 보여준다.Glass box, Rachel Draelos code exampleimport matplotlib.pyplot as pltfrom sklearn import datasets, metricsfrom sklearn.model_selection import train_test_splitfrom xgboost import XGBClassifierX, y = datasets.make_classification() # 분류 모델을 위한 데이터 자동 생성X_train, X_test, y_train, y_test = train_test_split(X, y) # 학습, 검증 데이터셋 분리, default : 25%model = XGBClassifier()model.fit(X_train, y_train) # training data로 모델 학습print(model.score(X_test, y_test)) 0.92metrics.plot_roc_curve(model, X_test, y_test)관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "LGBM", "url": "/posts/LGBM/", "categories": "1. 기술, 머신러닝, 딥러닝", "tags": "", "date": "2022-06-17 00:00:00 +0900", "snippet": "LGBM(Light Gradient Boosting Machine) LightGBM’s documentation LightGBM 주요 파라미터 정리 LightGBM이란?LGBM은 ‘Light’(==fast)하고 또 ‘Light’(==low memory)하다. 그만큼 또 예민하다는 단점이 있다. 예민하다는 것은 과적합(Overfitting)되기 쉽다는 것이며 1만 개 이하의 적은 데이터셋을 다루기에는 적합하지 않은 모델이다.LGBM을 사용할 때에는 과적합에 특히 유의해야 하며 아래의 하이퍼파라미터들을 통해 학습을 조절할 필요가 있다.max_depth Tree의 깊이가 깊을 수록 당연하게도 train set에 더 가까운 가중치를 학습하게 된다. 다른 모델을 사용할 때보다 현저하게 max_depth를 줄일 필요가 있으며 max_depth를 줄여도 test set에 대한 정확도가 올라가지 않는다면 해당 모델이 적합하지 않은 것이다.min_data_in_leaf 하나의 Leaf가 가져야 할 최소한의 record 수. 과적합을 해결하기 위해 20 아래로 줄여볼 수 있다.(default : 20)bagging_fraction bagging 단계에서 모든 데이터를 사용하지 않고 일부만 사용하는 비율 딥러닝 신경망에서 drop_out과 같이 과적합 방지 목적으로 사용된다.early_stopping_round XGBoost처럼 early_stopping 옵션을 지원한다. training set에 과하게 적합될 수록 validation set에 대한 정확도는 떨어지는데 이 때 계속해서 validation accuracy(혹은 성능)이 떨어지는 방향으로 머물러 있다면 학습을 중단한다. early_stopping은 모델 설계보다는 학습 단에서 과적합을 막아주는 옵션이다.lightGBM은 XGBoost와 마찬가지로 sklearn에서 지원하지 않는다. lightgbm library를 별도로 설치하여 사용한다.from lightgbm import LGBMClassifiermodel = LGBMClassifier() # all hyper-parameter : defaultmodel.fit(X_train, y_train)# Predictionprint(&#39;Train Score : {}&#39;.format(model.score(X_train, y_train)))print(&#39;Test Score : {}&#39;.format(model.score(X_test,y_test)))Train Score : 1.0Test Score : 0.97lightgbm은 Classification과 Regression 모두 지원한다. 아래는 Regression 예제이다.from lightgbm import LGBMRegressormodel = LGBMRegressor() # all hyper-parameter : defaultmodel.fit(X_train, y_train)# Predictionprint(&#39;Train Score : {}&#39;.format(model.score(X_train, y_train)))print(&#39;Test Score : {}&#39;.format(model.score(X_test,y_test)))Train Score : 0.97Test Score : 0.89lightgbm 역시 XGBoost와 마찬가지로 분류/회귀 유형 모두를 지원하므로 모델 학습 목적에 따라 LGBMClassifier 혹은 LGBMRegressor를 유연하게 사용할 수 있다.추가로, 위 Regression 예제에서 암묵적으로 사용된 데이터는 sklearn에 내장된 boston 데이터이다. 데이터의 수(X:404x13, y:404x1)가 적다보니 XGBoost보다 성능이 좋지 않다. lightgbm은 말 그대로 light하므로 대용량 데이터에 더 적합한(이점이 있는) 모델임을 알 수 있다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "xgboost", "url": "/posts/xgboost/", "categories": "1. 기술, 머신러닝, 딥러닝", "tags": "", "date": "2022-06-16 00:00:00 +0900", "snippet": "XGBoost(eXtreme Gradient Boost) 캐글로 배우는 머신러닝 #10 XGBoost 파이썬 Scikit-Learn 형식 XGBoost 파라미터 XGBoost 알고리즘의 개념 이해 머신러닝 앙상블(ensemble) xgboost란?XGBoost는 기존 Gradient Boosting 방식의 느리고, training set에 overfitting되는 문제를 어느 정도 해결한 고성능 ensemble 기법이다.규제 Overfitting을 방지하도록 하이퍼파라미터를 통해 규제할 수 있다. reg_alpha : L1 규제 reg_lambda : L2 규제early stopping 주로 딥러닝 학습에 파라미터로 사용되는 early stopping을 지원한다. model.fit(X_train, y_train, early_stopping_rounds=n…)CART Classification And Regression Tree 분류, 회귀 모두 지원한다.Common Hyper-parameters n_estimators : 결정 트리 수 learning_rate : 학습률(학습 보폭) max_depth : 트리의 깊이xgboost는 sklearn에서 지원하지 않는다. xgboost library를 별도로 설치하여 사용한다.from xgboost import XGBClassifiermodel = XGBClassifier() # all hyper-parameter : defaultmodel.fit(X_train, y_train)# Predictionprint(&#39;Train Score : {}&#39;.format(model.score(X_train, y_train)))print(&#39;Test Score : {}&#39;.format(model.score(X_test,y_test)))Train Score : 1.0Test Score : 0.98XGBoost는 Classification과 Regression 모두 지원한다. 아래는 Regression 예제이다.from xgboost import XGBRegressormodel = XGBRegressor() # all hyper-parameter : defaultmodel.fit(X_train, y_train)# Predictionprint(&#39;Train Score : {}&#39;.format(model.score(X_train, y_train)))print(&#39;Test Score : {}&#39;.format(model.score(X_test,y_test)))Train Score : 0.99Test Score : 0.97이처럼 XGBoost는 분류/회귀 유형 모두를 지원하므로 모델 학습 목적에 따라 XGBClassifier 혹은 XGBRegressor를 유연하게 사용할 수 있다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "sklearn - GridSearchCV", "url": "/posts/sklearn-GridSearchCV/", "categories": "1. 기술, 머신러닝, 딥러닝", "tags": "", "date": "2022-06-15 00:00:00 +0900", "snippet": "GridSearchCVreference : sklearn.model_selection.GridSearchCV(scikit-learn.org) 모형최적화, 데이터사이언스스쿨 [Chapter 4. 분류] 랜덤포레스트(Random Forest)머신러닝 모델의 하이퍼파라미터를 조정하는 일은 매우 까다롭다. 아주 미묘한 파라미터 값 변화가 모델의 성능을 좌우하고, 모델마다 다양한 파라미터들이 유기적으로 얽혀있기 때문이다. GridSearchCV를 통하면 다양한 하이퍼파라미터 값을 미리 입력하고, 최적의 값과 해당 값으로 표현된 모델 정확도를 돌려받을 수 있다.from sklearn.model_selection import GridSearchCVparams = {&#39;n_estimators&#39; : [10, 100], &#39;max_depth&#39; : [6, 8, 10, 12], &#39;min_samples_leaf&#39; : [8, 12, 18], &#39;min_samples_split&#39; : [8, 16, 20] }이제, 아래와 같이 모델을 넣고, 입력한 하이퍼 파라미터를 n_estimators, max_depth, min_samples_leaf, min_samples_split 순으로 (10, 6, 8, 8) -&amp;gt; (10, 6, 8, 16) -&amp;gt; (10, 6, 8, 20) -&amp;gt; (10, 6, 12, 8) 과 같이 순회하며 최고의 성능을 내는 최적의 하이퍼파라미터를 찾게 된다.model = RandomForestClassifier(random_state=42)grid_cv = GridSearchCV(model, param_grid=params, cv=3)여기서 cv를 3으로 지정하였는데, train, test set을 ‘폴드’라는 개념으로 검증을 위해 분할되는 수를 의미한다. 즉, 위에서 하이퍼 파라미터 조합의 수는 72(2x4x3x3)개인데, cv=3으로 넣어주면 72=3, 216번 학습&amp;amp;평가가 진행된다.해당 알고리즘의 학습은 머신러닝 모델 학습과 마찬가지로 fit을 통해 진행된다.grid_cv.fit(X_train, y_train)학습을 마친 다음 아래와 같이 최적의 파라미터, 성능을 도출해낼 수 있다.print(&#39;Best Hyper-Parameter : {}&#39;.format(grid_cv.best_params_))print(&#39;Best Accuracy : {:.4f}&#39;.format(grid_cv.best_score_))Best Hyper-Parameter : {&#39;max_depth&#39;: 12, &#39;min_samples_leaf&#39;: 8, &#39;min_samples_split&#39;: 8, &#39;n_estimators&#39;: 100}Best Accuracy : 0.9206이제 해당 값을 통해 모델을 다시 학습하게 되고, 최종적으로 test 데이터에 적용하고 마무리한다.model_final = RandomForestClassifier(n_estimators=100, max_depth = 12, min_samples_leaf = 8, min_samples_split=8, random_state=0 )model.fit(X_train, y_train)pred = model.predict(X_test)print(&#39;Final Accuracy : {:.4f}&#39;.format(accuracy_score(y_test,pred))Final Accuracy : 0.9230관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "파이썬 퀀트 분석 패키지 - bt", "url": "/posts/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%ED%80%80%ED%8A%B8-%EB%B6%84%EC%84%9D-%ED%8C%A8%ED%82%A4%EC%A7%80-bt/", "categories": "3. 튜토리얼, 금융 분석 프로그래밍 기초", "tags": "", "date": "2022-06-14 00:00:00 +0900", "snippet": "https://pmorissette.github.io/bt/index.html[bt - Flexible Backtesting for Python — bt 0.2.9 documentationWhat is bt? bt is a flexible backtesting framework for Python used to test quantitative trading strategies. Backtesting is the process of testing a strategy over a given data set. This framework allows you to easily create strategies that mix and match difpmorissette.github.io](https://pmorissette.github.io/bt/index.html)bt는 ffn을 베이스로 하는 백테스팅 패키지다. 사용법은 간단하다. data = bt.get(ticker, period)로 데이터 불러오기(-&amp;gt;data 변수에 저장) s = bt.Strategy(name, [Algorithms])로 백테스팅 알고리즘 생성하기(-&amp;gt;s 변수에 저장) test = bt.Backtest(s, data)로 백테스팅 객체 생성하기 (-&amp;gt;test 변수에 저장) res = bt.run(test)으로 백테스팅 실행하기 (-&amp;gt; res 변수에 저장) res.plot()으로 시각화하기이 흐름으로 전개되며 1번에서 어떤 티커 데이터를 어떤 기간만큼 가져올지 정하고, 2번에서 어떤 알고리즘을 사용할지 정하면 된다.bt 패키지에서 기본적으로 제공하는 알고리즘들이 있으며, class 객체로 상속받아 커스텀할 수 있다. 여기서는 간단하게 특정 시점에 매매 후 장기투자하는 전략을 테스트해보자.마이크로소프트(msft), 애플(aapl), 테슬라(tsla), 엔비디아(nvda) 4개 종목을 포트폴리오에 담는다고 했을 때, 미국 기술주 전반에 투자하는 것(qqq)과 미국 시장에 투자하는 것(spy) 등 총 5가지 옵션을 두고 위와 같이 수익률을 확인해볼 수 있다.future2(테슬라, 엔비디아) 종목의 상승 폭으로 인해 그래프상 다른 전략의 움직임이 보이지 않을 때 display() 함수로 디테일한 내용들을 살펴보면 된다.최대 낙폭이나 샤프지수, 소르티노 지수 등 자주 포는 포트폴리오 평가 척도를 간편하게 확인할 수 있다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "sklearn - GradientBoostingClassifier", "url": "/posts/sklearn-GradientBoostingClassifier/", "categories": "1. 기술, 머신러닝, 딥러닝", "tags": "", "date": "2022-06-14 00:00:00 +0900", "snippet": "GradientBoostingClassifier Gradient Boosting Model sklearn.ensemble.GradientBoostingClassifier 지도학습 - 그래디언트 부스팅 2.3.6 결정 트리의 앙상블, 텐서 플로우 블로그GradientBoosting 모델은 RandomForest 모델과 달리 learning_rate를 통해 오차를 줄여나가는 학습 방식을 사용한다. RandomForest 모델은 말그대로 Random하게 Bagging, Tree를 생성한다. 하지만 GradientBoosting 모델은 Tree를 생성할 때마다 이전 Tree보다 오차를 줄이게 된다. 또한 개별 Tree의 깊이는 얕게 만들어내면서 오차가 줄어든 Tree를 계속해서 연결해나가는 구조다.(때문에, Tree 깊이는 얕게, 갯수는 늘리는 방식을 주로 사용한다.)아래 3가지 하이퍼파라미터는 GradientBoosting 모델 성능(정확도)에 민감하게 작용한다.n_estimator default : 100 트리의 갯수를 의미한다. GradientBoosting 모델은 매 Tree 생성마다 학습오차를 줄이기 때문에(learning_rate) n_estimator가 많아질 수록, 즉 Tree를 많이 생성할 수록 Training 데이터셋에 대한 학습(예측)오차는 줄어들며 과적합된다. 이는 RandomForest 모델과 다른 점이다. RandomForest 모델은 n_estimator를 크게 할 수록 좋다.learning_rate default : 0.1 값이 작으면 이전 Tree의 학습 오차를 살짝만 줄익고, 값이 크면 크게 줄인다. 즉, 값이 커질 수록 training 데이터셋에 과적합된다.max_depth default : 3 개별 Tree의 깊이를 의미한다. 보통 1~3 정도로 설정한다. 깊이가 작은 특성때문에 이러한 단일 트리를 weak learner라고 한다. 이러한 weak learner는 적은 데이터(전체 중 좁은 일부)만 담기에 메모리를 적게 사용하고 예측이 빠른 장점이 있다.사용법은 다른 sklearn classification 모델들과 동일하다. 단, sklearn의 ensemble 패키지를 활용한다.from sklearn.ensemble import GradientBoostingClassifiermodel = GradientBoostingClassifier(randome_state=0) # default : max_depth=3, learning_rate=0.1model.fit(X_train, y_train)# Predictionprint(&#39;Train Score : {}&#39;.format(model.score(X_train, y_train)))print(&#39;Test Score : {}&#39;.format(model.score(X_test,y_test)))Train Score : 0.98Test Score : 0.93“sparse한 고차원 데이터에는 잘 동작하지 않는다”는 단점이 있다. 이는 Tree 모델의 공통적인 특징이다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "파이썬 퀀트 분석 패키지 - ffn(Financial Functions for Python)", "url": "/posts/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%ED%80%80%ED%8A%B8-%EB%B6%84%EC%84%9D-%ED%8C%A8%ED%82%A4%EC%A7%80-ffn-Financial-Functions-for-Python/", "categories": "3. 튜토리얼, 금융 분석 프로그래밍 응용", "tags": "", "date": "2022-06-13 00:00:00 +0900", "snippet": "https://github.com/pmorissette/ffn[GitHub - pmorissette/ffn: ffn - a financial function library for Pythonffn - a financial function library for Python. Contribute to pmorissette/ffn development by creating an account on GitHub.github.com](https://github.com/pmorissette/ffn)Python ffn 패키지는 퀀트 분석을 편하게 하도록 작성된 라이브러리다. 유용한 함수들을 많이 제공하고 있는데, 그중에서도 어렵지 않게 써먹을만한 함수들을 빠르게 익혀보자.Step 1. 데이터 추출기본적으로 야후 파이낸스를 통해 데이터를 가져오게 되어있고, 데이터 로드 속도도 빠른 편이다.Step 2. 기준일 스케일링자산 등락을 확인할 때 4개의 자산 가격의 기준일을 맞춰서 그래프를 그려보게 되는데, 여기서는 이러한 스케일링 작업을 rebase() 함수로 제공한다.Step 3. 수익률굳이 있어야 하나? 싶은 함수도 있다. 아래의 경우 일간 변화율(수익률)을 구하는 함수인데, pct_change()와 함수명만 다르고 동작은 동일하다.Step 4. 기술 통계이 패키지에서 제공하는 가장 유용한 부분은 기술통계다. 기본적인 기술 분석에 필요한 내용들은 calc_stats() 함수를 사용하면 빠르게 확인할 수 있다.기간별 수익률과 샤프지수, 최대 낙폭, 연간 승률 등 유용한 데이터를 제공하고 있다. 기본적이면서 중요하고, 반복적이라 귀찮은 작업들을 대신해주는 고마운 녀석이다.Step 5. 손실률손실률을 따로 볼 수도 있다. 수익이 난 구간은 0으로 대체하고 나머지 구간은 백분율을 비율로 풀어서(나누기 100) 보여준다.손실률을 그래프로 그리고, 가장 큰 손실을 MDD로 체크해볼 수 있다.Step 6. 기간별 증감 추이display_lookback_returns() 함수는 각 기간(월초, 3개월 전, 6개월 전, 연초, 1년 전.. , 최초 관찰일) 대비 증감률을 간단히 보여준다.Step  7. stats 객체stats 객체(ffn.core.PerformanceStats)는 판다스 각 변수(자산 데이터)에 대해 데이터 프레임처럼 인덱싱하게 해준다. 특정 자산의 월간 수익률을 다음과 같이 확인할 수 있다.객체로 히스토그램을 그릴 수도 있는데, 커스텀이 불편해 잘 사용하지는 않을 것 같다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "K-Nearest Neighbor Algorithm", "url": "/posts/K-Nearest-Neighbor-Algorithm/", "categories": "3. 튜토리얼, 금융 분석 프로그래밍 응용", "tags": "", "date": "2022-06-13 00:00:00 +0900", "snippet": "K-Nearest Neighbor Algorithm(최근접 이웃 알고리즘) Reference : K-NN 알고리즘(K-최근접이웃) 개념 파이썬 라이브러리를 활용한 머신러닝, 한빛미디어 Classification Regression1. Classification (n = 1) 기존에 분포하는 값 중 가장 가까운 값의 label을 현재 Test값의 label로 지정한다. (n &amp;gt; 1) 기존에 분포하는 값 중 가장 가까운 순서대로 n개의 값을 찾고, 가장 많이 나오는 label을 현재 Test값의 label로 지정한다. ex. N = 3일 때, 탐색 방식 ex. N = 3일 때, 코드 예시from sklearn.model_selection import train_test_splitX, y = mglearn.datasets.make_forge()X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)from sklearn.neighbors import KNeighborsClassifierclf = KNeighborsClassifier(n_neighbors=3)clf.fit(X_train, y_train) 이산형(분류) y_hat 값을 잘 만들어냈는지 확인print(&quot;y_pred : {}&quot;.format(clf.predict(X_test)))y_pred : [1 0 1 0 1 0 0] Accuracy Checkprint(&#39;Accuracy : {:.2f}&#39;.format(clf.score(X_test, y_test))) # Classification의 score는 전체 값 중 맞춘 값의 %를 반환한다.Accuracy : 0.862. Regression (n = 1) 기존에 분포하는 값 중 가장 가까운 값의 label을 현재 Test값의 label로 지정한다. (n &amp;gt; 1) 기존에 분포하는 값 중 가장 가까운 순서대로 n개의 값을 찾고, **n개 label의 평균 값(y_mean)**을 현재 Test값의 label로 지정한다. ex. N = 3일 때, 탐색 방식 ex. N = 3일 때, 코드 예시from sklearn.neighbors import KNeighborsRegressorX, y = mglearn.datasets.make_wave(n_samples=40)X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) # random_state를 통해 인덱스를 shuffle해주고 seed를 0으로 줌reg = KNeighborsRegressor(N_neighbors=3)reg.fit(X_train, y_train) 연속형(회귀) y_hat 값을 잘 만들어냈는지 확인print(&quot;y_pred : {}&quot;.format(reg.predict(X_test)))y_pred : [-0.054 0.357 1.137 -1.894 -1.139 -1.631 0.357 0.912 -0.447 -1.139] Accuracy Checkprint(&#39;Accuracy : {:.2f}&#39;.format(reg.score(X_test, y_test))) # regression의 score는 R^2값을 사용한다.Accuracy : 0.83종합 기존에 분포하고 있는 값들을 활요하는 컨셉이기 때문에 ‘훈련(학습)’의 개념이 필요하지 않다. 머신러닝 모델은 공통적으로 모든 X가 숫자형으로 구성되어야 한다. 여기서, KNN 알고리즘은 모두 숫자형 컬럼이라 하더라도 정규화 되어 있지 않거나, X가 sparse(one-hot-encoded)하다면 성능은 매우 떨어진다. 이는 ‘데이터의 거리’를 기준으로 Y_hat을 찾기 때문에 그렇다. 입력 X는 각 Data Point의 거리가 일관성 있고(정규화, Normalized), 연속 변수로 채워진 Vector(유클리드 거리 계산이 가능한)여야 한다. K가 작으면 과대적합, K가 크면 과소적합되는 경향이 있다. 분류 분석(이산형 y)에 사용된 KNN의 과대/과소 적합 예시 왼쪽부터 n=1, n=3, n=9 회귀 분석(연속형 y)에 사용된 KNN의 과대/과소 적합 예시왼쪽부터 n=1, n=3, n=9관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "Precision vs Recall", "url": "/posts/Precision-vs-Recall/", "categories": "1. 기술, 머신러닝, 딥러닝", "tags": "", "date": "2022-06-12 00:00:00 +0900", "snippet": "Precision and Recall in an anomaly detection situation reference : Classification : Precision and Recall 분류성능평가지표 - Precision(정밀도), Recall(재현률) and Accuracy(정확도) Examples : Anomaly Detection실제 이상 징후 : 1실제 정상 징후 : 0Timeseries1112131415161718192022232425                               Actual-Anm 0 0 0 0 0 0 0 0 0 1 0 1 1 1 threshold                       &amp;gt;     Detect_Anm 0 0 0 0 0 0 0 0 0 0 0 1 1 1 True-Pstv                       T T T False-Ngtv                   F         True-Ngtv T T T T T T T T T   T       False-Pstv                             Precision(정밀도) = 감지해낸(맞췄을 수도, 맞추지 못했을 수도 있는) 이상 징후 중 실제 이상 징후의 비율 = 실제 이상 징후 수 / 감지해낸 이상 징후 수 = 실제 이상 징후 수 / (맞춘 이상 징후 수 + 잘못 감지한 이상 징후 수) = n(TP) / (n(TP)+n(FP)) = 3 / 3 = 1Recall(재현률) = 실제 이상 징후 중 감지해낸(맞춘) 이상 징후의 비율 = 맞춘 이상 징후 수 / 실제 이상 징후 수 = 맞춘 이상 징후 수 / (맞춘 이상 징후 수 + 맞추지 못한 이상 징후 수) = n(TP) / (n(TP)+n(FN)) = 3 / 4 = 0.75보안 침해 대응시 위협을 판별하는 알고리즘은 정확도(Accuracy) 혹은 정밀도(Precision)보다 재현률(Recall)이 중요하다.실제 이상 징후가 들어왔을 때 모든 이상 징후를 100%에 가깝게 감지해내야 하기 때문이다. 즉, 정확도가 어느 정도 떨어지는 것을 감수하더라도 임계값(Threshold)을 낮춰서 이상치에 기민하게 반응하는(False Negative가 없는) 알고리즘을 구현해야 한다.Timeseries1112131415161718192022232425                               Actual-Anm 0 0 0 0 0 0 0 0 0 1 0 1 1 1 threshold                   &amp;gt;         Detect_Anm 0 0 0 0 0 0 0 0 0 1 1 1 1 1 True-Pstv                   T   T T T False-Ngtv                             True-Ngtv T T T T T T T T T           False-Pstv                     F       Precision(정밀도) = 감지해낸(맞췄을 수도, 맞추지 못했을 수도 있는) 이상 징후 중 실제 이상 징후의 비율 = 실제 이상 징후 수 / 감지해낸 이상 징후 수 = 실제 이상 징후 수 / (맞춘 이상 징후 수 + 잘못 감지한 이상 징후 수) = n(TP) / (n(TP)+n(FP)) = 4 / 5 = 0.8Recall(재현률) = 실제 이상 징후 중 감지해낸(맞춘) 이상 징후의 비율 = 맞춘 이상 징후 수 / 실제 이상 징후 수 = 맞춘 이상 징후 수 / (맞춘 이상 징후 수 + 맞추지 못한 이상 징후 수) = n(TP) / (n(TP)+n(FN)) = 4 / 4 = 1관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "파이썬으로 최적의 포트폴리오 비율 찾기 (한국 주식,국채 + 미국 주식,국채)", "url": "/posts/%ED%8C%8C%EC%9D%B4%EC%8D%AC%EC%9C%BC%EB%A1%9C-%EC%B5%9C%EC%A0%81%EC%9D%98-%ED%8F%AC%ED%8A%B8%ED%8F%B4%EB%A6%AC%EC%98%A4-%EB%B9%84%EC%9C%A8-%EC%B0%BE%EA%B8%B0-%ED%95%9C%EA%B5%AD-%EC%A3%BC%EC%8B%9D-%EA%B5%AD%EC%B1%84-%EB%AF%B8%EA%B5%AD-%EC%A3%BC%EC%8B%9D-%EA%B5%AD%EC%B1%84/", "categories": "2. 도메인, 금융", "tags": "", "date": "2022-06-11 00:00:00 +0900", "snippet": "주식 시장의 난이도가 올라가면서 개인 투자자들도 자산 배분을 고려하고 있다. 자산 배분의 방식에는 여러 가지가 있겠지만 이번에는 미국과 한국 2개의 시장에만 투자하고, 각 시장에 대해 채권과 주식을 고르게 배분하는 전략으로 전개하고자 한다. 안전자산인 채권과 위험자산 주식을 어떤 비율로 분배하는 것이 가장 효율적일까?Step 1. 야후 파이낸스 데이터 추출Step 2. 일일 수익률 및 최종 수익률 확인Step 3. 일일 수익률간 상관관계 시각화Step 4. 변동성(위험) 대비 수익률 시각화Step 5. 샤프 지수에 따른 포트폴리오 비율 시각화Step 6. 사프 지수에 따른 포트폴리오 수익률 및 변동성 시각화Step 7. 최적의 포트폴리오 비율Step 1. 야후 파이낸스 데이터 추출데이터는 야후 파이낸스에서 가져올 것이므로 클래스를 미리 정의해두자.class YFinance(): def __init__(self, tickers, period): self.period = period self.df_price = pd.DataFrame() self.df_dividends = pd.DataFrame() for ticker in tickers: print(f&#39;[INFO] 데이터 불러오는 중 .. (ticker : {ticker})&#39;) full_data = yf.Ticker(ticker).history(start=self.period[0], end=self.period[1]) self.df_price[ticker] = full_data[&#39;Close&#39;] self.df_dividends[ticker] = full_data[&#39;Dividends&#39;] time.sleep(0.5) print(f&#39;[INFO] 데이터셋 구성 완료&#39;) def get_price(self): return self.df_price def get_dividends(self): return self.df_dividends미국 주식은 S&amp;amp;P500 지수를, 한국 주식은 코스피 지수를 사용하며 채권은 각 국가의 7~10년물 국채 가격을 사용한다.tickers = [&#39;SPY&#39;, &#39;^KS11&#39;,&#39;IEF&#39;, &#39;148070.KS&#39;] # 미국 주식, 한국 주식, 미국 채권, 한국 채권start_date = &#39;2012-02-28&#39;end_date = &#39;2022-02-28&#39;y_finance = YFinance(tickers=tickers, period=(start_date, end_date))yf_price = y_finance.get_price() # 주가yf_dividends = y_finance.get_dividends() # 배당금print(&#39;-------------------------------\\n&#39;,yf_price.shape, yf_dividends.shape)2012년 2월 28일부터 2022년 2월 28일 전까지 데이터를 불러왔지만 미국 장과 한국 장의 개장일이 다르기 때문에 데이터가 부족한 컬럼이 있을 수 있다. dropna()를 통해 한 컬럼이라도 데이터가 빠진 경우 행을 날려준다.yf_price = yf_price.dropna()yf_price.columns = [&#39;us_stock&#39;,&#39;kr_stock&#39;,&#39;us_bond&#39;,&#39;kr_bond&#39;]yf_dividends = yf_dividends.dropna()yf_dividends.columns = yf_price.columnsprint(yf_price.shape, yf_dividends.shape)yf_price현재 포트폴리오에서 배당금은 미국 주식(S&amp;amp;P500)과 미국 채권(7~10년물)에서만 발생했다. 이후 포트폴리오 수익률 계산에 사용할 예정이니 참고하자.# 총 누적 배당금yf_dividends.sum()이렇게 가져온 데이터로 그래프를 한번 그려준다. 4개 티커 종가 범위가 모두 다르기 때문에 데이터가 시작되는 날짜를 기준일로 잡고 이후 증감에 대해 차트를 그린다.price_rate = yf_price/yf_price.iloc[0] # 기준일(2012-02-28) 대비 증감plt.figure(figsize=(12,4))sns.lineplot(data=price_rate, linewidth=0.85)plt.ylim((0, price_rate.max().max()))plt.title(&#39;Increase/decrease rate compared to the base date&#39;)plt.show()Step 2. 일일 수익률, 최종 수익률 시각화차트를 그렸으니 이번에는 일일 변동과 최종 수익률도 그래프로 그려보자. 일일 변동을 그려보면, 전체 기간 중 가격이 오른 날의 비중이 얼마나 되는지 확인할 수 있다.plt.figure(figsize=(12,8))pcc = yf_price.pct_change().iloc[1:,:] # 첫째 날 데이터 제거(NaN)for i in range(4): data = pcc.iloc[:,i] plt.subplot(int(f&#39;22{i+1}&#39;)) sns.lineplot(data=data, linewidth=0.85, alpha=0.7) inc_rate = (data &amp;gt; 0).sum() / len(data) * 100 plt.title(f&#39;&amp;amp;lt; {data.name} : Increase rate {inc_rate:.2f}% &amp;amp;gt;&#39;) plt.axhline(y=0, color=&#39;r&#39;, linestyle=&#39;--&#39;, linewidth=0.7, alpha=0.9)plt.suptitle(&#39;Percent change of each asset&#39;)plt.tight_layout()plt.show() S&amp;amp;P500의 경우, 2012년 2분기부터 2022년 1분기까지 전체 일수 중 55.64%가 상승했다. 코스피의 경우, 동기간 중 52.65%가 상승했다. 미국 국채의 경우, 동기간 중 52.4%가 상승했다. 한국 국채의 경우, 동기간 중 51.9%가 상승했다.각 컬럼별로 최종 수익률도 살펴보자. 최종 수익률은 수집 기간의 첫째 날 대비 마지막 날이 몇 % 상승했는가를 확인하면 알 수 있다.plt.figure(figsize=(9,4))bars = sns.barplot(x=return_rate.index, y=return_rate.values, color=&#39;Blue&#39;, alpha=0.3)for p in bars.patches: bars.annotate(f&#39;{p.get_height():.1f}%&#39;, (p.get_x() + p.get_width() / 2., p.get_height()), ha = &#39;center&#39;, va=&#39;center&#39;, xytext = (0,9), textcoords = &#39;offset points&#39;) plt.title(f&#39;Return rate of total period (%) - {(yf_price.index[-1] - yf_price.index[0]).days} days&#39;)plt.show() S&amp;amp;P500의 경우, 관찰 기간 동안 285.4% 상승했다. 코스피의 경우, 관찰 기간 동안 33.6% 상승했다. 미국 국채 선물의 경우, 관찰 기간 동안 24.8% 상승했다. 한국 국채 선물의 경우, 관찰 기간 동안 14.5% 상승했다.Step 3. 일일 수익률 간 상관관계 시각화포트폴리오를 꾸릴 때에는 상관관계가 낮은 자산들로 구성해야 한다. 수익률은 최대한 유지하면서 변동을 낮출 수 있기 때문이다. 아래 히트맵에 상관계수가 표시되어 있는데, 상관계수가 1에 가까울수록 같이 움직이는 자산이다. 가능하다면 상관계수가 0.2 이하의 낮은 수준으로 자산 조합을 선택하는 것이 좋다. 여기서는 미국 주식과 미국 국채, 한국 국채 3가지 조합이 좋아보인다. 한국 주식의 경우 미국 주식과 상관관계가 0.26으로, 포트폴리오 기준으로 고려할 때에는 다소 높은 편이다.plt.title(&quot;Correlation of all asset&#39;s percent change&quot;)sns.heatmap(yf_price.pct_change().corr(), cmap=&#39;Blues&#39;, linewidth=0.2, annot=True)plt.show()Step 4. 변동성(위험) 대비 수익률 시각화미국 주식과 채권, 한국 주식과 채권 4가지 조합은 상관계수가 과하게 높은 케이스가 없기 때문에(미국 주식과 한국 주식 조합이 우려되긴 하지만) 포트폴리오로 고려해볼 수 있다. 이제는 포트폴리오 비중을 계산할 시간이다. 비중은 numpy random 함수를 통해 무작위로 총합 1이 되게 만들어주고, 1만 개의 비율 조합에 따른 연간 포트폴리오 리스크와 연평균 수익률을 계산하면 되겠다.연 평균 수익률의 경우 전체 기간의 수익과 배당금을 합산한 금액을 첫날 자산 가격으로 나눠준 다음 10년의 기간에 대해 기하평균(10 제곱근)을 계산해주면 구할 수 있다. 복리 개념이기 때문에 단순 평균으로 계산하면 오차가 커진다.연간 포트폴리오 리스크는 연간 수익률의 공분산을 구한 다음 ‘포트폴리오 비율의 역행렬 x 공분산 x 포트폴리오 비율’을 계산하면 포트폴리오 분산(변동성 제곱)이 나오는데, 루트를 씌워 표준편차(변동성)로 만들어주면 된다.import numpy as npport_ratios = []port_returns = np.array([])port_risks = np.array([])for i in range(10000): # 포트폴리오 비율 조합 10000개 # 포트폴리오 비율 port_ratio = np.random.rand(len(yf_price.columns)) # 4가지 랜덤 실수 조합 port_ratio /= port_ratio.sum() # 합계가 1인 랜덤 실수 port_ratios.append(port_ratio) # 연 평균 수익률 total_return_rate = (yf_price.iloc[-1] + yf_dividends.sum()) / yf_price.iloc[0] # 배당금 합산 총 수익률(%) annual_avg_rr = total_return_rate ** (1/10) # 연 (기하)평균 수익률(%) port_return = np.dot(port_ratio, annual_avg_rr-1) # 연 평균 포트폴리오 수익률 = 연 평균 수익률과 포트폴리오 비율의 행렬곱 port_returns = np.append(port_returns, port_return) # 연간 리스크 annual_cov = yf_price.pct_change().cov() * len(yf_price)/10 # 연간 수익률의 공분산 = 일별 수익률 공분산 * 연간 평균 거래일수 port_risk = np.sqrt(np.dot(port_ratio.T, np.dot(annual_cov, port_ratio))) # E(Volatility) = sqrt(WT*COV*W) port_risks = np.append(port_risks, port_risk)위 식이 돌면, 1만 개의 포트폴리오 비율 조합과 각 포트폴리오 비율에 따른 연 평균 수익률과 연간 리스크가 계산된다.여기서 구한 연 평균 수익률을 연간 리스크로 나눠주면 샤프지수의 근사치(*샤프지수는 ‘(포트폴리오 수익률-시장 수익률)/(포트폴리오 리스크)’로 계산되는데, 여기서는 순위만 매기면 되기 때문에 시장 수익률을 빼는 과정이 생략되어 있다. 이 때문에 근사치라 표현했다.)가 나온다. 샤프지수는 공식에서 알 수 있듯이 단위 위험(편차, 변동성) 당 수익률로, 감수하는 리스크 대비 기대 수익이 얼마나 되는가를 표현한다. 포트폴리오의 효율성을 판단하는 데에 유용하게 쓰이는 지표다.아래 차트는 1만개의 무작위 포트폴리오를 리스크 대비 수익률로 보여주고 있다. 그중에서 샤프지수가 가장 높은 것과 변동성이 가장 낮은 것을 각각 빨간색 삼각형, 파란색 삼각형으로 표시해줬다.sorted_shape_idx = np.argsort(port_returns/port_risks)sorted_risk_idx = np.argsort(port_risks)plt.figure(figsize=(12,6))sns.scatterplot(x=port_risks, y=port_returns, c=port_returns/port_risks, cmap=&#39;cool&#39;, alpha=0.85, s=20)sns.scatterplot(x=port_risks[sorted_shape_idx[-1:]], y=port_returns[sorted_shape_idx[-1:]], color=&#39;r&#39;, marker=&#39;^&#39;, s=500)sns.scatterplot(x=port_risks[sorted_risk_idx[:1]], y=port_returns[sorted_risk_idx[:1]], color=&#39;b&#39;, marker=&#39;v&#39;, s=500)plt.title(&#39;Return per unit risk&#39;)plt.show()Step 5. 샤프 지수에 따른 포트폴리오 비율 시각화이번에는 샤프 지수가 가장 높은 포트폴리오부터 가장 낮은 포트폴리오까지 샤프 지수 순으로 나열해서 보여줄 것인데, 포트폴리오에서 각 자산이 차지하는 비율을 색상으로 표현한다.port_df = pd.DataFrame(port_ratios)sorted_port_df = port_df.iloc[sorted_shape_idx[::-1]] # 역순sorted_port_df.columns = yf_price.columnsplt.figure(figsize=(12,4))plt.stackplot(np.arange(1,len(sorted_port_df)+1,1), np.array(sorted_port_df.T), labels=sorted_port_df.columns)plt.xlim(0,10000)plt.legend(bbox_to_anchor=(1.12,0.95))plt.xlabel(&#39;Ranking of Sharpe Ratio&#39;)plt.ylabel(&#39;Portfolio Ratio&#39;)plt.title(&#39;Ranking of Optimal Portfolios by Sharpe Ratio&#39;)plt.show()왼쪽으로 갈수록 샤프지수가 가장 높은 포트폴리오다. 샤프지수가 높을수록 미국 채권의 비중이 높아지고, 한국 주식 비중은 낮아지는 것을 알 수 있다. 한국 국채와 미국 주식의 비중은 남는 포지션에 적절히 배분하는 것이 좋겠다.Step 6. 샤프 지수에 따른 포트폴리오 수익률 및 변동성 시각화포트폴리오의 샤프지수가 높을 수록 수익률과 변동성이 어떤지 눈으로 확인하고 싶다. 따라서 같은 X축(샤프지수 내림차순)을 놓고 수익률과 변동성을 y축으로 그려본다.sorted_returns = port_returns[[sorted_port_df.index]]sorted_risks = port_risks[[sorted_port_df.index]]plt.figure(figsize=(12,4))plt.fill_between(x=np.arange(1,len(sorted_returns)+1,1), y1=sorted_returns.tolist(), label=&#39;return&#39;)plt.fill_between(x=np.arange(1,len(sorted_risks)+1,1), y1=sorted_risks.tolist(), alpha=0.3, label=&#39;risk&#39;)plt.xlabel(&#39;Ranking of Sharpe Ratio&#39;)plt.ylabel(&#39;Return &amp;amp; Risk&#39;)plt.title(&#39;Returns &amp;amp; Risks of Portfolio by Sharpe Ratio Ranking&#39;)plt.legend()plt.show()샤프지수가 높을 수록 수익률은 높아지고, 변동성은 낮아지는 경향을 보인다.Step 7. 최적의 포트폴리오 비율그래서 만약 미국 국채와 주식, 그리고 한국 국채와 주식으로 포트폴리오를 구성한다면, 2012년 2분기부터 2022년 1분기까지의 수익률 기록을 토대로 볼 때 아래의 비율이 최적의 포트폴리오가 된다.print(f&#39;최적의 포트폴리오 비율 : \\n{pd.Series(sorted_port_df.iloc[0], index=sorted_port_df.columns)}&#39;)관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "sklearn - RFE", "url": "/posts/sklearn-RFE/", "categories": "1. 기술, 머신러닝, 딥러닝", "tags": "", "date": "2022-06-10 00:00:00 +0900", "snippet": "RFE (recursive feature elimination)Reference : Python 데이터 분석 실무 04-06.모델 성과 개선 (WikiDocs) Simple, yet, Powerful Bankrupt Prediction Model sklearn.feature_selection.RFE, scikit-learn.orgRFE는 주요 Feature를 선별해내는 기법 중 하나로 이름(Recursive feature elimination) 그대로 ‘반복적으로 feature를 제거해나가는’ 방식이다.대표적인 머신러닝 라이브러리 sklearn에서는 feature_selection 모듈을 제공한다. RFE는 아래와 같이 해당 모듈에서 import 할 수 있다.from sklearn.feature_selection import RFERFE를 두고, 반복적으로 feature를 제거해나가는 방식이라 했다. 2개의 파라미터를 넣어주는 것을 통해 간단히 사용해볼 수 있다.select_10features = RFE(RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=10) # RandomForestClassifier 모델 import는 생략하였음. Reference : Kaggle - Simple, yet, Powerful Bankrupt Prediction Modelkaggle 아래와 같이 깔끔하게 줄여볼 수 있다.model = RandomForestClassifier(n_estimators=100, random_state=42)select_10features = RFE(model, 10) # n_features_to_select 생략RFE는 머신러닝 모델을 학습할 때와 마찬가지로 fit을 통해 feature를 선별해나가는 과정이 필요하다.selector = select_10features.fit(X,y)fit을 했다면 selector를 통해 몇가지 데이터를 확인할 수 있다.print(&#39;Number of Features : {}&#39;.format(selector.n_features_))print(&#39;Is feature selected? : {}&#39;.format(selector.support_))print(&#39;Ranking of Features : {}&#39;.format(selector.ranking_))Number of Features : 10Is feature selected? [True, True, True, True, True, False, False, False, False, False]Ranking of Features : [1, 1, 1, 1, 1, 6, 4, 3, 2, 5]관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "sklearn - RandomForestClassifier", "url": "/posts/sklearn-RandomForestClassifier/", "categories": "1. 기술, 머신러닝, 딥러닝", "tags": "", "date": "2022-06-09 00:00:00 +0900", "snippet": "Random Forestreference: Random Forest(랜덤 포레스트) 개념 정리 sklearn.ensemble.RandomForestClassifierBagging(Bootstrap aggregating) 모집단(데이터셋)에서 부분집합 형태로 임의의 N개를 추출하여 트리 하나를 만든다. 추출한 N개는 다시 넣어서 모집단 원본을 만들고, 또 다시 임의의 N개 데이터를 추출하여 두 번째 트리를 만든다.(실제로 데이터를 다시 넣지는 않는다. 중복을 허용한다는 의미로 서술) max_features 파라미터를 통해 feature는 최대 몇개를 샘플링하여 트리를 구성할 지 정의할 수 있다. 전체 트리 결과값이 나오면 가장 분포가 많은 값이 RandomForestClassifier 모델의 최종 결과 값으로 선정된다.RandomForestClassifier는 위 Bagging 동작을 거쳐 최종 모델을 반환한다. 이 원리만 이해한다면 문법은 단순하다.from sklearn.ensemble import RandomForestClassifiermodel = RandomForestClassifier(n_estimators=100) # 100개의 트리를 만든다. 필요하다면 max_features = n 을 통해 최대 feature 갯수를 지정할 수도 있다.model.fit(X_train, y_train)print(&#39;Train Accuracy : {:.2f}&#39;.format(model.score(X_train, y_train)))print(&#39;Test Accuracy : {:.2f}&#39;.format(model.score(X_test, y_test)))Train Accuracy : 0.96Test Accuracy : 0.92관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "pandas - melt()", "url": "/posts/pandas-melt/", "categories": "3. 튜토리얼, 금융 분석 프로그래밍 응용", "tags": "", "date": "2022-06-08 00:00:00 +0900", "snippet": "pandas.meltreference document : pandas.meltmelt() 함수는 표현처럼 데이터프레임을 마치 ‘녹이는’ 것처럼 재구성한다.기존의 열(columns) 이름을 값(values)으로 내려 각각의 행(index)에 열 이름과 각각 해당하는 값을 나열해서 보여준다.import pandas as pddf = pd.DataFrame({ &#39;id&#39;: [&#39;201&#39;,&#39;873&#39;,&#39;385&#39;,&#39;505&#39;], &#39;type&#39;:[&#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;D&#39;], &#39;count&#39;:[8,8,6,2]})print(df)idtypecount         0 201 A 8 1 873 B 8 2 385 C 6 3 505 D 2 위 데이터프레임에서 type 컬럼은 A,B,C,D 라는 4가지 값을 가지고 있다.melt()를 사용해 type을 값으로 표기하고 각각의 기존 값(A,B,C,D)는 그것에 뒤따라 붙일 수 있다.pd.melt(df, id_vars=&#39;id&#39;, value_vars=&#39;type&#39;)idvariablevalue         0 201 type A 1 873 type B 2 385 type C 3 505 type D 만약 type, count 컬럼을 모두 값으로 표기하고 싶다면 value_vars 인자를 리스트로 전달하면 된다.pd.melt(df, id_vars=&#39;id&#39;, value_vars=[&#39;type&#39;,&#39;count&#39;])idvariablevalue         0 201 type A 1 873 type B 2 385 type C 3 505 type D 4 201 count 8 5 873 count 8 6 385 count 6 7 505 count 2 혹은 id와 함께 type도 그대로 컬럼으로 유지하고 count만 값으로 내리고 싶을 수도 있다. 이 경우 id_vars인자를 리스트로 전달하면 된다.pd.melt(df, id_vars=[&#39;id&#39;,&#39;type&#39;], value_vars=&#39;count&#39;)idtypevariablevalue           0 201 A count 8 1 873 B count 8 2 385 C count 6 3 505 D count 2 마지막으로 melt() 함수를 통해 새롭게 생성된 variable, value 컬럼의 이름을 바꾸고 싶다면, var_name과 value_name 파라미터를 추가해주면 되겠다.pd.melt(df, id_vars=&#39;id&#39;, value_vars=[&#39;type&#39;,&#39;count&#39;], var_name=&#39;info_group&#39;, value_name=&#39;info&#39;)idinfo_groupinfo         0 201 type A 1 873 type B 2 385 type C 3 505 type D 4 201 count 8 5 873 count 8 6 385 count 6 7 505 count 2 각각의 id에 대해 정보를 표시하는 type과 count를 하나의 그룹 info_group으로 묶었고, 그룹에 해당하는 값들을 info로 묶었다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "파이썬을 활용한 코스피, 달러 환율정보 수집부터 차트 시각화까지", "url": "/posts/%ED%8C%8C%EC%9D%B4%EC%8D%AC%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-%EC%BD%94%EC%8A%A4%ED%94%BC-%EB%8B%AC%EB%9F%AC-%ED%99%98%EC%9C%A8%EC%A0%95%EB%B3%B4-%EC%88%98%EC%A7%91%EB%B6%80%ED%84%B0-%EC%B0%A8%ED%8A%B8-%EC%8B%9C%EA%B0%81%ED%99%94%EA%B9%8C%EC%A7%80/", "categories": "4. 실전, 글로벌 매크로 분석", "tags": "", "date": "2022-06-06 00:00:00 +0900", "snippet": "Step 0. XHR 이란XHR은 XMLHttpRequest의 약자로, Ajax처럼 페이지 리로드 없이 서버의 데이터를 불러올 수 있는 통신 방식이다. 브라우저 상에서 제공하는 객체 형태의 API라고 이해하면 좀 더 와닿겠다.그래서 이 XHR은 기본적으로 브라우저가 웹서버와 통신하기 위한 수단이지만, 우리가 직접 사용해 웹 서버에서 직접 원하는 데이터를 호출할 수도 있다. 방식은 크게 2가지로, POST와 GET 방식이 있다. 여기서 두 케이스를 모두 경험해보기 위해 코스피 정보는 krx정보데이터시스템에서, 달러 정보는 yahoofinance에서 긁어보도록 하겠다. 참고로 이렇게 사이트마다 데이터를 호출하는 방식이 다르기 때문에 데이터를 수집하고자 한다면 어떤 형태로 서버와 통신하는지 사전에 확인해야 한다.Step 1. KOSPI 200 거래 정보 가져오기먼저, 두 방식 모두 url 주소로 데이터를 요청하게 되며 python은 requests 패키지를 사용한다. POST 방식의 경우 requests.post() 함수를 사용하는데, 이때 요청 정보를 함께 넘겨줘야 한다.아래 화면은 Safari 브라우저에서 개발자 모드를 켠 다음 네트워크 탭의 XHR 영역을 보여준다. 해당 영역 하단에 ‘요청 데이터’라는 부분이 보인다. POST 방식은 이렇게 요청할 데이터를 넘겨주면 해당하는 데이터를 다시 반환해주는 구조다. 코드에서는 이 부분을 payloads 객체로 만들어서 requests.post(url, data=payloads) 형태로 넘겨줬다.Step 2. 환율 정보 가져오기다음은 yahoofinance다. 여기는 GET 방식으로 데이터를 가져오고 있으며, 별도의 ‘요청 데이터’는 넘겨주지 않아도 된다. 하지만 아래 화면에서 ‘header’를 url과 함께 넘겨줬는데, 이는 서버에서 보안상 헤더가 없는 요청은 봇이나 침입으로 감지하고 차단하기 때문이다. 헤더 없이 요청하면 403 에러를 볼 수 있는데, 모든 사이트가 그런 것은 아니지만 yahoofinance처럼 데이터를 서비스하는 사이트는 대부분 이러한 최소한의 조치를 하고 있다.yahoofinance가 특이했던 점은 원달러 환율 정보를 요청할 때 기간 등 여러가지 옵션을 선택할 수 있는데, 예를 들면 언제부터 언제까지 며칠 주기의 데이터를 확인할 것인지에 대한 정보를 넘겨줘야 한다. 보통 이런 부분들을 POST로 구현하는데 yahoofinance의 경우 url 내에 query 형태로 던져주는 방식을 취하고 있다. 그래서 상단의 URL 부분을 보면 굉장히 긴데, 요청하고자 하는 정보가 담겨있다고 보면 된다.긴 내용중에 우리가 눈여겨볼 부분은 period1, period2, 그리고 interval 부분이다. 짐작할 수 있듯이 period1은 조회 시작 날짜, period2는 조회 마지막 날짜, interval은 조회 간격이다. period는 기본적으로 unix 시간을 채택하고 있다. 참고로 unix 시간 체계는 1970년부터 초 단위로 계산되는 표준 시간 단위이며, python을 사용해 오늘 날짜를 unix 시간으로 변경할 수 있다. 우리는 yahoofinance에서 제공하는 최대 기간을 모두 조회할 것이기 때문에 period1은 0으로 설정했다. unix 시간은 정수형이기 때문에 꼭 특정 값을 입력하지 않고 가장 작은 숫자를 입력해줘도 알아서 최솟값을 잡아준다. 물론 서버마다 처리 로직이 다를 수 있으니 무조건 통용되는 방법은 아니다.이렇게 데이터를 호출하면 json 형태의 결과값을 받을 수 있는데, 계층 구조가 눈에 확 들어오지 않기 때문에 key값을 한 단계씩 조회해보면서 데이터를 확인하면 된다.이렇게 찾아가다보면 우리가 필요한 두 list를 추출할 수 있는데, 하나는 시간이고 다른 하나는 환율 정보다.Step 3. 데이터 병합이렇게 KOSPI 200과 환율 정보를 확보했으므로 데이터를 합쳐주겠다.일자를 기준으로 inner join 방식의 집계를 사용했다. 두 데이터의 길이가 다르기 때문에 중복 구간만 추출하는 것이다. 우리는 여기서 종가, per, pbr, 환율만 사용할 것이며 4개의 데이터는 분석을 위해 모두 실수형으로 변환해줘야 한다. 또한 index는 ‘일자’로 설정해줘서 시계열 분석이 편하도록 세팅한다.Step 4. 평균 및 표준편차를 포함한 시계열 데이터 그리기10년간 각 지표의 평균치를 붉은 점선으로 그렸고, 붉은 박스 영역은 평균으로부터 1 표준편차 이내의 영역을 알려준다. 이는 각 지표의 높낮이에 대한 가늠좌가 되어준다.Step 5. 표준화를 통해 시계열 겹쳐보기위 그래프를 보면 y 축의 수치가 모두 다르다. 그래프를 겹쳐서 보고 싶다면 방법은 2가지인데, 하나는 2중 y축(왼쪽+오른쪽)을 사용하는 방식, 다른 하나는 각 지표에 대해 표준화 혹은 정규화를 수행하는 방식이 있다. 여기서는 후자의 사례를 보여준다.이렇게 표준화를 해주면 4개의 지표 모두 평균이 0이고 표준편차가 1인 동일한 분포를 가질 수 있다.이제 환율과 나머지 3개 지표를 겹쳐 그려보면 다음과 같다.이를 통해 원달러 환율과 코스피 지표간의 움직임을 상대적으로 비교해서 확인할 수 있으며, 추후 rolling(시점 이동)이나 이동평균을 사용해 선/후행 지표를 찾아낼 수도 있다. 또한, 실시간으로 주가를 모니터링할 수 있도록 스케줄러를 사용하면 위 그래프의 실시간 추이도 확인할 수 있다. 그러니 더 멋진 아이디어로 발전시켜보길 바란다!관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "pandas - forward fill", "url": "/posts/pandas-forward-fill/", "categories": "3. 튜토리얼, 금융 분석 프로그래밍 응용", "tags": "", "date": "2022-06-06 00:00:00 +0900", "snippet": "df.ffill()reference: pandas.DataFrame.ffill(pandas.pydata.org) Python pandas - 결측값 채우기 … [Python Pandas dataframe.ffill()](https://www.geeksforgeeks.org/python-pandas-dataframe-ffill/) Pandas DataFrame ffill() Method머신러닝 모델은 결측치가 존재하는 데이터프레임을 받아들이지 못한다.따라서 데이터 전처리 과정에서는 반드시 NaN 값을 체크하고 해당 결측치를 특정 값으로 대체하거나 해당 결측치가 포함된 열 혹은 행을 제거해야 한다.그러나 열 혹은 행을 제거한다면 중요한 데이터 소스를 잃어버리게 된다. 이를 감수할 만큼 해당 데이터 열 혹은 행이 유의미하지 않다면 무관하나 유의미하다면 제거하기보다 새로운 값으로 대체하는 것이 좋겠다.결측치 처리 방법으로 다양한 것이 있지만 여기서는 forward-fill 방식, 즉 동일한 column에서 바로 위 인덱스를 참조하여 동일한 값을 입력하는 방식 을 정리한다.forward-fill 방식을 지원하는 함수는 아래와 같다. df.ffill() df.fillna(method=’ffill’) df.fillna(method=’pad’)편한 방식으로 사용하면 되고, 해당 함수를 적용했을 때 결측값이 처리되는 모습은 아래와 같다.1. df.ffill()print(df) X1 X2 X30 0.88 56 1021 0.23 NaN NaN2 0.10 52 1203 0.15 49 114df.ffill() # Default : axis=0 (along the index) X1 X2 X30 0.88 56 1021 0.23 56 1022 0.10 52 1203 0.15 49 114여기서는 axis가 0인 경우, 즉 ‘행(index)’을 기준으로 위 인덱스를 참조하여 아래 인덱스에 해당하는 NaN값을 채웠다.이와 달리 ‘열(column)’을 기준으로 앞의 열을 따라 뒤 열을 채우겠다면 axis를 1로 지정하면 된다.df.ffill(axis=1) X1 X2 X30 0.88 56 1021 0.23 0.23 0.232 0.10 52 1203 0.15 49 114데이터는 row와 column으로 구성되어 있다. row는 각각의 개별 관측치를 담고, column은 개별 관측치마다의 서로 다른(독립적인) 특징들을 담는다.axis=1은 독립된 특징을 담는 서로 다른 column을 앞 뒤로 참조하여 채우는 방식이다. 독립된 column끼리 참조하여 값을 채운다는 것은 일반적인 사례가 아니다.따라서 일반적인 경우 axis=0 Default 파라미터를 사용한다.2. df.fillna(method=’ffill’)사실, df.ffill() 보다는 df.fillna() 메소드가 더 많이 활용된다.df.fillna(n) 을 통해 NaN 값을 n으로 채우거나 df.fillna(method=’ffill’)을 통해 앞에서 봤던 df.ffill()의 기능도 동일하게 구현할 수 있다. 물론 df.ffill(axis=1)과 마찬가지로 axis=1 파라미터를 지정함으로써 column을 기준으로 앞의 컬럼에 해당하는 값을 참조하여 NaN값을 채우는 것도 가능하다.print(df) X1 X2 X30 0.88 56 1021 0.23 NaN NaN2 0.10 52 1203 0.15 49 114df.fillna(0) # Default : axis=0 (along the index) X1 X2 X30 0.88 56 1021 0.23 0 02 0.10 52 1203 0.15 49 114df.fillna(method=&#39;ffill&#39;) # Default : axis=0 (along the index) X1 X2 X30 0.88 56 1021 0.23 56 1022 0.10 52 1203 0.15 49 114df.fillna(method=&#39;ffill&#39;, axis=1) # Default : axis=0 (along the index) X1 X2 X30 0.88 56 1021 0.23 0.23 0.232 0.10 52 1203 0.15 49 1143. df.fillna(method=’pad’)method=’pad’는 mehod=’ffill’과 완전히 동일한 기능을 한다. 내용은 생략한다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "sklearn - confusion_matrix()", "url": "/posts/sklearn-confusion-matrix/", "categories": "1. 기술, 머신러닝, 딥러닝", "tags": "", "date": "2022-06-05 00:00:00 +0900", "snippet": "sklearn.metrics.confusion_matrixreference : sklearn.metrics.confusion_matrix, scikit-learn.org 분류 성능평가, 데이터사이언스스쿨 Documentsklearn.metrics.confusion_matrix(y_true, y_pred, *, labels=None, sample_weight=None, normalize=None) 사용 예시from sklearn.metrics import confusion_matrixy_true = [2, 0, 2, 2, 0, 1]y_pred = [0, 0, 2, 2, 0, 2]confusion_matrix(y_true, y_pred)array([[2, 0, 0], [0, 0, 1], [1, 0, 2]])해석해보면,클래스012         0       1       2       위 표가 pandas DataFrame이고, Column은 예측 클래스(y_pred), Index는 정답 클래스(y_true)라고 해보자. 이제 빈 공간(value)은 예측 클래스와 정답 클래스가 일치한다면 1씩 채워지게 된다.클래스012         0 2 0 0 1 0 0 1 2 1 0 2 즉, 실제 클래스가 0일 때, 0이라 예측한 수가 2 실제 클래스가 2일 때, 0이라 예측한 수가 1 실제 클래스가 1일 때, 2라 예측한 수가 1 실제 클래스가 2일 때, 2라 예측한 수가 2 숫자를 합치면 6이 되고, 이는 예측한 클래스 수와 동일. 나머지 0으로 표기된 부분은 예측하지 않은 부분.를 의미한다.마찬가지로 이진분류의 경우도 생각해볼 수 있다.클래스FalseTrue       False     True     (맞다, 아니다), (양성, 음성), (정상, 불량) 등 많은 곳에 활용되는 이진분류는 위 표와 같이 표현할 수 있다. 다중 분류와 마찬가지로 Column은 예측 클래스, Index는 정답 클래스다.y_true = [1, 0, 1, 1, 0, 1]y_pred = [0, 0, 1, 1, 0, 1]confusion_matrix(y_true, y_pred)array([[2, 0], [1, 3]])표를 통해 아래와 같이 표현할 수 있다.클래스FalseTrue       False 2 0 True 1 3 False : 0 True : 1해석하면, 실제 False(0)를 False(0)라고 제대로 예측한 경우가 2번 실제 True(1)를 True(1)라고 제대로 예측한 경우가 3번 실제 False(0)임에도 불구하고 True(1)라고 잘못 예측한 경우는 0번 실제 True(1)를 False(0)로 잘못 예측한 경우가 1번여기서, 양성(True) 값을 제대로 발견하지 못한 경우가 1번 있었고 이런 부분들이 모델의 정확도(Accuracy)를 떨어뜨린다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "GCP - firestore", "url": "/posts/GCP-firestore/", "categories": "1. 기술, 서버, 데이터, 클라우드", "tags": "", "date": "2022-06-05 00:00:00 +0900", "snippet": "firestore를 사용하기 위해 먼저 구글 클라우드 플랫폼의 IAM 관리자에서 서비스 계정을 하나 만들어준다(이미 있으면 pass)서비스계정을 만들면 자동으로 인증키 json 파일이 다운로드 된다. 해당 파일을 가지고  나의 firebase_admin을 초기화해주면 설정 완료.설정(키 인식)이 끝났다면, 데이터를 넣어본다. 만약 doc_ref 변수로 지정한 collection과 document가 없다면 알아서 생성된다.*데이터 구조: 컬렉션 &amp;gt; 문서 &amp;gt; 필드 &amp;gt; 데이터 기존에 가지고 있던 collection, document가 있다면, 해당 경로에 데이터가 추가된다문서 아래에는 하위 컬랙션을 추가할 수도 있다.해당 컬랙션에는 message 정보를 담아보자만약, 데이터프레임 형태의 데이터를 가지고 있다면 딕셔너리 타입으로 변경해서 넘겨주면 된다.그동안 사용한 set 명령어는 문서 전체를 덮어쓰게 된다. 따라서 일부 값만 수정하고 싶으면 update를 사용한다. update 명령어를 통해 특정 필드와 필드에 해당하는 값만 삭제할 수도 있다.get()을 사용하면 단일 문서의 내용을 가져올 수 있다.만약 한 컬랙션 내의 모든 문서를 조회하고 싶다면,  컬렉션 객체에 대해 stream()함수를 사용하면 된다 단, 이 때 하위 컬렉션은 함께 조회되지 않는다.컬랙션 내 문서 조회 시 쿼리를 적용할 수도 있다.정렬 쿼리는 아래와 같이 사용한다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "Docker - 삭제 명령어", "url": "/posts/Docker-%EC%82%AD%EC%A0%9C-%EB%AA%85%EB%A0%B9%EC%96%B4/", "categories": "1. 기술, 서버, 데이터, 클라우드", "tags": "", "date": "2022-06-05 00:00:00 +0900", "snippet": "도커 컨테이너 전체 삭제$docker container rm -f $(docker ps -aq)도커 이미지 전체 삭제$docker image rm -f $(docker images -q)관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "tmux 명령어, 단축키", "url": "/posts/tmux-%EB%AA%85%EB%A0%B9%EC%96%B4-%EB%8B%A8%EC%B6%95%ED%82%A4/", "categories": "1. 기술, 서버, 데이터, 클라우드", "tags": "", "date": "2022-06-04 00:00:00 +0900", "snippet": " 설치$brew install tmux 세션 시작$tmux new -s session_name 실행중인 세션 리스트$tmux list 실행중인 세션 연결$tmux attach -t session_number (or session_name) 화면 분할(위/아래로)(ctrl + b) “ 화면 분할(왼쪽/오른쪽으로)(ctrl + b) % 분할 화면간 이동(ctrl + b) o 세션 유지하고 나가기(ctrl + b) d관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "git 원격 저장소 변경", "url": "/posts/git-%EC%9B%90%EA%B2%A9-%EC%A0%80%EC%9E%A5%EC%86%8C-%EB%B3%80%EA%B2%BD/", "categories": "1. 기술, 서버, 데이터, 클라우드", "tags": "", "date": "2022-06-04 00:00:00 +0900", "snippet": "기존 git의 커밋을 유지한 채 원격 저장소 위치만 변경git remote set-url –push origin new_repo_name단, 이 때 새 저장소에 뭔가 커밋된 이력이 있으면 pull 해서 저장소와 맞춘 다음 push 하거나강제로 덮어써야한다.(force option 사용)git push -f origin main관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "pandas - reset_index()", "url": "/posts/pandas-reset-index/", "categories": "3. 튜토리얼, 금융 분석 프로그래밍 응용", "tags": "", "date": "2022-06-03 00:00:00 +0900", "snippet": "reset_index(drop=True)자꾸 까먹어서 적어놓는다. 데이터전처리가 끝난 후, 혹은 중간에 데이터 정리가 필요한 경우 reset_index()는 유용하게 사용된다. reset_index() 는 마치 엑셀, 구글스프레드시트의 고정된 index 번호처럼 0부터 순서대로 index를 정리해주는 함수다.아래와 같은 과일의 특징을 정리한 데이터프레임이 있다고 해보자.print(df) x1 x2 x3 x4Apple 1 1 0 0Banana 1 0 1 1 Kiwi 0 1 1 0reset_index() 함수를 사용해보면, 인덱스가 다시 정리되고 기존의 index는 새로운 ‘열’로 추가된다.print(df.reset_index()) index x1 x2 x3 x40 Apple 1 1 0 01 Banana 1 0 1 1 2 Kiwi 0 1 1 0그러나 보통은 이러한 형태가 아닌 index를 ‘정리’하거나 새로운 index로 ‘변환’하기 위해 reset_index()를 사용하게 된다.이 때는 파라미터 값으로 drop=True를 주면 된다.print(df.reset_index(drop=True)) x1 x2 x3 x40 1 1 0 01 1 0 1 1 2 0 1 1 0여기서 데이터 원본값을 수정하고 싶다면 ìnplace=True를 추가로 넣어준다.print(df.reset_index(drop=True, inplace=True)) x1 x2 x3 x40 1 1 0 01 1 0 1 1 2 0 1 1 0관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "Elastic Stack 활용 서울시 상권 매출 대시보드 구축 - (1) elastic search, kibana", "url": "/posts/Elastic-Stack-%ED%99%9C%EC%9A%A9-%EC%84%9C%EC%9A%B8%EC%8B%9C-%EC%83%81%EA%B6%8C-%EB%A7%A4%EC%B6%9C-%EB%8C%80%EC%8B%9C%EB%B3%B4%EB%93%9C-%EA%B5%AC%EC%B6%95-1-elastic-s/", "categories": "1. 기술, 서버, 데이터, 클라우드", "tags": "", "date": "2022-06-03 00:00:00 +0900", "snippet": "데이터 파이프라인 구축 (1)에서 csv 파일의 텍스트 전처리가 필요했다. 해당 부분부터 이어서 kibana 대시보드 작업까지 진행한다.지난 글에서 작성했지만, 작업환경과 파이프라인 flow도 다시 보자.””“작업 환경: Macbook Pro 2019 - i9, 16GB GCP 가상 머신 4대 할당(e2-small 3대 + e2-medium 1대) 가상 머신 1대에 클러스터 1개 배치(클러스터당 노드도 1개씩) filebeat 설치(Local) logstash 설치(Local) elastic search 설치(e2-small 3개 각각 설치) kibana 설치(e2-medium 1대에 설치)데이터:서울시 상권 추정 매출(서울시 열린 데이터 광장 제공)구축할 데이터 파이프라인 Flow: Beats : 데이터 수집 Logstash : 데이터 전처리(필터링) Elasticsearch : 데이터 저장 및 관리 Kibana : 데이터 시각화””“Step 6. (이어서) csv 파일 수정하기먼저, csv의 큰 따옴표 제거한다. 시스템상 특수문자마다 역슬래시 처리가 된 채로 전달되어야 텍스트를 정상적으로 읽을 수 있기 때문에 beats가 역슬래시를 포함한 텍스트를 logstash의 input값으로 전달하게 된다. 따라서 역슬래시가 포함되지 않도록 vim을 사용해 특수문자(“)를 지워준다.특수문자 포함특수문자 제거Step 7.  Logstash filter 추가하기 - 데이터 타입 지정다음으로, int형을 지정해준다. default로 문자열 형태로 입력되는데 이러면 대소 비교가 되지 않는다. 따라서 logstash filter를 활용해 int 변환 후 elastic search로 보내준다.Step 8.  Kibana Index pattern 설정하기이제 키바나 대시보드에 올린건데, 그전에 Index pattern을 생성해줘야 한다. 카바나에서 각각의 field와 해당 값을 인식하게 해 준다.Index pattern을 정상적으로 설정했다면 Discover 탭에서 timestamp에 따라 데이터가 쌓이는 모습을 볼 수 있다. 지금은 데이터가 약 12만 건이고, 5월 30일 기준으로 한 번에 넣었기 때문에 이렇게 데이터가 몰려있다.Step 9.  Kibana Dashboard 생성하기이제 Dashboard 탭에 들어가서 맘껏 패널을 생성하면 된다. 대시보드 내 각각의 화면 요소가 패널이고, 패널은 Create panel 버튼을 통해 생성할 수 있다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "sklearn - numeric encoding", "url": "/posts/sklearn-numeric-encoding/", "categories": "1. 기술, 머신러닝, 딥러닝", "tags": "", "date": "2022-06-02 00:00:00 +0900", "snippet": "Encoder LabelEncoder OneHotEncoder LabelBinarizer *reference : kaggle, Heart Attack Prediction_95.4% accuracy, Avinash Bagul* 머신러닝 프로젝트 실행-3, cyan91 1. LabelEncoder object 타입의 label을 숫자형으로 변환 머신러닝 모델은 모든 컬럼의 값을 숫자형으로 통일시켜줘야 하는데(혹은 categorical column은 제거해야 하는데) 이는 확률 모델이기 때문에 그렇다(+수식 계산이 가능해야 미분을 통한 학습이 이루어질 수 있다) 예시 코드from sklearn.preprocessing import LabelEncoderle = LabelEncoder()X_encoded = le.fit_transform(X)X_encoded # 출력 출력array([1,5,3,5...,0,0,3]) 0~5 사이의 값으로 endcoding 되었다고 가정. 컴퓨터는(머신러닝 모델은) 0과 1, 1과 2의 연관성을 0과 5보다 더 크게 인지한다. 따라서 우리가 의도한(그러한 연관성이 있는) 것이 아니라면 LabelEncoder는 사용하기에 적절하지 않다.2. OneHotEncoder 모든 범주를 0 혹은 1로 변환 예를 들면, 개와 고양이를 1과 2로 변환하는 것이 아니라 해당 실험군이 ‘개’라면 ‘개’ column에 1을, ‘고양이’ column에 0을 넣는다. 예시 코드from sklearn.preprocessing import OneHotEncoderoh = OneHotEncoder()X_encoded = oh.fit_transform(X)X_encoded # 출력 출력&amp;lt;16513x5 sparse matrix of type &#39;&amp;lt;class &#39;numpy.float64&#39;&amp;gt;&#39; with 16513 stored elements in Compressed Sparse Row Format&amp;gt; Sparse Row Format으로 저장되었다고 출력한다. 이는 메모리 효율성을 위한 것인데, 원핫인코딩은 모든 값을 0과 1로 넣기 위해 매트릭스가 방대하게 확장되기 때문에(+모든 값은 0 혹은 1로 저장되어 있으므로) 메모리에 0이 아닌 요소들(1)의 위치만 저장하도록 하는 방식이다. 그런데, 원핫인코딩은 애초에 숫자형 범주를 입력값으로 받기 때문에 텍스트 범주를 LabelEncoder에 먼저 통과시켜야 한다는 번거로운 점이 있다.3. LabelBinarizer 컬럼 내 모든 텍스트 값을 숫자형으로 변환 + 숫자형을 원핫벡터로 변환 예시 코드from sklearn.preprocessing import LabelBinarizer # sklearn.preprocessing에서 LabelBinarizer 패키지 사용lb = LabelBinarizer() # LabelBinarizer() 할당X_encoded = lb.fit_transform(X) X_encoded 출력array([[0,1,0,0,0], [0,1,0,1,0], ... [1,0,0,1,1], [1,0,0,0,0], [0,0,1,0,1]]) LabelBinarizer()는 Default로 numpy 배열을 즉시 할당한다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "딥러닝 Multiple-Object Detection", "url": "/posts/%EB%94%A5%EB%9F%AC%EB%8B%9D-Multiple-Object-Detection/", "categories": "3. 튜토리얼, 금융 분석 프로그래밍 응용", "tags": "", "date": "2022-06-01 00:00:00 +0900", "snippet": "Multiple-Object Detection - Type Areference : Multiple-Object Detection (1) : R-CNN Multiple-Object Detection (2) : SPP-Net Multiple-Object Detection (3) : Fast R-CNN 갈아먹는 Object Detection [1] R-CNN 갈아먹는 Object Detection [2] Spatial Pyramid Pooling Network 갈아먹는 Object Detection [3] Fast R-CNN 갈아먹는 Object Detection [4] Faster R-CNNMultiple Object Detection의 동작 방식은 Region Proposal과 Classification을 별도로 수행하는 2 Step 방식과 한번에 수행하는 1 Step 방식으로 크게 나뉜다.여기서는 2 Step 방식을 정리한다. 발전 과정은 아래와 같다. R-CNN -&amp;gt; SPP-Net -&amp;gt; Fast R-CNN -&amp;gt; Faster R-CNNR-CNN1. Region Proposala. selective search 과정을 통해 image segmentation을 수행한다. b. image segmentation을 통해 object detection(multiple object, 약 2000개의 box)을 수행한다. c. 2000개의 box들은 227x227로 resize(warp)2. Feature Extractiona. 227x227 box들이 CNN(Image Classification, imagenet) 모델을 통과한다. b. CNN 모델은 4096 size vector Output을 돌려준다. c. 최종 레이어의 Output은 Object Detection Class 수 +1(배경)4. Non-Maximum Suppression2000개 박스 중 불필요한(스코어가 낮은) 박스는 모두 제거하는 과정.동일한 물체에 여러 박스가 표시될 수 있는데, 이 때 그 물제를 가장 잘 설명하는 박스만 남긴다.5. Bounding Box RegressionSelective Search를 통해 찾은 박스 위치는 정확하지 않기 때문에 Linear Regression 모델 학습을 통해 위치를 교정한다.SPP-Net기존 CNN 모델은 Input size가 (224,224) 등으로(설계된대로) 고정되어야 했음. 즉, 이미지 원본을 모델로 직접 통과시키기 어려웠다.Input size가 고정되는 이유는 fc 레이어 때문(Conv 레이어는 Input size를 자유롭게 받을 수 있음).SPP(Spatial Pyramid Pooling)는 Conv 레이어와 fc 레이어의 중간에서 fc 레이어가 받아야 하는 고정된 input size로 vector 크기를 변환시켜주는 역할을 한다.1. CNNConv layer에서 selective search를 수행한다.2. SPPselective search 결과를 고정된 크기로 반환시켜준다. 즉, 고정된 크기의 feature vector를 추출한다.3. Fully Connected Layerfeature vector를 1차원 벡터로 펼친다.4. SVM(Support Vector Machine)이미지 클래스별로 이진 분류를 수행한다.5. Bounding Box RegressorLinear Regression 모델을 통해 box 결과값을 더 정교하게 교정한다.Fast R-CNNFeature extraction(CNN 수행), Classification, Bounding Box Regression까지 모두 하나의 모델에서 학습시키는 컨셉1. CNN전체 이미지를 Pre-trained CNN에 통과시켜 selective search과정을 통해 Feature Map을 추출한다.2. RoI Poolingselective search를 통해 찾은 각각의 RoI에 대해 RoI Pooling을 수행하여 고정된 크기의 feature vector를 추출한다.3. Fully Connected LayerRoI Pooling을 통해 얻은 feature vector를 Fully Connected Layer에 통과시켜 2개의 브런치로 나눈다.4-1. Branch (1) - Softmax1개 브런치는 Softmax 결과값으로 Classification을 수행한다.4-2. Branch (2) - Bounding Box Regression다른 1개 브런치는 selective search로 찾은 박스의 위치를 더 정확하게 조정한다.Faster R-CNNRegion Proposal Network를 사용한 방식이다. 즉, 기존의 Network 앞단에서 RoI를 계산하던 selective search를 Network 안으로 끌고와서 수행하는 end-to-end 모델이다. Network 안에서 RoI 계산을 수행하므로 모든 모델이 GPU 위에서 돌아갈 수 있는 환경이다.1. CNN전체 이미지를 입력받아 Feature Map을 추출한다.2. Region Proposal Network(RPN) - intermediate layerCNN에서 추출된 Feature Map을 (height, width, channel)로 받아 3x3 Conv 레이어를 통과시키고 padding을 1(same)로 설정하여 (height, width)를 보존한다. 그 결과 (height, width, 256) 또는 (height, width, 512) 형태의 두 번째 Feature Map을 반환한다.3. Fully Convolution Network두 번째 Feature Map을 입력받아 2개의 1x1 Conv 레이어를 통과시킨다. 하나는 Classification, 다른 하나는 Bounding Box Regression 목적이다.4-1. Branch (1) - Classification x softmax채널은 18(2x9)로 받아서(1x1 Conv) (height, width, 18) 크기의 Feature Map을 반환한다. 이 값을 reshape 하여 softmax를 적용한다.4-2. Branch (2) - Bounding Box Regression채널은 36(4x9)로 받아서(1x1 Conv) Regression을 수행한다.5. Non-Maximum-SuppressionClassification으로 얻은 확률 값을 정렬 후, 높은 순으로 K개의 앵커를 추려내고 Bounding Box Regression을 수행한다. 최종적으로 Non-Maximum-Suppression을 통해 RoI를 구한다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "git - branch", "url": "/posts/git-branch/", "categories": "1. 기술, 서버, 데이터, 클라우드", "tags": "", "date": "2022-06-01 00:00:00 +0900", "snippet": "terminal에서 폴더를 돌아다니다보면 git의 branch가 변하는 것을 볼 수 있다. remote 할 때 origin을 어느 브런치로 지정하였는지에 따라 현재(default) branch 이름이 바뀐다.TIA_main [main] git status현재 브랜치 main브랜치가 &#39;origin/main&#39;에 맞게 업데이트된 상태입니다.커밋할 사항 없음, 작업 폴더 깨끗함TIA_main [main] cd ..TIA [master] git status현재 브랜치 master브랜치가 &#39;origin/main&#39;에 맞게 업데이트된 상태입니다.커밋할 사항 없음, 작업 폴더 깨끗함현재 브런치와 Github Repository의 브런치가 일치하지 않는다면 git push가 불가능하다. 이 때는 아래와 같이 브런치를 변경해주어야 한다.TIA [master] git checkout mainTIA [main]&#39;master&#39; 브랜치가 리모트의 &#39;master&#39; 브랜치를 (&#39;origin&#39;에서) 따라가도록 설정되었습니다.새로 만든 &#39;master&#39; 브랜치로 전환합니다checkout이라는 표현 때문에 조금 헷갈릴 수 있다. 현재 브런치 master를 checkout -&amp;gt; main으로 떠난다고 생각하면 이해가 쉽다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "Elastic Stack 활용 서울시 상권 매출 대시보드 구축 - (1) beats, logstash", "url": "/posts/Elastic-Stack-%ED%99%9C%EC%9A%A9-%EC%84%9C%EC%9A%B8%EC%8B%9C-%EC%83%81%EA%B6%8C-%EB%A7%A4%EC%B6%9C-%EB%8C%80%EC%8B%9C%EB%B3%B4%EB%93%9C-%EA%B5%AC%EC%B6%95-1-beats-lo/", "categories": "1. 기술, 서버, 데이터, 클라우드", "tags": "", "date": "2022-06-01 00:00:00 +0900", "snippet": "Step 0. 실습 전 확인 사항Elastic Stack은 Elastic Search를 중심으로 Beats, Logstash, Kibana를 활용해 데이터파이프를 구축하는 일련의 설계 방식이다. 이번 실습은 아래의 순서로 진행된다. 서울시 상권 추정 매출 데이터를 Beats로 읽어들여서, Logstash로 보내 문자열 처리를 거친 다음, Elastic search로 적재하고, Kibana로 대시보드를 그려보는 것먼저, 실습 전에 나의 작업 환경은 아래와 같다. 동일하게 세팅할 필요는 없으며, 클라우드를 사용하지 않고 개인 로컬 환경에서도 충분히 실습이 가능하다.작업 환경: Macbook Pro 2019 - i9, 16GB GCP 가상 머신 4대 할당(e2-small 3대 + e2-medium 1대) 가상 머신 1대에 클러스터 1개 배치(클러스터당 노드도 1개씩) filebeat 설치(Local) logstash 설치(Local) elastic search 설치(e2-small 3개 각각 설치) kibana 설치(e2-medium 1대에 설치)그리고. 우리가 사용할 데이터는 다음과 같다.데이터:서울시 상권 추정 매출(서울시 열린 데이터 광장 제공)Elastic Stack의 각 구성요소별 역할을 다시 확인하고 넘어가자.구축할 데이터 파이프라인: Beats : 데이터 수집 Logstash : 데이터 전처리(필터링) Elasticsearch : 데이터 저장 및 관리 Kibana : 데이터 시각화나의 작업 환경도 참고할 수 있도록 캡쳐해보았다.GCP VM에 접속한 모습 :Step 1. beats -&amp;gt; logstash 연결하기beats를 정상적으로 설치했다면, filebeat.yml 파일이 있을 것이다.파일을 vim이나 다른 텍스트 에디터로 열어서 output.logstash 부분의 hosts가 logstash를 가리키도록 수정해주자. logstash의 포트 번호를 따로 건드리지 않았다면 기본적으로 5044번으로 설정되어 있다. 그리고 logstash 폴더 내에 conf 파일을 생성해서 input과 output을 아래와 같이 설정해주자. input으로는 beats가 5044번 포트로 보내줄 데이터를 받고, output으로는 기본 출력(stdout)과 elastic search 2개 방향으로 보내준다.stdout 부분의  codec 은 출력 형태를 말한다. rubydebug는 json 형태로 출력하라는 의미다. elasticsearch 부분의  host는  beats 설정과 마찬가지로 elasticsearch의 외부 호스팅 주소와 포트 번호를 설정해주면 된다. 내 경우는 gcp 가상 머신을 사용하기 때문에 해당 머신의 주소를 설정해줬다. index는 elasticsearch 자료구조에서 해당 문서를 조회할 수 있도록 하는 이름이다. 데이터 파일 이름이라 생각하면 된다.모두 설정했다면 “$/bin/logstash -f conf파일명” 명령을 통해 logstash를 실행해주자.logstash가 잘 실행되었다면 위와 같이 Pipeline running과 함께 5044 포트번호가 표시된다. 이제 beats와 elasticsearch의 중간다리를 세운 것이다.Step 2. raw data -&amp;gt; Beats 연결하기이제 다시 filebeat.yml 파일을 열어서 filebeat.inputs를 설정해준다. logstash의 input이 beats였다면,  beats의 input은 원본 데이터이다. 우리는 실시간 데이터를 사용하지 않기 때문에 데이터가 들어 있는 파일 경로를 설정해주면 된다.####Step 3. raw data -&amp;gt; Beats -&amp;gt; Logstash -&amp;gt; Stdout 출력 확인하기다음으로 beats를 설치한 루트 폴더에서 “$./filebeat -e -c filebeat.yml” 명령을 실행하면 아래와 같이 logstash로 데이터가 들어가고, 데이터 로그가 stdout으로 출력된다.beats가 파일 읽어서 데이터 전송하는 모습 :그런데 csv 파일을 한 줄씩 읽어서 전송하다보니 컬럼 명과 매치되지 않은 원천 데이터가 그대로 입력된다.####Step  4. Elastic Search 검색하기이렇게 입력되면 elastic search로 “일반의원”이라 검색했을 때, 매출 정보를 파악할 수 없다. 아래 화면은 kibana를 실행하면 띄울 수 있는 브라우저다. 여기서 간단하게 elastic search 명령어를 사용해볼 수 있다. 브라우저에 접속해 Dev Tools에 들어가서 GET index/\\_search 쿼리를 날려주면 되는데, message에 입력한 키워드와 매칭되는 텍스트를 불러올 수 있다.Step 5. Logstash filter 설정하고 파이프라인 다시 세우기무튼, 이렇게 역슬래시도 들어가고 제대로 데이터 분리가 되지 않은 원본 형태이기 때문에 데이터 소스를 그대로 사용하기에는 적절하지 않은 것 같다. elastic search나 kibana를 사용할 때에는 최대한 메모리 자원을 아껴서 방대한 데이터를 효율적으로 활용하는 것이 좋다. 따라서 Logstash에 input, output 사이에 filter를 설정해준다.다시 beats를 실행해 데이터를 입력한다. 이때, stdout codec은 dots로 설정해서 데이터가 들어가는 상황만 모니터링할 수 있도록 한다.이제 다시 elastic search로 입력된 데이터를 확인해보자.데이터는 잘 분리되어 들어가는데, 좀 이상하다. 역슬래시도 여전히 포함되어 있고, 전부 string으로 입력됐다. 키바나로 모니터링하려면 매출 관련 수치는 모두 정수 혹은 실수형으로 입력되어야 하기 때문에 해당 부분도 처리해야 한다. (다음에 이어서..)관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "Docker 명령어 모음", "url": "/posts/Docker-%EB%AA%85%EB%A0%B9%EC%96%B4-%EB%AA%A8%EC%9D%8C/", "categories": "1. 기술, 서버, 데이터, 클라우드", "tags": "", "date": "2022-06-01 00:00:00 +0900", "snippet": "reference : 도커(Docker) 입문편 [Docker]Docker File을 이용하여 Docker Image 만들기 vi/vim 편집기 명령어 정리 (단축키 모음/목록) 위 레퍼런스의 예시 명령어 요약도커 저장소로부터 centos:latest image를 받아와 아래 기본 명령어들을 경험해본다. docker run -it centos:latest bash docker ps docker ps -a docker restart docker attachubuntu:bionic image를 받아와 git을 설치해보고, 아래 명령어들을 통해 버전관리를 경험해본다. docker pull ubuntu:bionic docker run -it ubuntu:bionic bash /# git –version docker ps /# apt update /# apt install -y git /# git –version docker diff process_id  head docker run -it –rm ubuntu:bionic bash /# git –version /# exit docker commit process(container)_id ubuntu:git docker run -i -t ubuntu:git bash /# git –version /# exit docker rm process(container)_id docker rmi image_idDockerfile을 통해 도커 이미지를 만들어본다. midir git-from-dockerfile cd git-from-dockerfile vim Dockerfile ~ FROM ubuntu:bionic ~ RUN apt-get update ~ RUN apt-get install -y git docker built -t ubuntu:git-from-dockerfile . docker run -it ubuntu:git-from-dockerfile bash /# git –version관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "linux - 파일 인코딩 변경", "url": "/posts/linux-%ED%8C%8C%EC%9D%BC-%EC%9D%B8%EC%BD%94%EB%94%A9-%EB%B3%80%EA%B2%BD/", "categories": "1. 기술, 서버, 데이터, 클라우드", "tags": "", "date": "2022-05-31 00:00:00 +0900", "snippet": "로컬 서버에서 지정된 인코딩 정보와 파일의 인코딩 정보가 일치하지 않는 경우 terminal에서 해당 파일을 읽을 때 아래와 같이 한글이 깨질 수 있다.해당 파일의 인코딩 정보는 $file -I filename 으로 확인할 수 있다.(윈도우는 $file -bi filename)해당 파일의 charset은 iso-8859-1이다. 일반적으로 맥 서버의 한글 인코딩 설정은 utf-8로 되어있다.이렇게 인코딩 정보가 다르면 한글을 정상적으로 읽을 수 없기 때문에 vim으로 파일을 열어서 인코딩 형식을 변경해준다.그러고나서 다시 인코딩 정보를 확인하면 utf-8로 변경된 것을 확인할 수 있고, 한글도 정상적으로 출력된다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "Kibana - 모니터링 멈추기", "url": "/posts/Kibana-%EB%AA%A8%EB%8B%88%ED%84%B0%EB%A7%81-%EB%A9%88%EC%B6%94%EA%B8%B0/", "categories": "1. 기술, 서버, 데이터, 클라우드", "tags": "", "date": "2022-05-31 00:00:00 +0900", "snippet": "키바나를 elastic search와 연동하여 실행시 default로 실시간 데이터가 적재된다. 만약 테스트를 위한 작은 VM을 띄우고 실습한다면, 용량에 부담을 주고 ssh 서버 접속이 제한될 수 있으니 경우에 따라 제한할 필요가 있다. 실행시 데이터 적재되는 모습메모리 용량 조절을 위해 모니터링하지 않을때는 데이터가 쌓이지 않도록 Dev-Tools 탭에서 elastic search 명령으로 초기화할 수 있다._cluster/settings에 persistent -&amp;gt; null명령어를 실행해주면 아래와 같이 데이터 수집이 중단된다(붉은 선 기점으로 중단)관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "GCP - VM 이미지 생성, 인스턴스 복제", "url": "/posts/GCP-VM-%EC%9D%B4%EB%AF%B8%EC%A7%80-%EC%83%9D%EC%84%B1-%EC%9D%B8%EC%8A%A4%ED%84%B4%EC%8A%A4-%EB%B3%B5%EC%A0%9C/", "categories": "1. 기술, 서버, 데이터, 클라우드", "tags": "", "date": "2022-05-31 00:00:00 +0900", "snippet": "가상머신 Copy하는법(이미지생성)**VM 인스턴스 탭 -&amp;gt;**이미지 생성 -&amp;gt; 생성된 이미지로 새로운 인스턴스 생성**관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "Elastic Search - 외부 호스팅 개방", "url": "/posts/Elastic-Search-%EC%99%B8%EB%B6%80-%ED%98%B8%EC%8A%A4%ED%8C%85-%EA%B0%9C%EB%B0%A9/", "categories": "1. 기술, 서버, 데이터, 클라우드", "tags": "", "date": "2022-05-31 00:00:00 +0900", "snippet": "외부에서 엘라스틱서치 클러스터에 접속하기 위해서는 config/elasticsearch.yml 파일에서 network.host: “_stie_” (혹은 internet-ip) 를 입력해줘야 한다.그런데, 이렇게 수정하고 실행하면 부트스트랩 에러가 발생한다.이 부분을 해결하려면 /etc/security/limits.conf 파일과 /etc/sysctl.conf 파일 수정 통해 리소스 제한을 영구적으로 풀어줘야한다.수정 내용은 아래 이미지 참고)수정했다면, 리눅스 시스템을 다시 올린다.$sudo shutdown -r추가로, config 파일에 discovery.seed_hosts로 호스트 설정도 해줘야 한다.이렇게 외부 호스트를 열고 실행시, 내부에서는 이제 localhost로는 접속이 안되고, 내부 IP로 접속해하는데, 만약 localhost로도 접속하고싶으면 아래와 같이 배열로  “_local_” 추가한다.curl 명령어로 호출해보면 정상적으로 연결이 된다.내부가 아닌 외부에서는 당연하게도 외부 IP로 접속해야하며 이를 위해 방화벽을 열어준다.좌- 내부접속, 우-외부접속관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "이진 분류를 위한 머신러닝 예측 성능 비교 - 로지스틱 회귀, 랜덤포레스트, 부스팅, 신경망", "url": "/posts/%EC%9D%B4%EC%A7%84-%EB%B6%84%EB%A5%98%EB%A5%BC-%EC%9C%84%ED%95%9C-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%98%88%EC%B8%A1-%EC%84%B1%EB%8A%A5-%EB%B9%84%EA%B5%90-%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1-%ED%9A%8C%EA%B7%80-%EB%9E%9C%EB%8D%A4%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8-%EB%B6%80%EC%8A%A4%ED%8C%85-%EC%8B%A0%EA%B2%BD/", "categories": "3. 튜토리얼, 금융 분석 프로그래밍 응용", "tags": "", "date": "2022-02-08 00:00:00 +0900", "snippet": "딥러닝 신경망 모형이 언제나 이길까?본 분석은 머신러닝 모델의 예측 성능을 비교함으로써 딥러닝(다층 신경망)이 언제나 만능일 수 없음을 검증하고자 한다. 물론 신경망의 경우 Hyper-parameter 튜닝 및 딥러닝에 최적화된 Feature Engineering을 통해 미세한 성능 개선이 가능하지만, 단순 이진 분류의 경우에 딥러닝보다 빠른 속도와 우수한 성능을 보여주는 가벼운 머신러닝 모델을 쉽게 찾을 수 있음을 보여주기 위함이다.그럼, 자연스럽게 가설을 하나 설정해두고 분석을 진행하도록 하겠다.1. 가설 설정 및 데이터 분포 확인 귀무가설 : 동일한 데이터로 동일한 정규화 과정을 거쳤을 때 머신러닝 모델 중 신경망 알고리즘의 성능이 가장 우수하다. 연구가설 : 동일한 데이터로 동일한 정규화 과정을 거쳤을 때 머신러닝 모델 중 신경망 알고리즘의 성능이 가장 우수한 것은 아니다.데이터는 UCI Machine Learning Repository의 Adult Data Set을 사용한다. 해당 데이터는 소득 예측을 위한 정보를 담고 있으며, 연 $ 50K 이하 혹은 초과 케이스에 대한 label(y)를 함께 제공하고 있다.각 컬럼은 범주형, 수치형 데이터가 섞여 있으며 각각의 분포는 아래와 같다.Categorical ColumnsNumerical Columns2. 데이터 전처리 과정 확인머신러닝의 경우 모든 학습 데이터가 수치로 입력되어야 하므로 One-Hot-Encoding 방식을 사용해 범주형 데이터를 모두 변환해주었고,  이때 데이터가 희소해지는 현상(One-Hot-Encoding시 데이터는 0과 1로 채워지며 0이 1에 비해 많은 공간을 차지하므로 머신러닝의 학습이 원활하지 못하게 됨)에 대응하여 차원 축소(PCA, Principal Component Analysis)도 함께 수행해주었다. 과정마다의 결과물을 확인하고 넘어가자.원본 데이터(범주형 데이터)One-Hot-Encoding 수행 결과(8개 컬럼은 99개 컬럼으로 증폭되었고, 대부분 값이 0으로 입력됨) - 본 이미지에서는 9개 컬럼만 표시PCA 수행 결과(99개 컬럼을 3개 컬럼으로 줄였고, 희소한 벡터를 유의미한 수치 벡터로 변환함) - *train/test set 분리로 인해 rows 갯수가 다름이렇게 범주형 변수를 수치화한 다음 머신러닝 모델에 바로 입력하지 않고 정규화를 거치는 것이 좋다. 데이터의 평균 및 분산이 컬럼마다 다르게 되면 수치상 평균이 큰 특정 칼럼에 대한 영향이 다른 칼럼들의 분포를 상쇄시키기 때문이다. 범주형, 수치형 변수를 모두 정규화(평균 0, 표준편차 1)하고, 분포를 확인하면 다음과 같다.train set의 분포지만 test set도 거의 유사한 분포를 띈다. 초록색 기준선이 평균을 가리키고 있다.3. 모델간 예측 성능 비교해당 데이터에 대해 총 5개의 머신러닝 모델로 예측을 수행했다.3-1. Logistic Regression precision recall f1-score support 0 0.87 0.93 0.90 6171 1 0.72 0.56 0.63 1970 accuracy 0.84 8141 macro avg 0.79 0.74 0.76 8141weighted avg 0.83 0.84 0.83 8141---Precision- 연소득 $50,000 이하로 예측한 케이스의 87% 일치- 연소득 $50,000 초과로 예측한 케이스의 72% 일치Recall- 실제 연소득 $50,000 이하 케이스 중 93% 정상 분류- 실제 연소득 $50,000 초과 케이스 중 56% 정상 분류데이터 불균형으로 연소득 $50,000에 대한 예측 성능이 더 우수3-2. Random Forest precision recall f1-score support 0 0.89 0.93 0.91 6171 1 0.74 0.62 0.68 1970 accuracy 0.86 8141 macro avg 0.81 0.78 0.79 8141weighted avg 0.85 0.86 0.85 8141---Precision- 연소득 $50,000 이하로 예측한 케이스의 89% 일치- 연소득 $50,000 초과로 예측한 케이스의 74% 일치Recall- 실제 연소득 $50,000 이하 케이스 중 93% 정상 분류- 실제 연소득 $50,000 초과 케이스 중 62% 정상 분류마찬가지로 데이터 불균형으로 연소득 $50,000에 대한 예측 성능이 더 우수하지만 Linear Model보다 부족한 데이터셋에 대한 예측력이 좋음3-3. XGBoost precision recall f1-score support 0 0.89 0.94 0.91 6171 1 0.76 0.65 0.70 1970 accuracy 0.87 8141 macro avg 0.83 0.79 0.81 8141weighted avg 0.86 0.87 0.86 8141---Precision- 연소득 $50,000 이하로 예측한 케이스의 89% 일치- 연소득 $50,000 초과로 예측한 케이스의 76% 일치Recall- 실제 연소득 $50,000 이하 케이스 중 94% 정상 분류- 실제 연소득 $50,000 초과 케이스 중 65% 정상 분류기본 배깅 방식에서 개선된 학습 알고리즘인 랜덤포레스트보다도 부스팅 알고리즘이 성능이 좋았음.3-4. LightGBM precision recall f1-score support 0 0.89 0.94 0.91 6171 1 0.76 0.64 0.70 1970 accuracy 0.87 8141 macro avg 0.83 0.79 0.81 8141weighted avg 0.86 0.87 0.86 8141---Precision- 연소득 $50,000 이하로 예측한 케이스의 89% 일치- 연소득 $50,000 초과로 예측한 케이스의 76% 일치Recall- 실제 연소득 $50,000 이하 케이스 중 94% 정상 분류- 실제 연소득 $50,000 초과 케이스 중 64% 정상 분류위에서 확인한 XGBoost와 매우 유사한 수치 확인. 하이퍼파라미터를 사용하지 않는다면 Boosting 계열간 큰 차이는 볼 수 없음3-5. Deep Learning precision recall f1-score support 0 0.89 0.91 0.90 6171 1 0.70 0.63 0.67 1970 accuracy 0.85 8141 macro avg 0.79 0.77 0.78 8141weighted avg 0.84 0.85 0.84 8141---Precision- 연소득 $50,000 이하로 예측한 케이스의 89% 일치- 연소득 $50,000 초과로 예측한 케이스의 70% 일치Recall- 실제 연소득 $50,000 이하 케이스 중 91% 정상 분류- 실제 연소득 $50,000 초과 케이스 중 63% 정상 분류Boosting 계열보다 정확도는 떨어지며 Logistic Regression보다 불균형 데이터셋을 잘 반영한다는 점에서 유리하지만, 큰 차이는 없었다.4. 결론모델간 성능 비교 결과 Gradient Boosting 계열의 알고리즘(XGBost, LightGBM)의 예측 정확도가 가장 좋았고, 속도를 고려했을 때, LightGBM이 가장 우수했다. 따라서 귀무가설을 기각하고 연구가설(동일한 데이터로 동일한 정규화 과정을 거쳤을 때 머신러닝 모델 중 신경망 알고리즘의 성능이 가장 우수한 것은 아니다)을 채택한다.실제로 캐글 대회에서도 많은 분석가들이 부스팅 계열 알고리즘으로 빠르고 정확한 모델을 구현해내고 있고, 특히 모델의 연산 속도가 중요한 금융이나 보안 등의 주요 산업 분야에서 부스팅 모델을 선호하는 경향이 있다. 산업 현장에서는 엔지니어의 회고, 피드백 과정을 통한 책임이 중요하기 때문에 모델의 해석 가능성과 설명력이 모델 선택의 주요 기준 중 하나다. 이러한 점에서도 신경망 모형보다 부스팅 계열 알고리즘 혹은 랜덤 포레스트 같은 트리 모형이 신경망 보다 유리한 부분이 있다.요즘은 여러 모델을 함께 비교 분석하는 AutoML 패키지가 잘 나와있기 때문에 무조건 딥러닝을 사용하는 것보다 다양한 모델을 비교분석하면서 장단을 따져보는 것이 좋겠다.👉 분석 및 모델링에 사용된 파이썬 코드(주피터 노트북) 링크관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "과거 주가 데이터로 미래 주가를 예측할 수 있을까? - 파이썬 시계열 패턴 검색", "url": "/posts/%EA%B3%BC%EA%B1%B0-%EC%A3%BC%EA%B0%80-%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%A1%9C-%EB%AF%B8%EB%9E%98-%EC%A3%BC%EA%B0%80%EB%A5%BC-%EC%98%88%EC%B8%A1%ED%95%A0-%EC%88%98-%EC%9E%88%EC%9D%84%EA%B9%8C-%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EC%8B%9C%EA%B3%84%EC%97%B4-%ED%8C%A8%ED%84%B4-%EA%B2%80%EC%83%89/", "categories": "3. 튜토리얼, 시계열 예측 및 계량 분석 방법론", "tags": "", "date": "2021-09-16 00:00:00 +0900", "snippet": "Step 0. 패턴 검색이번에는 현재 차트와 유사한 차트를 찾아서 매매에 활용하는 패턴 검색을 파이썬으로 구현해보자. 패턴 검색은 과거 주가에서 현재 주가와 유사한 패턴을 관측한 다음, 과거 주가 이후의 흐름을 확인하는 것을 통해 앞으로의 주가를 예상해보는 방식이다.Step 1. 코스피 종가 가져오기2010년부터 현재까지의 데이터를 스크리닝한다. 즉, 유사한 패턴을 관측하는 가장 먼 시점은 2010년이다.Step 2. 기준 구간 확인종가만 추출해서 검색하고자 하는 현재 주가의 패턴을 확인한다. 여기서 현재 주가는 6월 2일까지만 확인한다. 마지막에 6월 2일 이후 주가를 그려보면서 패턴 검색 방식의 예측을 검증해보기 위함이다.코스피는 올해 1월에 급락 후 횡보하는 모습을 보인다. 이와 유사한 패턴을 과거에서 찾아본다.​Step 3. 패턴 검색 구현패턴 검색은 다음과 같은 로직으로 구성했다. 먼저 앞에서 지정한 기준 구간의 데이터가 있을 것이다. 해당 데이터 길이만큼 최초 상장일부터 하루씩 비교하면서 코사인 유사도를 계산한다. 단, 과거의 주가와 현재 주가의 액수 차이가 크기 때문에 패턴만 추출하기 위해서 표준화를 먼저 수행한 다음 데이터를 비교한다. 이렇게 모든 데이터를 비교해서 코사인 유사도가 가장 작은 값 즉, 현재 주가 패턴과 가장 유사한 과거 시점의 주가를 찾는다.​이러한 흐름을 코드로 구현해보면 다음과 같다.이렇게 반복문을 통해 모든 코사인 유사도를 단일 리스트에 저장했다면, 이 값이 가장 작았던 구간을 추출해야 한다. 반복문에서 인덱스를 함께 저장했기 때문에 간단하게 불러낼 수 있다.​​Step 4. 패턴 비교 시각화패턴이 유사한 시점을 찾았는데, 이때의 주가 패턴이 얼마나 현재 패턴과 유사한지 궁금하다. 두 그래프를 동시에 그려보고 싶다. 두 시점의 주가 분포는 차이가 있기 때문에 그래프를 그리기 전에 표준화를 진행한다.범위를 맞춰줬으니 그래프를 그려본다.target(과거)도 base(현재)와 유사하게 급락 후 횡보하는 구간을 겪고 있다.​Step 5. 검색 기간 이후 추세 확인이번에는 검색된 과거 주가 이후 7일간의 데이터와 6월 2일 이후 주가를 비교해보려 한다. 패턴 검색 방식의 예측이 유효하다면 두 기간 이후 발생하는 추세도 유사할 것이다.아쉽게도 2015년 2월 주가는 반등했지만 6월 주가는 다시 급락하고 있다. 이를 통해 가장 유사한 패턴 하나만 가지고 예측하는 것이 바람직하지 않다는 것을 알 수 있다.​주가 움직임은 확률적으로 접근해야 한다. 만약 패턴 검색 방식으로 주가를 예측하고자 한다면 유사한 패턴(표본)을 최대한 많이(최소 30개) 추출한 다음 상승 확률을 계산하는 것이 좋다. 물론 이를 위해서는 코스피 뿐만 아니라 전 세계 주가 데이터를 대상으로 스크리닝해야 충분한 표본을 확보할 수 있다.관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "광고 프로모션 효율 증진을 위한 커머스 고객 세분화 - (2) 고객 군집 분석", "url": "/posts/%EA%B4%91%EA%B3%A0-%ED%94%84%EB%A1%9C%EB%AA%A8%EC%85%98-%ED%9A%A8%EC%9C%A8-%EC%A6%9D%EC%A7%84%EC%9D%84-%EC%9C%84%ED%95%9C-%EC%BB%A4%EB%A8%B8%EC%8A%A4-%EA%B3%A0%EA%B0%9D-%EC%84%B8%EB%B6%84%ED%99%94-2-%EA%B3%A0%EA%B0%9D-%EA%B5%B0%EC%A7%91-%EB%B6%84%EC%84%9D/", "categories": "3. 튜토리얼, 비즈니스 통계 분석 프로그래밍", "tags": "", "date": "2021-09-15 00:00:00 +0900", "snippet": "“분석이 먼저, 머신러닝은 그다음”분석이 먼저가 된 다음 머신러닝을 조미료처럼 얹어보는 것은 훌륭한 선택이다. 반대로 분석에 대한 이해가 부족한 상태에서 머신러닝을 사용한다면 해석에 오류를 덮어쓰게 되고 데이터 분석이 주는 설득의 이점을 잃게 만든다. 끝에서 한번 더 언급할텐데, 이 점을 유의하고 분석을 따라가다 보면 결론과 함께 앞으로 머신러닝을 어떻게 사용하면 좋을지 가늠할 수 있다.데이터의 차원(칼럼)이 많을수록 고객을 분류할 기준을 찾기가 복잡해진다. 사람의 머리로는 4차원 이상의 데이터는 상상하기 어렵고 시각화 차트로 분포를 확인할 수도 없다. 이런 상황에서 머신러닝은 차원 수가 많더라도 그들 관계에 대한 가장 설명력이 높은 최적의 패턴을 찾아내는데, X와 Y를 연결하는 복잡한 식을 찾아낼 때도 사용되지만 X라는 집단을 x_a, x_b, x_c 등으로 세분화하는 임의의 기준을 발견하기 위해서도 사용된다. 후자를 군집 분석이라 하며 대표적으로 K-means 알고리즘이 다양한 분야에서 활용된다.이번 장에서는 고객을 2개 그룹으로 분류해보면서 사람이 임의로 기준을 나누는 것과 머신러닝을 사용하는 방식의 차이를 살펴보고 머신러닝을 왜 ‘잘’ 사용해야 하는지 알아볼 것이다. 최종적으로 ‘잘’ 사용한 머신러닝을 활용해 이번 분석 주제인 “광고 프로모션 효율 증진을 위한 커머스 고객 세분화”에 대한 결론을 내면서 마무리하겠다.목차___Step 2. 고객 그룹 세분화 2-1. 나이에 따라 분류하기 2-2. 최근 구매일자에 따라 분류하기 2-3. 가입 기간에 따라 분류하기 2-4. 온/오프라인 선호도에 따라 분류하기 2-5. 프로모션 동의 여부에 따라 분류하기 2-6. 머신러닝으로 분류하기Step 2. 고객 그룹 세분화Step 1에서 군집 분석에 들어가기 전, 현실 상황을 가정하기 위해 일부 컬럼을 제거하고 age컬럼의 이상치를 조정하는 등 필요한 전처리를 끝냈다. 이번 Step 2에서는 고객을 다양한 방식으로 세분화할 예정이며 이를 위해 필요한 파이썬 라이브러리와 이전 장에서 저장한 데이터를 다시 불러오자.import pandas as pdimport matplotlib.pyplot as pltimport seaborn as snsfrom sklearn.cluster import KMeanscustomer_df = pd.read_csv(&#39;./data/customer.csv&#39;)customer_df   id day_f_buy buy_f_promo buy_f_web buy_f_catalog buy_f_store visit_web promo3 promo4 promo5 promo1 promo2 complain promo6 age period 0 5524 58 3 8 10 4 7 0 0 0 0 0 0 1 59 1042 1 2174 38 2 1 1 2 5 0 0 0 0 0 0 0 62 196 2 4141 26 1 8 2 10 4 0 0 0 0 0 0 0 51 543 3 6182 26 2 2 0 4 6 0 0 0 0 0 0 0 32 136 4 5324 94 5 5 3 6 5 0 0 0 0 0 0 0 35 392 … … … … … … … … … … … … … … … … … 2235 10870 46 2 9 3 4 5 0 0 0 0 0 0 0 49 612 2236 4001 56 7 8 2 5 7 0 0 0 1 0 0 0 70 132 2237 7270 91 1 2 3 13 6 0 1 0 0 0 0 0 35 386 2238 8235 8 2 6 5 10 3 0 0 0 0 0 0 0 60 387 2239 9405 40 3 3 1 4 7 0 0 0 0 0 0 1 62 853 2240 rows × 16 columns2-1. 나이에 따라 분류하기가장 먼저 생각해볼 수 있는 분류 기준은 ‘나이’다. 일반적으로 인적 정보 중에서도 ‘나이’는 가장 정규분포와 가까운 형태를 보이며, 그룹화를 위한 주요 기준이 된다. 특히 커머스 시장에서는 제품을 소싱하거나 광고 문구를 제작할 때부터 어떤 나이대의 고객을 타깃으로 할 것인지 1순위로 고려하기 때문에 ‘나이’는 매우 중요한 컬럼이다.sns.histplot(customer_df[&#39;age&#39;]);50세 정도를 기준으로 2개 그룹으로 나눌 수 있을 것 같다. 총 2240명 중 1120번째 고객의 나이를 확인해보고 약 절반을 나눌 수 있는 해당 나이대를 기준 값으로 선택한다.customer_df[&#39;age&#39;].sort_values()[:1120]1170 2046 202213 21747 211850 21 ..1668 461477 4636 462045 46467 46Name: age, Length: 1120, dtype: int6446세 이하 고객을 B-a그룹, 46세가 넘는 고객을 B-b그룹에 포함시킬 수 있다.# 46세 이하 그룹은 B-a로, 46세 초과 그룹은 B-b로 표기하여 over_46이라는 새로운 변수에 담는다.over_46 = customer_df[&#39;age&#39;].apply(lambda x : &#39;B-a&#39; if x &amp;amp;lt;= 46 else &#39;B-b&#39;)# B-a그룹에 몇명, B-b그룹에 몇명이 있는지에 대한 정보를 담은 시리즈 데이터를 counts_46에 담는다.counts_46 = over_46.value_counts()# counts_46의 인덱스(B-a, B-b)를 x축으로 하고 값(B-a,B-b 각각 갯수)을 y축에 표현한다.sns.barplot(x=counts_46.index, y=counts_46.values);이렇게 적절히 그룹을 나눈 다음 우리가 설계한 프로모션으로 테스트를 진행해볼 수 있다. 다만 A/B 테스트를 각 그룹에 적용하는 것이 아니라 A는 기존 방식의 일괄 프로모션을 적용하는 것이고, B는 우리가 분류한 B-a, B-b로 고객을 나눠 각 그룹의 연령대 성향에 맞는 프로모션을 적용하게 된다. 이 때문에 그룹명도 B-a, B-b로 설정했다.2-2. 최근 구매일자에 따라 분류하기광고, 프로모션은 피로도 관리가 매우 중요하다. 따라서 최근에 구매한 고객이라면 연이어 프로모션을 진행하는 것보다 구매 기간이 상당일 지난 고객에게만 프로모션을 별도로 제공하는 것이 효율이 좋을 수도 있다. 물론 아닐 수도 있기 때문에 정책을 바꾸기 전에 이렇게 가정을 하고 실험을 하는 것이 중요하다.# 최근 구매 이후 구매가 발생하지 않은 일수(날짜)별 고객 분포를 확인한다.sns.histplot(customer_df[&#39;day_f_buy&#39;]);마찬가지로 고객은 B-a, B-b 2개 그룹으로 나눠준다.(3개 이상의 그룹으로도 나눌 수 있지만 논의를 단순화하기 위해 2개 그룹으로만 나눠 실습을 진행하겠다.)그 기준은 구매하지 않은 기간(day_f_buy)을 날짜순으로 나열했을 때 1120번째 고객의 데이터로 정한다.customer_df[&#39;day_f_buy&#39;].sort_values()[:1120]66 0551 0347 023 0391 0 ..1670 491257 49435 491624 491282 49Name: day_f_buy, Length: 1120, dtype: int64최근 구매일로부터 50일이 지나지 않은(0일-49일) 고객들을 B-a 그룹, 50일을 초과한 고객들을 B-b 그룹으로 설정하고, 분포를 확인해보면 아래와 같다.# 50일이 지나지 않은 고객 -&amp;amp;gt; B-a, 50일 지난 고객 -&amp;gt; B-bover_50 = customer_df[&#39;day_f_buy&#39;].apply(lambda x : &#39;B-a&#39; if x &amp;amp;lt; 50 else &#39;B-b&#39;)# B-a그룹에 몇명, B-b그룹에 몇명이 있는지에 대한 정보를 담은 시리즈 데이터를 counts_50에 담는다.counts_50 = over_50.value_counts()# counts_50의 인덱스(B-a, B-b)를 x축으로 하고 값(B-a,B-b 각각 갯수)을 y축에 표현한다.sns.barplot(x=counts_50.index, y=counts_50.values);‘age’ 컬럼에 비해 더 유사한 비율의 분포를 확인할 수 있다. 각 그룹에 정확히 몇 명씩 포함되어 있는지 확인하고 이어서 다른 컬럼도 분석해보자.counts_50B-a 1122B-b 1118Name: day_f_buy, dtype: int642-3. 가입 기간에 따라 분류하기일반적으로 서비스 가입 기간은 정규분포를 따르지 않는다. 따라서 가입 기간별로 고객 그룹을 나누게 되면 연도별로 나누거나 특정 파트(기능) 런칭일을 기준일로 하게 돤다. 그러나 우리가 분석하려는 서비스의 경우 고객들이 이탈하고 있는 ‘문제 상황’이 존재하기 때문에 최근 가입자 수가 줄어들고 있을 가능성이 크다.다시 말해 전체 사용자 수는 늘고 있다 하더라도 최근 가입자 수가 정체되거나 줄어들고 있다면 초기부터 중기(문제 상황 시작)까지 가입자 수가 늘다가 중기부터 가입자 수가 줄어드는 정규분포 형태를 보일 수 있다. 다행히 아래 차트를 보면 우리의 예상과 일치한다.sns.histplot(customer_df[&#39;period&#39;]);데이터를 분석하다 보면 이렇게 정규분포와 가까운 형태를 찾게 된다. 2240건의 데이터에서 이러한 형태라면 실무 데이터를 직접 확인했을 때 정규성 검사를 통과할 확률이 적지만 있을 수 있고, 이 경우 상관분석, 회귀분석 등 다양한 통계기법들을 사용해볼 수 있다. 따라서 데이터 분석을 실제로 수행하기에 앞서 데이터가 정규분포를 띄는지, 독립 변수간 상관성이 있는지 등 다양한 검정 후에 본격적으로 분석을 수행하게 된다.‘가입기간’ 역시 신규 고객, 기존 고객 정도로 나눠볼 수 있겠다. 여기서 신규 고객을 아주 최근에 가입한 고객으로 생각하기보다 어떤 문제지점(표에서 볼 때 가입자 수가 가장 많았던 600-500일 지점) 이후와 이전으로 생각해보겠다. 이번에도 고객을 가입기간으로 절반 가량 분류하고 분포를 확인해보자.customer_df[&#39;period&#39;].sort_values()[:1120]2003 7150 72288 722155 7288 72 ... 1537 5831293 583327 5842005 5841974 584Name: period, Length: 1120, dtype: int64# 가입기간 0일-584일 지난 고객 -&amp;amp;gt; B-a, 가입한지 584일이 넘은 고객 -&amp;gt; B-bover_584 = customer_df[&#39;period&#39;].apply(lambda x : &#39;B-a&#39; if x &amp;amp;lt;= 584 else &#39;B-b&#39;)# B-a그룹에 몇명, B-b그룹에 몇명이 있는지에 대한 정보를 담은 시리즈 데이터를 counts_584에 담는다.counts_584 = over_584.value_counts()# counts_50의 인덱스(B-a, B-b)를 x축으로 하고 값(B-a,B-b 각각 갯수)을 y축에 표현한다.sns.barplot(x=counts_584.index, y=counts_584.values);이번에도 고객이 잘 분류된 것 같습니다. 정확한 숫자를 확인해보고 다음 컬럼을 살펴봅니다.counts_584B-a 1125B-b 1115Name: period, dtype: int642-4. 컴플레인 여부에 따라 분류하기컴플레인을 한 경험이 있는 고객과 그렇지 않은 고객은 명백한 경계가 존재한다. 곧 이탈할 것만 같은 고객에게는 강도 높은 프로모션을 제공하여 충성고객으로 돌아서게 만들 수 있다. 그러나 데이터 분석가가 이렇게 실험을 해야 하는 상황까지 왔다면 컴플레인 같은 현업의 눈에 띄는 마이너스 포인트가 애초에 없었을 가능성이 크다. 현업에서 혹은 기존 실무 관점에서 원인을 찾기 어려우니 데이터를 분석하는 단계로 넘어가게 된다.customer_df[&#39;complain&#39;].value_counts()0 22191 21Name: complain, dtype: int64count_complain = customer_df[&#39;complain&#39;].value_counts()sns.barplot(x=count_complain.index, y=count_complain.values);예상했듯이 컴플레인이 약 1% 수준으로, 겉으로 볼 때는 매우 양호한 서비스다. 평점으로 생각해보면 5점 만점에 4.9이다. 그래서 아무 문제가 없어 보이지만, 이탈하는 것이 너무 이상하기 때문에 이렇게 우리가 가설을 세우고 A/B테스트를 설계하고 테스트에 포함시킬 그룹을 정의하고 있는 것이다.사실, 이렇게 고객을 분류했을 때 불균형이 심하다면 A/B테스트는 적절하지 않다. 컴플레인을 한 21명의 고객과 나머지 1998명의 고객을 각 그룹으로 묶어서 A/B 테스트를 진행하는 것보다 21명의 고객에게 할인 쿠폰을 발급한다거나 유입률이 좋은 고효율 프로모션을 별도로 제공하는 것이 시간과 비용 모든 측면을 고려했을 때 더 좋은 선택이다. 1%의 고객을 대상으로 테스트하는 것은 테스트 성과가 성공적이라 하더라도 그것으로부터 얻는 인사이트가 비용 대비 저조하기 때문이다. 따라서 이 경우에는 해당 고객에게만 별도의 프로모션을 제공하는 것을 추가로 제안해보는 것이 좋겠다.2-5. 온/오프라인 선호도에 따라 분류하기실제로 데이터를 분석하게 되면 컬럼 하나씩 깊게 파고드는 것보다 여러 컬럼을 조합해서 새로운 정보를 취득하는 것이 중요하고, 분석가의 시야와 역량에 따라 조합의 수는 천차만별이기 때문에 다양한 분석 결과들이 나오게 된다.이번에는 4개 컬럼을 조합해서 간단하지만 중요한 정보를 얻어보고자 한다. 우리는 고객이 온라인에 익숙한지, 오프라인에 익숙한지 구매 상품의 노출 경로를 통해 확인할 수 있다. 다시 말해 오프라인에서(진열대, 전단지) 노출되는 상품을 온라인(온라인 광고, 웹사이트 등록)에서 노출되는 상품보다 많이 구매했다면 오프라인을 통해 상품 정보를 많이 획득하는 고객이라는 것이다. 따라서 해당 고객은 오프라인에 익숙하기 때문에 온라인 프로모션의 효율은 좋지 않을 것이라 예상할 수 있다.이 아이디어를 가지고 고객을 온라인 선호, 오프라인 선호 그룹으로 나눠볼 수 있다.# 오프라인 노출 상품 구매 수 : 오프라인 매장 진열 상품 구매 수 + 전단지 노출 상품 구매 수counts_buy_offline = customer_df[&#39;buy_f_store&#39;]+customer_df[&#39;buy_f_catalog&#39;]# 온라인 노출 상품 구매 수 : 광고(프로모션) 상품 구매 수 + 웹사이트 표기 상품 구매 수counts_buy_online = customer_df[&#39;buy_f_promo&#39;]+customer_df[&#39;buy_f_web&#39;]# 오프라인 노출 상품을 온라인 노출 상품보다 많이 구매한 경우 True, 그렇지 않은 경우 Falses = counts_buy_offline &amp;amp;gt; counts_buy_online# True or False --&amp;gt; 1 or 0 으로 변환s = s.astype(int)customer_df[&#39;prefer_offline&#39;] = s# 1, 0 각각 카운트count_offline = customer_df[&#39;prefer_offline&#39;].value_counts()# 시각화sns.barplot(x=count_offline.index, y=count_offline.values);위 분포는 오프라인을 선호하는 고객이라면 1이라는 값을 주고, 그렇지 않다면 0이라는 값을 줬다. 이번에 B-a, B-b가 아니라 0과 1을 사용한 이유는 코드에서 볼 수 있듯이 새로운 컬럼으로 사용하기 위함이다. 물론 본 분석에서 새롭게 만든 컬럼을 사용하지는 않겠지만 이렇게 다양한 컬럼의 조합으로 얻어진 정보는 새로운 컬럼으로 추가해주면 또 유용하게 사용할 수 있다. 그리고 이 ‘정보’를 머신러닝을 돌리기 위한 데이터로 사용할 수도 있기 때문에 기계가 이해할 수 있도록 미리 1과 0으로 데이터를 기입해준 것이다.다시 돌아와서, 이렇게 코드를 통해 온라인 선호 그룹을 0번 그룹, 오프라인 선호 그룹을 1번 그룹으로 나눌 수 있었다. 이 그룹을 실제 테스트 그룹(B)으로 활용한다면 온라인 프로모션을 0번 그룹(B-a)에만 진행하고 1번 그룹(B-b)에는 오프라인 전단을 실 거주지에 발송하는 것으로 실험을 진행해볼 수 있다. 일괄적으로 온라인 프로모션을 해오던 방식(A)과 결과를 비교해보고 이를 통해 실제 정책에 적용 여부를 판단한다.2-6. 프로모션 동의 여부에 따라 분류하기이번에도 1-5와 마찬가지로 여러 컬럼을 조합해서 정보를 얻어보겠다.우리가 분석하고 있는 데이터를 보면 promo1부터 promo6까지 각 프로모션(숫자가 높을수록 최근 프로모션) 동의 여부가 기록되어 있다. 이 데이터들을 고객별로 합산해서 각 고객마다 총 몇 개의 프로모션 발송 동의를 체크했는지 여부를 확인할 수 있다.customer_df.head(3)   id day_f_buy buy_f_promo buy_f_web buy_f_catalog buy_f_store visit_web promo3 promo4 promo5 promo1 promo2 complain promo6 age period 0 5524 58 3 8 10 4 7 0 0 0 0 0 0 1 59 1042 1 2174 38 2 1 1 2 5 0 0 0 0 0 0 0 62 196 2 4141 26 1 8 2 10 4 0 0 0 0 0 0 0 51 543 # 각 고객별 프로모션 동의 횟수 총합 계산 후 전체 동의 횟수 카운트counts_check_promo = customer_df[[&#39;promo1&#39;,&#39;promo2&#39;,&#39;promo3&#39;,&#39;promo4&#39;,&#39;promo5&#39;,&#39;promo6&#39;]].sum(axis=1).value_counts()# 시각화sns.barplot(x=counts_check_promo.index, y=counts_check_promo.values);한 번도 동의를 하지 않은(x축 ‘0’ 표기) 그룹의 비중이 너무 높기 때문에 나머지를 하나의 그룹으로 묶어줄 필요가 있다. 그리고 우리는 2개 그룹으로만 계속해서 분류하고 있기 때문에 이 방식이 오히려 적합하다.# 한번이라도 동의한사람check_promo = customer_df[[&#39;promo1&#39;,&#39;promo2&#39;,&#39;promo3&#39;,&#39;promo4&#39;,&#39;promo5&#39;,&#39;promo6&#39;]].sum(axis=1)==0check_promo = check_promo.astype(int)count_check_promo_any = check_promo.value_counts()# 시각화sns.barplot(x=count_check_promo_any.index, y=count_check_promo_any.values);‘1회 이상 동의한 고객’이 약 1600명으로, ‘한 번도 동의를 하지 않은 고객’보다 2배 이상 많다. 물론 여기서 ‘2번 이상 동의한 고객’과 같이 비율을 수정할 수도 있다. 여기서는 이대로 진행하겠다.우리는 ‘문제 상황’을 인지하고 다음 프로모션을 어떻게 설계할지 기획하고 있다. 다음 프로모션에 ‘동의’를 한 고객이 1번 그룹(1-6번 프로모션에 1회 이상 동의)에 있다면 기존 프로모션의 할인율은 유지한 채 전달하는 메시지나 스타일을 변경하고, 해당 고객이 0번 그룹(1-6번 프로모션에 동의한 적이 없는 그룹)에 속한다면 할인율 자체를 조정해보는 방식으로 B-a, B-b를 나눌 수 있다. 물론 이외에도 다양한 아이디어가 존재한다.2-7. 머신러닝으로 분류하기지금까지 데이터 분포 혹은 몇 가지 컬럼 조합으로 정보를 얻어내서 임의로 고객 집단을 분류해보았다. 이번에는 매우 매우 간단한 머신러닝을 사용해서 고객을 나눠볼 텐데, 머신러닝 알고리즘은 K-Means(K평균)를 사용하겠다. K-Means 알고리즘은 우리가 특정한 K개 군집으로 그룹을 나눈다고 생각하면 된다.# id 컬럼은 사용하지 않기 때문에 잠시 다른 변수에 할당한다.## (꼭 이렇게 해주지 않아도 괜찮지만 여러 상황에서 이후에 key값으로 사용하게 될 수도 있으니 이렇게 미리 셋팅해주면 편하다.)id = customer_df[&#39;id&#39;]# id 컬럼은 정보를 담고 있는 컬럼이 아닌 key값으로 사용되기 때문에 머신러닝 학습 데이터에서는 제외한다.data = customer_df.iloc[:,1:]# 2개 집단으로 분류할 것이다. 머신러닝은 2개 집단의 각 중심점에서 가까운 데이터를 하나의 군집으로 분류해낸다.kmeans = KMeans(n_clusters=2)# 각 고객(row)별로 소속 집단(0 or 1)에 대한 정보를 받는다.labels = kmeans.fit_predict(data)sns.scatterplot(data=data, x=&#39;period&#39;, y=&#39;age&#39;, c=labels);모델은 ‘period’를 기준으로 그룹을 나눴다. 모든 데이터를 입력받았지만 결국 분류 기준을 period로 사용한 것이다.확인을 위해 축을 변경해볼 수도 있다.# 나이와 최근 구매일 정보로 시각화sns.scatterplot(data=data, x=&#39;day_f_buy&#39;, y=&#39;age&#39;, c=labels);period가 아닌 다른 연속형 변수 조합으로는 패턴을 찾을 수 없다. period와 다른 컬럼을 조합하면 아래와 같이 표현된다.# 웹페이지 방문 수와 가입 기간 정보로 시각화sns.scatterplot(data=data, x=&#39;visit_web&#39;, y=&#39;period&#39;, c=labels);sns.scatterplot(data=data, x=&#39;prefer_offline&#39;, y=&#39;period&#39;, c=labels);period를 축으로 세우면 다른 컬럼은 period를 기준으로 정확히 분류된다. 즉, k-means 알고리즘이 period를 기준으로 고객을 절반씩 분류해낸 것이다. 어떤가? 이렇게만 설명하면 머신러닝이 훨씬 편하고 좋은 방법인 것 같다. 굳이 분포를 확인하지 않아도 되고, 기계가 찾아낸 패턴이니 왠지 가장 정확한 군집 방식인 것 같다. 차원이 복잡하고 많아서 생각하기 골치 아프니 머신러닝에게 맡기자!라는 식으로 접근하면 될 것 같고 그것이 기계가 인간을 위해 해줘야 하는 일인 것 같다. 그런데 여기서 잠깐 생각해보면, 저런 패턴이 과연 사람이 찾기 어려운 패턴인가? 에 대한 의문을 가질 수 있다. 사실 우리도 이미 1-3에서 period를 기준으로 집단을 나눈 적이 있고, 머신러닝의 결과와 크게 다르지 않았다. 오히려 머신러닝은 하나의 패턴만 찾아줬지만 우리는 간단한 분석만으로 6가지의 다양한 분류 패턴을 찾아냈다.또한 이런 접근법(머신러닝에게 맡기자!)에는 치명적인 문제가 있는데, ‘설명이 난해하다’는 것이다. 말 그대로 머신러닝에게 맡겼기 때문에 왜 그런지, 어떤 기준으로 저런 결과가 나온 것인지 K-mean 알고리즘에 대한 구체적인 수식을 덜어내고는 설명할 수 없다. 실무에서 분석은 결국 설명과 설득의 근거를 만들기 위한 과정이다. 따라서 설명할 수 없는 분석은 아무런 의미가 없다. ‘데이터 전부 넣고 기계를 돌려봤더니 이렇게 나왔어요’라고 할 수는 없는 노릇이다.사실, 이번 장에서는 적절하지 않은 머신러닝 사용이 해석에 오류를 가져오는 예를 보여주고 싶었다. 머신러닝을 직접 분석에 사용해본 적이 없고 글로만 접했다면 앞에서 이상한 부분을 느끼지 못했을 것이다. 머신러닝을 사용하기 위해서는 도메인 혹은 데이터에 대한 이해가 굉장히 중요하며 데이터를 분석하기 전에 반드시 이러한 이해가 선행되어야 한다. 머신러닝은 아무 데이터나 들이민다고 알아서 분류하고 예측하고 그룹을 묶어주는 마법이 아니다. 어떤 기준으로 분류할지, 어떤 컬럼을 머신러닝의 입력값으로 사용할지 분석가가 사전에 판단해야 한다.이번에는 머신러닝을 주도적으로, 적절히 사용하는 과정을 살펴보겠다. 먼저 어떤 기준으로 고객을 분류할 것인지 분석가가 지정해야 한다. 다양한 기준으로 분석을 해볼 수 있겠지만, 우리는 연속형 변수만 가지고 분석을 해보자. 프로모션 동의 여부, 컴플레인 여부 등은 그 속에서 복잡한 패턴을 찾을 필요가 없고, 우리가 앞에서 분석했듯이(1-4, 1-6) 단순 집계만으로도 유의미한 인사이트를 얻을 수 있기 때문이다.# 연속형 변수 3개만 추출한다.test_df = customer_df[[&#39;day_f_buy&#39;,&#39;age&#39;,&#39;period&#39;]]test_df   day_f_buy age period 0 58 59 1042 1 38 62 196 2 26 51 543 3 26 32 136 4 94 35 392 … … … … 2235 46 49 612 2236 56 70 132 2237 91 35 386 2238 8 60 387 2239 40 62 853 2240 rows × 3 columns다음으로, 머신러닝 모델을 학습시키기 전에 시각화를 해보면 좋다. 이전에 직접 분석했던 것처럼 데이터 분포를 눈으로 직접 확인하면서 분석 과정을 추적하는 것은 분석의 주도성을 확보하는 데에 도움이 된다. 또한 그렇게 추적해나가야만 머신러닝을 사용해서 집단을 분류해냈을 때 과연 합당한가에 대해 스스로 인지하고 판단할 수 있다.from mpl_toolkits.mplot3d import Axes3Dfig = plt.figure(figsize=(6,6))ax = Axes3D(fig)ax.scatter(xs=test_df[&#39;day_f_buy&#39;], ys=test_df[&#39;age&#39;], zs=test_df[&#39;period&#39;])ax.set_xlabel(&#39;age&#39;)ax.set_ylabel(&#39;day_f_buy&#39;)ax.set_zlabel(&#39;period&#39;)plt.show()2240명의 고객은 나이, 최근 구매일로부터 지난 일자, 총 가입 기간에 따라 3차원 데이터로 표현되고 있다. 우리는 이 데이터 공간에서 서로 가까운 데이터끼리 2개 그룹으로 나누고 싶다. 그리고 이 알고리즘을 직접 한 땀 한 땀 구현하기보다 보편적으로 활용되는 k-means 알고리즘을 사용할 것이다.# 2개 군집으로 나누는 kmeans 모델 생성kmeans_new = KMeans(n_clusters=2)# 연속형 변수만 포함한 데이터로 머신러닝 수행labels_new = kmeans_new.fit_predict(test_df)from mpl_toolkits.mplot3d import Axes3Dfig = plt.figure(figsize=(6,6))ax = Axes3D(fig)ax.scatter(xs=test_df[&#39;day_f_buy&#39;], ys=test_df[&#39;age&#39;], zs=test_df[&#39;period&#39;], c=labels_new)ax.set_xlabel(&#39;age&#39;)ax.set_ylabel(&#39;day_f_buy&#39;)ax.set_zlabel(&#39;period&#39;)plt.show()k-means 알고리즘을 적용해보니 period가 높은 집단과 낮은 집단으로 나눌 수 있었다. 즉, 서로 유사한 데이터 쌍(각 집단의 중심으로부터 가까운)으로 묶으면 period 기준으로 둘로 나눠진다는 것이다. 그렇다면 period 데이터에 어떤 특성이 기준이 되어서 집단이 분류될 수 있었는지 확인해보겠다. 먼저 그 ‘기준’이 되는 데이터를 찾아보자.# labels 컬럼을 새롭게 만들어준다.customer_df[&#39;labels&#39;] = labels# labels가 1인 데이터의 분포를 확인한다.customer_df[customer_df[&#39;labels&#39;] == 1][&#39;period&#39;].sort_values()902 581913 581621 581938 581445 582 ... 1473 1133724 11342194 1134976 11342029 1134Name: period, Length: 1135, dtype: int64# labels가 0인 데이터의 분포를 확인한다.customer_df[customer_df[&#39;labels&#39;] == 0][&#39;period&#39;].sort_values()2003 71288 7250 7288 722155 72 ... 1251 57986 579318 5791879 5802209 580Name: period, Length: 1105, dtype: int64k-평균 알고리즘은 period가 581 이상인 고객을 하나의 그룹, 581 미만인 고객을 다른 하나의 그룹으로 묶었다. 그렇다면 581을 기준으로 고객별 period의 분포가 어떻게 변하는지 확인해보자.sns.kdeplot(customer_df[&#39;period&#39;]);plt.axvline(x=581, c=&#39;r&#39;, linestyle=&#39;--&#39;);위 차트를 통해 가입자 수가 어떻게 변해왔는지 확인할 수 있다. 최근 가입자 수는(period-가입기간 200일 이하) 초기 가입자 수(period-가입기간 1000일 이상)만큼 그 수가 작아졌다.(사실 1-3에서 이미 확인했다.) 따라서 특정 기점(581일)으로 가입자 수가 급감하고 있기 때문에 이를 기준으로 그룹을 나누는 것이 적합하다고 판단(설명)할 수 있다.머신러닝은 이렇게 우리의 분석에 좋은 도구로 활용될 수 있으며 전적으로 의존하지 않는다면 설명력까지 확보할 수 있다. 따라서 분석이 먼저가 된 다음 머신러닝을 조미료처럼 얹어보는 것은 훌륭한 선택이고 더 좋은 머신러닝 알고리즘, 더 높은 머신러닝 성능은 한참 나중에나 필요하다.이제 분석을 마무리하면, 우리는 아래와 같이 결론을 내릴 수 있다.“고객을 다양한 방식으로 2개 집단으로 나눌 수 있다. 그중에서도 나이, 최근 구매일, 가입 기간을 고려해 가장 동질성이 높은 2개 집단으로 분류하면 가입 기간이 긴 고객과 짧은 고객으로 나눌 수 있으며 이는 ‘추세상’ 가입자 수가 감소하기 시작하는 581일을 기준으로 한다. 따라서 우리가 A/B 테스트를 수행함에 있어서 가입자 수 늘고 있던 시기에 가입한 고객들(period 기준 581일 이상)과 가입자 수가 감소하는 시기에 가입한 고객들(period 기준 581일 미만)을 서로 다른 집단으로 묶어서 각기 다른 프로모션 전략을 취하는 것이 좋겠다.”관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "광고 프로모션 효율 증진을 위한 커머스 고객 세분화 - (1) 문제 상황 가정 및 데이터 전처리", "url": "/posts/%EA%B4%91%EA%B3%A0-%ED%94%84%EB%A1%9C%EB%AA%A8%EC%85%98-%ED%9A%A8%EC%9C%A8-%EC%A6%9D%EC%A7%84%EC%9D%84-%EC%9C%84%ED%95%9C-%EC%BB%A4%EB%A8%B8%EC%8A%A4-%EA%B3%A0%EA%B0%9D-%EC%84%B8%EB%B6%84%ED%99%94-1-%EB%AC%B8%EC%A0%9C-%EC%83%81%ED%99%A9-%EA%B0%80%EC%A0%95-%EB%B0%8F-%EB%8D%B0%EC%9D%B4%ED%84%B0/", "categories": "3. 튜토리얼, 비즈니스 통계 분석 프로그래밍", "tags": "", "date": "2021-09-11 00:00:00 +0900", "snippet": "“데이터”가 아니라 “문제”를 먼저 보는 연습이번 분석은 임의의 문제 상황을 가정하고 데이터를 통해 문제를 해결할 방법을 찾는다. 처음에는 의식적으로 ‘데이터’에 집중하게 되는데 본문에서 일부러 상황과 문제를 계속해서 강조했다. 그러니 ‘문제’에 집중해보자. ‘문제’를 이해하면 데이터는 자연스럽게 눈에 들어온다. 특히 후반부에 집단 군집 분석을 진행할 텐데, 이를 위해 머신러닝도 사용해볼 예정이다.목차___Step 1. 문제 상황 가정 및 데이터 전처리 1-1. 라이브러리 호출 및 데이터 확인 1-2. 일부 컬럼 제거 1-3. 컬럼명, 데이터타입 형식 통일 1-4. 현재 날짜 가정 1-5. 이상치 처리1.  문제 상황 가정 및 데이터 전처리지금부터 우리는 이커머스 스타트업의 데이터 분석가다. 상품이 많아지면서 사용자 수가 늘고 매출 규모는 늘어나고 있지만 최근에 고객들이 많이 이탈하고 있다. 이탈을 막기 위해 방안을 모색하던 중 상품 MD와 플랫폼 기획 쪽에서 서비스 개편이나 상품 수급 기준을 높이자는 의견이 나왔다. 하지만 우리가 볼 때 신규 유입자 수는 역대 최고치를 경신하고 있기 때문에 서비스를 전면 개편하거나 유통업체를 바꾸는 시도는 하지 않는 것이 좋은 상황인 것 같다.대신 우리는 사용자 수가 기존 대비 가파르게 늘고 있지만 그간 광고 프로모션에는 큰 변화를 주지 않았기 때문에 본인의 소비 성향과 관련 없는 광고에 피로감을 느꼈다고 생각했다. 그래서 기존 고객들이 이탈하고 있는 이유를 서비스가 마음에 들지 않아서가 아니라 광고에 대한 피로감 때문이라는 가설을 세우고 프로모션 A/B 테스트를 통해 검증해보자는 의견을 냈다.다행히 현업에서도 우리 의견에 공감하여 A/B 테스트 설계를 시작한 상황이다. 실험은 기존의 일괄적인 프로모션과 개인화된 프로모션으로 제작하고, 소비자 그룹을 무작위로 50:50으로 나눠 한쪽은 일괄 프로모션(A), 다른 한쪽은 개인화 프로모션(B)을 적용할 것이다. 이번 분석(광고 프로모션 효율 증진을 위한 커머스 고객 세분화)의 흐름은 개인화 프로모션(B)을 제작하기 전, 개인화(grouping) 기준을 설정해나가는 과정이 되겠다.1-1. 라이브러리 호출 및 데이터 확인여건상 캐글 사이트에서 데이터셋을 빌려오자. 대신, 실무 상황을 가정하기 위해 데이터셋을 약간 조정할 것이고 조금은 부족한 데이터셋으로 고객 그룹을 나눠볼 예정이다. 단, 데이터 양이 많지 않기 때문에(2240 rows) 결측치를 일부러 만들거나 하지는 않겠다. 데이터셋은 아래 링크에서 확보했다. Customer Personality Analysis, Kaggle Dataset# 데이터프레임import pandas as pd# 시간 계산import datetime# 시각화import matplotlib.pyplot as pltimport seaborn as snsimport missingno as msno# &#39;Tab&#39; 구분자로 나뉜 csv 파일을 불러온다.customer_df_r = pd.read_csv(&#39;./data/marketing_campaign.csv&#39;, delimiter=&#39;\\t&#39;)customer_df_r.head()   ID Year_Birth Education Marital_Status Income Kidhome Teenhome Dt_Customer Recency MntWines … NumWebVisitsMonth AcceptedCmp3 AcceptedCmp4 AcceptedCmp5 AcceptedCmp1 AcceptedCmp2 Complain Z_CostContact Z_Revenue Response 0 5524 1957 Graduation Single 58138.0 0 0 04-09-2012 58 635 … 7 0 0 0 0 0 0 3 11 1 1 2174 1954 Graduation Single 46344.0 1 1 08-03-2014 38 11 … 5 0 0 0 0 0 0 3 11 0 2 4141 1965 Graduation Together 71613.0 0 0 21-08-2013 26 426 … 4 0 0 0 0 0 0 3 11 0 3 6182 1984 Graduation Together 26646.0 1 0 10-02-2014 26 11 … 6 0 0 0 0 0 0 3 11 0 4 5324 1981 PhD Married 58293.0 1 0 19-01-2014 94 173 … 5 0 0 0 0 0 0 3 11 0 5 rows × 29 columns결측치와 데이터 타입을 살펴보자.customer_df_r.info()&amp;lt;class &#39;pandas.core.frame.DataFrame&#39;&amp;gt;RangeIndex: 2240 entries, 0 to 2239Data columns (total 29 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 ID 2240 non-null int64 1 Year_Birth 2240 non-null int64 2 Education 2240 non-null object 3 Marital_Status 2240 non-null object 4 Income 2216 non-null float64 5 Kidhome 2240 non-null int64 6 Teenhome 2240 non-null int64 7 Dt_Customer 2240 non-null object 8 Recency 2240 non-null int64 9 MntWines 2240 non-null int64 10 MntFruits 2240 non-null int64 11 MntMeatProducts 2240 non-null int64 12 MntFishProducts 2240 non-null int64 13 MntSweetProducts 2240 non-null int64 14 MntGoldProds 2240 non-null int64 15 NumDealsPurchases 2240 non-null int64 16 NumWebPurchases 2240 non-null int64 17 NumCatalogPurchases 2240 non-null int64 18 NumStorePurchases 2240 non-null int64 19 NumWebVisitsMonth 2240 non-null int64 20 AcceptedCmp3 2240 non-null int64 21 AcceptedCmp4 2240 non-null int64 22 AcceptedCmp5 2240 non-null int64 23 AcceptedCmp1 2240 non-null int64 24 AcceptedCmp2 2240 non-null int64 25 Complain 2240 non-null int64 26 Z_CostContact 2240 non-null int64 27 Z_Revenue 2240 non-null int64 28 Response 2240 non-null int64 dtypes: float64(1), int64(25), object(3)memory usage: 507.6+ KB칼럼이 많아서 한 번에 다 보여주지 못한다. 가로로 늘어뜨린 barplot으로 결측치를 확인해보자.msno.bar(customer_df_r);Income 칼럼에만 결측가 보인다. 과연 그런지 조건식을 통해 체크해보자.# 전체 결측치와 Income 컬럼의 결측치 수를 비교. 일치하면 True 반환customer_df_r.isnull().sum().sum() == customer_df_r[&#39;Income&#39;].isnull().sum()True전체 중 Income 칼럼에만 결측치가 존재한다. 그래서 저 결측치만 적절히 처리해주고 분류 모델에 욱여넣으면 될 것 같은데, 좀 더 현실적인 상황을 만들어보고 싶다.1-2. 일부 칼럼 제거머신러닝은 이렇게 양질의 데이터와 독립적인 칼럼들이 많을수록 성능이 좋다. 하지만 실무에서 이렇게 조건이 좋기는 어렵다.----------------------------------------------------------------------------------People----------------------------------------------------------------------------------ID: Customer&#39;s unique identifierYear_Birth: Customer&#39;s birth yearEducation: Customer&#39;s education levelMarital_Status: Customer&#39;s marital statusIncome: Customer&#39;s yearly household incomeKidhome: Number of children in customer&#39;s householdTeenhome: Number of teenagers in customer&#39;s householdDt_Customer: Date of customer&#39;s enrollment with the companyRecency: Number of days since customer&#39;s last purchaseComplain: 1 if customer complained in the last 2 years, 0 otherwise----------------------------------------------------------------------------------Products----------------------------------------------------------------------------------MntWines: Amount spent on wine in last 2 yearsMntFruits: Amount spent on fruits in last 2 yearsMntMeatProducts: Amount spent on meat in last 2 yearsMntFishProducts: Amount spent on fish in last 2 yearsMntSweetProducts: Amount spent on sweets in last 2 yearsMntGoldProds: Amount spent on gold in last 2 years----------------------------------------------------------------------------------Promotion----------------------------------------------------------------------------------NumDealsPurchases: Number of purchases made with a discountAcceptedCmp1: 1 if customer accepted the offer in the 1st campaign, 0 otherwiseAcceptedCmp2: 1 if customer accepted the offer in the 2nd campaign, 0 otherwiseAcceptedCmp3: 1 if customer accepted the offer in the 3rd campaign, 0 otherwiseAcceptedCmp4: 1 if customer accepted the offer in the 4th campaign, 0 otherwiseAcceptedCmp5: 1 if customer accepted the offer in the 5th campaign, 0 otherwiseResponse: 1 if customer accepted the offer in the last campaign, 0 otherwise----------------------------------------------------------------------------------Place----------------------------------------------------------------------------------NumWebPurchases: Number of purchases made through the company’s web siteNumCatalogPurchases: Number of purchases made using a catalogueNumStorePurchases: Number of purchases made directly in storesNumWebVisitsMonth: Number of visits to company’s web site in the last month여기서 어느 회사에나 있는 데이터는 다음과 같다. People 유형의 고객 ID, 출생연도(Year_Birth), 가입일(Dt_Customer), 가장 최근 구매일(Recency, 여기서는 구매 후 오늘까지 날짜로 저장되어 있다.), 클레임 수(Complain) 칼럼 Promotion 유형의 모든 칼럼(할인받고 구매한 상품 수와 프로모션에 대한 동의 여부) Place 유형의 모든 칼럼(다양한 홍보 채널의 구매 상품 수 관련)이렇게 상품에 대한 정보, 구매 경로, 고객 가입 정보 등은 어느 업체나 매출 관리를 위해 보유하고 있다.반면 아래 칼럼들은 설문과 같은 임의의 특정 방식으로 수집하지 않는다면 보유하기 어렵다. People 유형의 교육 수준(Education), 결혼 여부(Marital_Status), 소득 수준(Income), 10세 미만 자녀 수(Kidhome), 10대 자녀 수(Teenhome) Products 유형의 모든 칼럼(고객이 1년에 어떤 제품을 얼마나 소비하는지 여부, 이 데이터는 소비는 자사 제품의 구매 내역이 아닌 개인 소비 성향에 대한 답변으로 확보된 것으로 보인다.)우리는 일반적인 회사에 기본적으로 수집하고 있을 법한 데이터만 분석에 사용하겠다. 따라서 사용하지 않을 칼럼들을 모두 제거한다. 추가로, Z_CostContact와 Z_Revenue칼럼은 그 의미가 불분명해 역시 제거한다.# 혹시 중요한 컬럼이 있을지 모르니 원본은 남겨두고 다른 변수에 새롭게 할당하여 사용한다.customer_df = customer_df_r.drop(columns=[&#39;Education&#39;,&#39;Marital_Status&#39;,&#39;Income&#39;,&#39;Kidhome&#39;,&#39;Teenhome&#39;,\\ &#39;MntWines&#39;,&#39;MntFruits&#39;,&#39;MntMeatProducts&#39;,&#39;MntFishProducts&#39;,&#39;MntSweetProducts&#39;,&#39;MntGoldProds&#39;,\\ &#39;Z_CostContact&#39;,&#39;Z_Revenue&#39;], axis=1)customer_df.head()   ID Year_Birth Dt_Customer Recency NumDealsPurchases NumWebPurchases NumCatalogPurchases NumStorePurchases NumWebVisitsMonth AcceptedCmp3 AcceptedCmp4 AcceptedCmp5 AcceptedCmp1 AcceptedCmp2 Complain Response 0 5524 1957 04-09-2012 58 3 8 10 4 7 0 0 0 0 0 0 1 1 2174 1954 08-03-2014 38 2 1 1 2 5 0 0 0 0 0 0 0 2 4141 1965 21-08-2013 26 1 8 2 10 4 0 0 0 0 0 0 0 3 6182 1984 10-02-2014 26 2 2 0 4 6 0 0 0 0 0 0 0 4 5324 1981 19-01-2014 94 5 5 3 6 5 0 0 0 0 0 0 0 1-3. 칼럼명, 데이터 타입 형식 통일이어서 칼럼명이 복잡하니 이해하기 쉽도록 네이밍을 조금 수정하겠다. 오탈자 방지를 위해 소문자만 사용한다. f는 from의 약자, promo는 promotion의 약자로 사용한다.customer_df.columns = [&#39;id&#39;, &#39;birth&#39;, &#39;enroll&#39;, &#39;day_f_buy&#39;, \\ &#39;buy_f_promo&#39;, &#39;buy_f_web&#39;, &#39;buy_f_catalog&#39;, &#39;buy_f_store&#39;,\\ &#39;visit_web&#39;,&#39;promo3&#39;,&#39;promo4&#39;,&#39;promo5&#39;,&#39;promo1&#39;,&#39;promo2&#39;,&#39;complain&#39;,&#39;promo6&#39;]customer_df.head()   id birth enroll day_f_buy buy_f_promo buy_f_web buy_f_catalog buy_f_store visit_web promo3 promo4 promo5 promo1 promo2 complain promo6 0 5524 1957 04-09-2012 58 3 8 10 4 7 0 0 0 0 0 0 1 1 2174 1954 08-03-2014 38 2 1 1 2 5 0 0 0 0 0 0 0 2 4141 1965 21-08-2013 26 1 8 2 10 4 0 0 0 0 0 0 0 3 6182 1984 10-02-2014 26 2 2 0 4 6 0 0 0 0 0 0 0 4 5324 1981 19-01-2014 94 5 5 3 6 5 0 0 0 0 0 0 0 # 컬럼별 데이터타입 확인customer_df.dtypesid int64birth int64enroll objectday_f_buy int64buy_f_promo int64buy_f_web int64buy_f_catalog int64buy_f_store int64visit_web int64promo3 int64promo4 int64promo5 int64promo1 int64promo2 int64complain int64promo6 int64dtype: objectenroll 칼럼은 내용에 맞게 datetime으로 변경해주자. 그 후 오늘 날짜(가정)와 비교해서 가입 기간을 계산해보자.customer_df[&#39;enroll&#39;] = customer_df[&#39;enroll&#39;].astype(&#39;datetime64&#39;)customer_df[&#39;enroll&#39;]0 2012-04-091 2014-08-032 2013-08-213 2014-10-024 2014-01-19 ... 2235 2013-06-132236 2014-10-062237 2014-01-252238 2014-01-242239 2012-10-15Name: enroll, Length: 2240, dtype: datetime64[ns]1-4. 현재 날짜 가정customer_df[&#39;enroll&#39;].sort_values()2029 2012-01-08976 2012-01-082194 2012-01-08724 2012-01-081473 2012-01-09 ... 153 2014-12-05815 2014-12-05216 2014-12-0550 2014-12-052003 2014-12-06Name: enroll, Length: 2240, dtype: datetime64[ns]고객 2240명의 가입일을 날짜순으로 나열해보니 가장 최근 가입일이 ‘2014년 말’이다. 해당 고객의 데이터를 잠깐 살펴보자.customer_df.iloc[2003]id 6679birth 1966enroll 2014-12-06 00:00:00day_f_buy 29buy_f_promo 1buy_f_web 0buy_f_catalog 0buy_f_store 3visit_web 3promo3 0promo4 0promo5 0promo1 0promo2 0complain 0promo6 0Name: 2003, dtype: object가장 최근에 가입한 고객은 이번 달에 3번 웹사이트를 방문했으며 오프라인 스토어 상품 3개, 프로모션 상품 1개를 구매했고 구매일로부터 29일 지났다. 그렇다면 우리가 가정할 수 있는 ‘현시점’은 2015년 2월 초~중순 정도가 되겠다. 임의로 현재 날짜를 2015년 2월 15일로 설정하고 과거의 시점에서 데이터를 분석한다고 생각해보겠다.현시점을 설정함으로써 고객의 나이와 가입 기간을 확보할 수 있다. 먼저 birth 칼럼 대신 age 칼럼을 만들어보자.# 고객 나이 계산하기customer_df[&#39;age&#39;] = 2015 - customer_df[&#39;birth&#39;] + 1# 기존 birth 컬럼 제거customer_df.drop(&#39;birth&#39;, axis=1, inplace=True)customer_df[&#39;age&#39;]0 591 622 513 324 35 ..2235 492236 702237 352238 602239 62Name: age, Length: 2240, dtype: int64다음으로 enroll 칼럼 대신 가입기간을 뜻하는 period 칼럼을 만들어보자.# 날짜 차이(기간) 데이터 확보period = datetime.datetime(2015,2,15) - customer_df[&#39;enroll&#39;]period0 1042 days1 196 days2 543 days3 136 days4 392 days ... 2235 612 days2236 132 days2237 386 days2238 387 days2239 853 daysName: enroll, Length: 2240, dtype: timedelta64[ns]# 날짜 차이 데이터 -&amp;gt; 타입 변경(timedelta -&amp;gt; int) 후 컬럼 지정customer_df[&#39;period&#39;] = period.dt.dayscustomer_df.drop(&#39;enroll&#39;, axis=1, inplace=True)customer_df[&#39;period&#39;]0 10421 1962 5433 1364 392 ... 2235 6122236 1322237 3862238 3872239 853Name: period, Length: 2240, dtype: int641-5. 이상치 처리데이터 분포를 확인해서 분석에 방해가 되는 극단치 데이터 혹은 잘못 기입된(것으로 추측할 수 있는) 것이 있다면 적절히 처리한다.customer_df.describe()   id day_f_buy buy_f_promo buy_f_web buy_f_catalog buy_f_store visit_web promo3 promo4 promo5 promo1 promo2 complain promo6 age period count 2240.000000 2240.000000 2240.000000 2240.000000 2240.000000 2240.000000 2240.000000 2240.000000 2240.000000 2240.000000 2240.000000 2240.000000 2240.000000 2240.000000 2240.000000 2240.000000 mean 5592.159821 49.109375 2.325000 4.084821 2.662054 5.790179 5.316518 0.072768 0.074554 0.072768 0.064286 0.013393 0.009375 0.149107 47.194196 583.043304 std 3246.662198 28.962453 1.932238 2.778714 2.923101 3.250958 2.426645 0.259813 0.262728 0.259813 0.245316 0.114976 0.096391 0.356274 11.984069 232.229893 min 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 20.000000 71.000000 25% 2828.250000 24.000000 1.000000 2.000000 0.000000 3.000000 3.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 39.000000 411.750000 50% 5458.500000 49.000000 2.000000 4.000000 2.000000 5.000000 6.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 46.000000 584.000000 75% 8427.750000 74.000000 3.000000 6.000000 4.000000 8.000000 7.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 57.000000 756.250000 max 11191.000000 99.000000 15.000000 27.000000 28.000000 13.000000 20.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 123.000000 1134.000000 표로만 보는 것보다 시각화를 통해 살펴보는 것도 도움이 된다. 다만, prom1~prom6, complain 칼럼은 정보가 빽빽하게 채워진 형태가 아닌 scalar 값(0 혹은 1)이므로 시각화가 정보를 주지 못한다. 또한 id 컬럼은 개별 고객을 특정하는 고윳값이므로 시각화의 의미가 없다. 따라서 해당 칼럼들은 제외하고 pairplot을 통해 데이터를 펼쳐보자.# 아래 데이터만 시각화로 출력할 것이다.pairs_df = customer_df.loc[:, [col for col in customer_df.columns if col not in [&#39;id&#39;,&#39;promo1&#39;,&#39;promo2&#39;,&#39;promo3&#39;,&#39;promo4&#39;,&#39;promo5&#39;,&#39;promo6&#39;,&#39;complain&#39;]]]pairs_df   day_f_buy buy_f_promo buy_f_web buy_f_catalog buy_f_store visit_web age period 0 58 3 8 10 4 7 59 1042 1 38 2 1 1 2 5 62 196 2 26 1 8 2 10 4 51 543 3 26 2 2 0 4 6 32 136 4 94 5 5 3 6 5 35 392 … … … … … … … … … 2235 46 2 9 3 4 5 49 612 2236 56 7 8 2 5 7 70 132 2237 91 1 2 3 13 6 35 386 2238 8 2 6 5 10 3 60 387 2239 40 3 3 1 4 7 62 853 2240 rows × 8 columnssns.pairplot(data=pairs_df);대부분의 데이터에 튀는 값들이 있지만 구매 관련 칼럼들의 경우 그 값 자체로 의미 있는 정보를 담고 있을 확률이 크다. 따라서 그것을 제외하고 age, period 칼럼을 봤을 때, period는 정규분포와 유사한 형태로 잘 정돈되어 있으나 age 칼럼은 이상치가 정보량을 줄이고 있는 것을 볼 수 있다. 또한 그 이상치가 100을 넘는 것으로 볼 때 상식적으로 의심을 품을 만한 데이터로 보인다. 일반적이지 않은 데이터이므로 해당 칼럼만 다시 시각화를 통해 확인해보자.sns.boxplot(data=customer_df, x=&#39;age&#39;);age가 120 근처에 있는 것은 이상하다. 해당 고객들을 데이터를 살펴보자.customer_df[customer_df[&#39;age&#39;]&amp;gt;100]   id day_f_buy buy_f_promo buy_f_web buy_f_catalog buy_f_store visit_web promo3 promo4 promo5 promo1 promo2 complain promo6 age period 192 7829 99 1 2 1 2 5 0 0 0 0 0 1 0 116 507 239 11004 23 1 1 0 2 4 0 0 0 0 0 0 0 123 274 339 1150 36 1 4 6 4 1 0 0 1 0 0 0 0 117 507 나이 정보를 제외하고 나머지 칼럼들로 상관성을 분석해서 가장 상관성이 높은 다섯 고객의 나이 평균을 해당 고객의 나이로 지정해줄 것이다. 물론 100% 일치할 수는 없겠지만 컬럼을 제거하는 것보다 정보 손실을 줄일 수 있다.# id 컬럼을 제거하고 고객 리스트를 컬럼으로 전치한 다음 각 고객별로 상관도 테이블을 만든다.corr_df = customer_df.drop(&#39;id&#39;, axis=1).T.corr()# 상관도 테이블에서 나이가 110세 이상으로 표기된 고객 3명의 상관도 리스트를 추출한다. age_outliers_df = corr_df[customer_df[customer_df[&#39;age&#39;]&amp;gt;100].index]age_outliers_df   192 239 339 0 0.977110 0.929072 0.984905 1 0.996395 0.985838 0.989961 2 0.981023 0.941784 0.990341 3 0.999108 0.971865 0.992261 4 0.988590 0.924792 0.974089 … … … … 2235 0.982841 0.937229 0.988486 2236 0.949149 0.953366 0.920994 2237 0.988644 0.925626 0.974716 2238 0.982029 0.959088 0.995677 2239 0.978670 0.935204 0.987545 2240 rows × 3 columns# 고객별로 상관도가 높은 순으로 정렬한 다음 각각 5명의 인덱스를 추출한다.age_outliers_corr_dict = {}for col in age_outliers_df.columns: # 정렬했을 때 첫번째 값은 자기 자신의 인덱스이므로(상관성 100%) 2번째 값부터 5개를 가져온다. age_outliers_corr_dict[col] = list(age_outliers_df[col].sort_values(ascending=False).index[1:6])print(age_outliers_corr_dict){192: [74, 923, 1380, 2052, 458], 239: [635, 1420, 1848, 66, 1468], 339: [1158, 2023, 1232, 1184, 1462]}이제 해당 고객들의 기존 age데이터를 예측한 나이로 바꿔주고 어떤 데이터(나이)로 교체되었는지 출력도 해보자.for key, val in age_outliers_corr_dict.items(): pred_age = int(customer_df.iloc[age_outliers_corr_dict[key]][&#39;age&#39;].mean()) customer_df.iloc[key][&#39;age&#39;] = pred_age print(&#39;index {}번 고객의 수정된 나이(추정) : {}&#39;.format(key, pred_age))index 192번 고객의 수정된 나이(추정) : 62index 239번 고객의 수정된 나이(추정) : 52index 339번 고객의 수정된 나이(추정) : 52마지막으로 수정한 age 컬럼 분포를 시각화합니다.sns.boxplot(data=customer_df, x=&#39;age&#39;);이상치가 정상적으로 처리되었다. 지금까지 전처리한 데이터를 다시 펼쳐보면 아래와 같다.customer_df   id day_f_buy buy_f_promo buy_f_web buy_f_catalog buy_f_store visit_web promo3 promo4 promo5 promo1 promo2 complain promo6 age period 0 5524 58 3 8 10 4 7 0 0 0 0 0 0 1 59 1042 1 2174 38 2 1 1 2 5 0 0 0 0 0 0 0 62 196 2 4141 26 1 8 2 10 4 0 0 0 0 0 0 0 51 543 3 6182 26 2 2 0 4 6 0 0 0 0 0 0 0 32 136 4 5324 94 5 5 3 6 5 0 0 0 0 0 0 0 35 392 … … … … … … … … … … … … … … … … … 2235 10870 46 2 9 3 4 5 0 0 0 0 0 0 0 49 612 2236 4001 56 7 8 2 5 7 0 0 0 1 0 0 0 70 132 2237 7270 91 1 2 3 13 6 0 1 0 0 0 0 0 35 386 2238 8235 8 2 6 5 10 3 0 0 0 0 0 0 0 60 387 2239 9405 40 3 3 1 4 7 0 0 0 0 0 0 1 62 853 2240 rows × 16 columns혹시 모르니 결측치와 데이터 분포도 다시 한번 확인해준다.customer_df.info()&amp;lt;class &#39;pandas.core.frame.DataFrame&#39;&amp;gt;RangeIndex: 2240 entries, 0 to 2239Data columns (total 16 columns): # Column Non-Null Count Dtype--- ------ -------------- ----- 0 id 2240 non-null int64 1 day_f_buy 2240 non-null int64 2 buy_f_promo 2240 non-null int64 3 buy_f_web 2240 non-null int64 4 buy_f_catalog 2240 non-null int64 5 buy_f_store 2240 non-null int64 6 visit_web 2240 non-null int64 7 promo3 2240 non-null int64 8 promo4 2240 non-null int64 9 promo5 2240 non-null int64 10 promo1 2240 non-null int64 11 promo2 2240 non-null int64 12 complain 2240 non-null int64 13 promo6 2240 non-null int64 14 age 2240 non-null int64 15 period 2240 non-null int64dtypes: int64(16)memory usage: 280.1 KBcustomer_df.describe()   id day_f_buy buy_f_promo buy_f_web buy_f_catalog buy_f_store visit_web promo3 promo4 promo5 promo1 promo2 complain promo6 age period count 2240.000000 2240.000000 2240.000000 2240.000000 2240.000000 2240.000000 2240.000000 2240.000000 2240.000000 2240.000000 2240.000000 2240.000000 2240.000000 2240.000000 2240.000000 2240.000000 mean 5592.159821 49.109375 2.325000 4.084821 2.662054 5.790179 5.316518 0.072768 0.074554 0.072768 0.064286 0.013393 0.009375 0.149107 47.109375 583.043304 std 3246.662198 28.962453 1.932238 2.778714 2.923101 3.250958 2.426645 0.259813 0.262728 0.259813 0.245316 0.114976 0.096391 0.356274 11.699227 232.229893 min 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 20.000000 71.000000 25% 2828.250000 24.000000 1.000000 2.000000 0.000000 3.000000 3.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 39.000000 411.750000 50% 5458.500000 49.000000 2.000000 4.000000 2.000000 5.000000 6.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 46.000000 584.000000 75% 8427.750000 74.000000 3.000000 6.000000 4.000000 8.000000 7.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 57.000000 756.250000 max 11191.000000 99.000000 15.000000 27.000000 28.000000 13.000000 20.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 76.000000 1134.000000 이상 없는 것을 확인하여 이 데이터셋을 저장하고 다음 장(Step2~)에서 그대로 활용한다. 다음 장에서는 머신러닝을 활용해 군집 분석을 실시하고 고객 그룹을 다양한 유형으로 나눠볼 예정이다.customer_df.to_csv(&#39;./data/customer.csv&#39;, index=None)관성을 이기는 데이터저작자표시(새창열림)" }, { "title": "존버 vs 자동매매 백테스팅 수익률 비교(카카오) - 파이썬 금융 데이터 분석", "url": "/posts/%EC%A1%B4%EB%B2%84-vs-%EC%9E%90%EB%8F%99%EB%A7%A4%EB%A7%A4-%EB%B0%B1%ED%85%8C%EC%8A%A4%ED%8C%85-%EC%88%98%EC%9D%B5%EB%A5%A0-%EB%B9%84%EA%B5%90-%EC%B9%B4%EC%B9%B4%EC%98%A4-%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EA%B8%88%EC%9C%B5-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%B6%84%EC%84%9D/", "categories": "3. 튜토리얼, 금융 분석 프로그래밍 응용", "tags": "", "date": "2021-09-07 00:00:00 +0900", "snippet": "Step 0. 전략과거에 카카오 상장 초기에 배팅했다면 지금쯤 소위 대박을 터뜨렸을 것이다. 이를 아는 상황에서 과연 특정 (차트) 매매 전략이 단순 장기 보유보다 우세한지 확인해 보고자 한다.​전략은 잘 알려진 골든크로스 매수, 데드크로스 매도 방식을 시도한다. 골든크로스는 단기 이동평균선(보통 20일 이하)이 장기 이동평균선(보통 30일 이상)을 뚫고 올라가는 시점, 데드크로스는 그 반대 시점을 지칭한다. 여기에 모멘텀을 더해 상승 추세의 골든크로스를 매수 포인트로 잡을 수도 있지만, 논의를 단순화하기 위해 이 부분은 고려하지 않겠다.Step 1. 데이터 불러오기주가 정보를 가져오기 위해 FinanceDataReader를 사용한다.​Step 2. 장기 보유 성과 확인1999년 11월 11일에 카카오 주식을 100만원치 샀다고 가정하면 현재 주가 기준으로 약 7천만 원이 되었다. 작년까지만 해도 주가가 15만 원을 상회했기 때문에 이는 사실 반 토막 난 가격이다. 그럼에도 불구하고 70배 이상 수익을 본 것이다.전체 기간 주가 흐름이다.​Step 3. 장단기 이동평균 그래프 확인전략 구현을 위해 단기(5일), 장기(30일) 이동평균을 구한다.두 이동평균 그래프를 함께 그리면 다음과 같다.2022년 그래프만 확대해 보자.이렇게 기존 데이터 테이블에 단기 이동평균 데이터와 장기 이동평균 데이터를 추가했다. 이제 각 날짜(인덱스) 별로 short_window가 long_window를 역전 상승하는 지점에서 ‘종가’(close)에 매수하고, short_window가 long_window를 역전 하강하는 지점에서 ‘종가’(close)에 매도할 것이다. 즉, 위 차트에서 파란색 선이 초록색 선을 넘은 가운데 기간만 주식을 보유한다.​Step 4. 보유 구간 표기주식을 들고 있을 구간과 그렇지 않은 구간을 마킹해준다.우리는 해당 날짜의 단기 이동 평균이 장기 이동 평균을 넘어서는 순간 매수할 것이다. 그리고 그 반대가 될 때 매도해야 한다. 위 데이터를 보면 1999년 12월 22일부터 단기 이동평균이 장기 이동평균을 넘어서 있다. 이런 경우는 매수 포인트에 해당하지 않는다. 우리는 False에서 True로 바뀌는 순간 매수하고, True에서 False로 바뀌는 순간 매도한다.표기는 0과 1로 바꿔주자. 0과 1을 사용해 주식을 들고 있을(파란색 선&amp;gt; 초록색 선) 구간을 1로, 그렇지 않은 나머지 구간을 0으로 표기한다. 자료형을 int로 바꿔주면 알아서 숫자로 변환된다.Step 5. 매매 시점 표기이번에는 매수, 매도 포인트를 마킹한다. overs 칼럼은 보유 기간을 알려준다. overs 값이 0에서 1로 전환되는 시점과 1에서 0으로 전환되는 시점을 체크하면 되겠다.이제 두 데이터(buy_signal, sell_signal)를 더해준다. 이렇게 하면 buy_signal에 매수 포인트가 아닌 지점과 sell_signal에 매도 포인트가 아닌 지점은 모두 0이기 때문에 합산했을 때 매수 포인트는 1로, 매도 포인트는 -1로, 그리고 나머지는 0으로 표시된다.앞에서 언급했지만 종목을 사지 않은 시작일부터 매도 신호가 포착되고 있다. 이것은 데이터가 short_window(단기 이평선)가 long_window(장기 이평선) 보다 큰 상태에서 시작했기 때문이다. 이렇게 매수 신호보다 먼저 나온 매도 신호는 0으로 수정해 주고 첫 매수 신호(1)가 나오면 그때부터 포지션을 잡는다.​Step 6. 전략 수익률 계산전략을 통해 보유한 기간에 대한 누적 수익률을 계산한다. 즉, signal ‘1(매수)’과 signal ‘-1(매도)’ 간의 수익률을 복리로 계산해 주면 전체 기간의 전략 수익률을 확인할 수 있다.보유기간별 각 수익률은 다음과 같다.보유기간을 차트에 표시해 본다. 붉은 영역이 매수 후 보유 기간이다.이번에도 2022년만 확대해서 이동평균선과 함께 확인해 보겠다. 주가는 붉은색 굵은 선으로 함께 그렸고, 보유기간은 역시 붉은 영역으로 표시했다.단순한 전략이지만 상승 기간에 잘 보유하고 있었다.이제 누적 수익률을 확인해 보자. numpy array는 누적 곱을 계산하는 cumprod() 함수를 제공하고 있다.전략은 총 129회 시행(매수~매도 사이클) 되었고 시행 마다의 수익률은 다음과 같다.위 수익률을 토대로 초깃값을 100만 원으로 세팅하면, 최종 수익률은 약 609만 원이 나온다.​이렇게 오르는 추세를 포착해서 매수, 매도를 반복하는 전략을 테스트해 봤다. 상대적으로 손실 폭을 줄일 수는 있었으나 상승 폭 역시 상당 부분 놓쳤기 때문에 장기적으로는 단순 보유보다 좋지 못한 결과를 보였다. 또한 매매수수료를 고려하지 않았다는 점에서 매매가 잦은 위와 같은 전략이 더 불리하게 작용할 수 있겠다.관성을 이기는 데이터" }, { "title": "키움증권 API를 활용한 비 실시간 주가 정보 수집 - 파이썬 데이터 수집 자동화", "url": "/posts/%ED%82%A4%EC%9B%80%EC%A6%9D%EA%B6%8C-API%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%9C-%EB%B9%84-%EC%8B%A4%EC%8B%9C%EA%B0%84-%EC%A3%BC%EA%B0%80-%EC%A0%95%EB%B3%B4-%EC%88%98%EC%A7%91-%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%88%98%EC%A7%91-%EC%9E%90%EB%8F%99%ED%99%94/", "categories": "3. 튜토리얼, 금융 분석 프로그래밍 응용", "tags": "", "date": "2021-09-02 00:00:00 +0900", "snippet": "키움 증권 API를 활용한 주식 정보 및 일봉 데이터 수집 자동화 pykiwoom 모듈 활용 : github/sharebook-kr/pykiwoom 코드 참고 : 퀀트투자를 위한 키움증권 API, 조대표 외 1명1. 개요1-1. 주식 데이터 수집 방식 비교주가 정보를 수집하기 위한 방법으로는 주로 금융사에서 제공하는 api방식과 웹스크래핑 방식이 있다. 금융사 api를 활용하면 주가 정보 외에도 일반적으로 주식앱에서 볼 수 있는 다양한 데이터를 불러올 수 있다. 또한, api에서는 실시간 데이터 조회 기능도 제공하기 때문에 매매자동화 등에 활용할 수 있다.1-2. pykiwoom일반적으로 파이썬으로 키움 api를 사용하는 경우 Python GUI 패키지인 PyQt5를 활용하게 된다. 그러나 실시간 시세 정보를 활용해 매매자동화 프로그램을 구현하는 경우가 아니라면 다른 사용자들이 미리 raw 코드를 wrapping 해놓은 코드, 즉 PyQt5 등으로 구현한 코드를 클래스 및 함수로 한번 감싸서 더 단순하게 사용할 수 있도록 한 코드를 이용해보는 것도 좋은 방법이다.여기서는 파이썬으로 배우는 알고리즘 트레이딩 저자로 유명한 조대표님이 구현해주신 오픈소스 pykiwoom 패키지를 사용한다.2. 로그인 및 접속 상태 조회2-1. 환경 설정본격적으로 코드 실습 전, 필요한 환경을 반드시 세팅해야 한다. window 10 이상(권장) python 3.7 이상(32bit 필수*) 키움증권 개좌 개설 및 API 사용 신청(필수) 키움증권 Open API+ 모듈 설치(필수) KOA Studio 설치(필수, 데이터 조회 키 및 자동로그인 설정용)python의 경우 대부분은 이미 64bit로 설치되어 있을 것이다. 만약 아나콘다가 설치되어있다면 간단히 32bit 가상환경을 새롭게 세팅해주면 된다. 쉘 명령어는 아래와 같다.set CONDA_FORCE_32BIT=1conda create -n py37_32 python=3.7 anaconda2-2. 자동 로그인 실행아래 코드는 자동 로그인 후 로그인 정보를 돌려준다.단, 자동 로그인을 가능하게 하기 위해서는 미리 KOA Studio에서 OpenAPI를 연결 후 윈도우 하단 상태바에 생긴 OpenAPI+ 아이콘을 우클릭하여 ‘계좌비밀번호 저장’을 선택하고, 비밀번호는 0000, AUTO를 체크하여 자동로그인이 가능하도록 설정하자.from pykiwoom.kiwoom import *import timeimport pandas as pd# loginkiwoom = Kiwoom() # allocationprint(&#39;login..&#39;)kiwoom.CommConnect(block=True) # waiting to loginprint(&#39;login..complete&#39;)# connectstate = kiwoom.GetConnectState()if state == 0: print(&#39;not connected&#39;)elif state == 1: print(&#39;status : connected&#39;)# login - informationaccount_num = kiwoom.GetLoginInfo(&#39;ACCOUNT_CNT&#39;) # number of accountaccounts = kiwoom.GetLoginInfo(&#39;ACCNO&#39;) # list of accountuser_id = kiwoom.GetLoginInfo(&#39;USER_ID&#39;) # user iduser_name = kiwoom.GetLoginInfo(&#39;USER_NAME&#39;) # user nameprint(&#39;number of account : {}&#39;.format(account_num))#print(&#39;acounts : {}&#39;.format(accounts))#print(&#39;user id : {}&#39;.format(user_id)) #print(&#39;user name : {}&#39;.format(user_name))login..login..completestatus : connectednumber of account : 23. 기본 종목 정보 수집3-1. 종목 코드 수집아래 코드를 실행하면 코스피, 코스닥, ETF에 포함된 모든 종목 코드를 불러올 수 있다. 수집된 종목 코드를 활용하면 코스피 전체 데이터를 가져오는 등 리스트 형태로 유용하게 활용할 수 있다.# kospi : 0# kosdaq : 10# etf : 8code_kospi = kiwoom.GetCodeListByMarket(&#39;0&#39;) code_kosdaq = kiwoom.GetCodeListByMarket(&#39;10&#39;) code_etf = kiwoom.GetCodeListByMarket(&#39;8&#39;) print(&#39;number of kospi code : {}&#39;.format(len(code_kospi)))print(&#39;number of kosdaq code : {}&#39;.format(len(code_kosdaq)))print(&#39;number of etf code : {}&#39;.format(len(code_etf)))number of kospi code : 1634number of kosdaq code : 1511number of etf code : 5003-2. 개별 종목 정보 수집pykiwoom 패키지를 통해 간편하게 기업명, 전일가 등을 불러올 수 있다. 아래 코드는 기본적인 종목 정보를 불러온다.# 카카오 : 035720corp = kiwoom.GetMasterCodeName(&#39;035720&#39;)con = kiwoom.GetMasterConstruction(&#39;035720&#39;)listed_d = kiwoom.GetMasterListedStockDate(&#39;035720&#39;)prev_price = kiwoom.GetMasterLastPrice(&#39;035720&#39;)state = kiwoom.GetMasterStockState(&#39;035720&#39;)print(&#39;기업 : {}&#39;.format(corp))print(&#39;감리구분 : {}&#39;.format(con))print(&#39;최초상장일 : {}&#39;.format(listed_d))print(&#39;전일가 : {}&#39;.format(prev_price))print(&#39;종목상태 : {}&#39;.format(state))기업 : 카카오감리구분 : 정상최초상장일 : 2017-07-10 00:00:00전일가 : 148500종목상태 : [&#39;증거금20%&#39;, &#39;담보대출&#39;, &#39;신용가능&#39;] 여기서 ‘최초상장일’은 카카오가 현재 상장된 시장인 ‘코스피’에 최초로 상장된 날짜를 의미다.4. 주가 정보 수집4-1. 개별 주식 정보 수집특정 종목코드에 해당하는 주식 정보를 불러오고 싶다면 ‘opt10001’를 호출한다.‘opt10001’은 가장 기본적인 주식 정보를 반환하는 TR key다. TR이란 api를 통해 요청하는 transaction을 의미하며, 딕셔너리 형태로 구성되어 있기 때문에 ‘opt10001’이라는 key를 입력값으로 전달하면 된다. KOA Studio를 통해 이 외에도 다양하고 유용한 TR을 살펴볼 수 있다.# opt10001 : (TR)주식기본정보요청# single data (has single row)df_single = kiwoom.block_request(&#39;opt10001&#39;, 종목코드=&#39;035720&#39;, output=&#39;주식기본정보&#39;, next=0 # 0 : single transaction )# 출력df_single   종목코드 종목명 결산월 액면가 자본금 상장주식 신용비율 연중최고 연중최저 시가총액 … 250최저가대비율 현재가 대비기호 전일대비 등락율 거래량 거래대비 액면가단위 유통주식 유통비율 0 035720 카카오 12 100 445 444707 +0.49 +561000 -108000 662614 … +37.96 +149000. 2 +500 +0.34 2017066 -75.31 원 324274 73.0 1 rows × 45 columnsdf_single.T   0 종목코드 035720 종목명 카카오 결산월 12 액면가 100 자본금 445 상장주식 444707 신용비율 +0.49 연중최고 +561000 연중최저 -108000 시가총액 662614 시가총액비중   외인소진률 +31.95 대용가 112860 PER 422.20 EPS 353 ROE 2.7 PBR 10.47 EV 87.91 BPS 14237 매출액 41568 영업이익 4559 당기순이익 1734 250최고 +561000. 250최저 -108000 시가 +150000 고가 +150500 저가 -148000 상한가 +193000 하한가 -104000 기준가 148500 예상체결가 +149500 예상체결수량 1203 250최고가일 20210408 250최고가대비율 -73.44 250최저가일 20210513 250최저가대비율 +37.96 현재가 +149000 대비기호 2 전일대비 +500 등락율 +0.34 거래량 2017066 거래대비 -75.31 액면가단위 원 유통주식 324274 유통비율 73.0 4-2. 개별 주식 일봉 차트 조회일명 캔들이라 불리는 일봉 차트를 조회하는 방법은 아래와 같다.일봉차트 정보는 매일 거래된 정보에 대해 시가,고가,저가,현재가(종가)를 담고 있으며 일반적으로 투자자들이 주가 추세를 확인할 때 보는 가장 기본적인 데이터다.# opt10081 : (TR)주식일봉차트조회요청# multi-data (has multiple rows)df_multi = kiwoom.block_request(&#39;opt10081&#39;, 종목코드=&#39;035720&#39;, 기준일자=&#39;20210824&#39;, # 기준일로부터 총 600일(변동가능) 수정주가구분=1, # 수정주가 적용(액분 등 반영) output=&#39;주식일봉차트조회&#39;, # multi-data next=0 # 0 : single transaction(단일 요청) )# 기준일자 &#39;2021년 8월 24일&#39; 포함 지난 600일 일봉 데이터 출력df_multi   종목코드 현재가 거래량 거래대금 일자 시가 고가 저가 수정주가구분 수정비율 대업종구분 소업종구분 종목정보 수정주가이벤트 전일종가 0 035720 149000 2017066 300363 20210824 150000 150500 148000               1   148500 2678395 393910 20210823 146000 148500 144500               2   144000 2776813 403451 20210820 146500 149000 143000               3   146500 2813914 412982 20210819 144000 149000 144000               4   145500 2720552 392335 20210818 142000 146500 141500               … … … … … … … … … … … … … … … … 595   20672 1076891 22260 20190401 20772 20873 20471               596   20772 766432 15861 20190329 20672 20873 20572               597   20572 811988 16766 20190328 20471 20873 20471               598   20572 1039895 21532 20190327 20973 20973 20471               599   20873 1282770 26846 20190326 20572 21174 20471               600 rows × 15 columns키움 API는 기본적으로 일봉차트조회시 총 600줄(600일)에 해당하는 정보를 돌려준다. 따라서 600일이 넘는 일별 거래 정보가 필요한 경우 반복문을 통해 코딩을 해줘야 한다. pykiwoom 패키지를 사용하는 경우 tr_remained 모듈로 아래와 같이 연속 조회가 가능하다.# opt10081 : (TR)주식일봉차트조회요청# multi-data (has multiple rows)# 연속 조회 방식 (while kiwoom.tr_record)tr = &quot;opt10081&quot;code = &quot;035720&quot;set_d = &#39;20210824&#39;df_list = []df_firstblock = kiwoom.block_request(tr, 종목코드=code, 기준일자=set_d, 수정주가구분=1, output=&quot;주식일봉차트조회&quot;, next=0)df_list.append(df_firstblock)print(&#39;데이터 수집 시작.. ({}~)&#39;.format(df_firstblock.loc[0,&#39;일자&#39;]))print(&#39;데이터 수집 중.. (~{})&#39;.format(df_firstblock.loc[len(df_firstblock)-1,&#39;일자&#39;]))# 남은 데이터가 있다면 실행while kiwoom.tr_remained: df_remainblock = kiwoom.block_request(tr, 종목코드=code, 기준일자=set_d, 수정주가구분=1, output=&quot;주식일봉차트조회&quot;, next=2) df_list.append(df_remainblock) time.sleep(1) print(&#39;데이터 수집 중.. (~{})&#39;.format(df_remainblock.loc[len(df_remainblock)-1,&#39;일자&#39;])) if kiwoom.tr_remained == False: print(&#39;데이터 수집 완료&#39;)df_all = pd.concat(df_list)df_all.reset_index(drop=True, inplace=True)데이터 수집 시작.. (20210824~)데이터 수집 중.. (~20190326)데이터 수집 중.. (~20161011)데이터 수집 중.. (~20140429)데이터 수집 중.. (~20111128)데이터 수집 중.. (~20090707)데이터 수집 중.. (~20070201)데이터 수집 중.. (~20040903)데이터 수집 중.. (~20020329)데이터 수집 중.. (~19991111)데이터 수집 완료df_all   종목코드 현재가 거래량 거래대금 일자 시가 고가 저가 수정주가구분 수정비율 대업종구분 소업종구분 종목정보 수정주가이벤트 전일종가 0 035720 149000 2017066 300363 20210824 150000 150500 148000               1   148500 2678395 393910 20210823 146000 148500 144500               2   144000 2776813 403451 20210820 146500 149000 143000               3   146500 2813914 412982 20210819 144000 149000 144000               4   145500 2720552 392335 20210818 142000 146500 141500               … … … … … … … … … … … … … … … … 5374   1654 2021 3 19991117 1654 1654 1654               5375   1479 2265 3 19991116 1479 1479 1479               5376   1323 4286 6 19991115 1323 1323 1323               5377   1181 1481 2 19991112 1181 1181 1181               5378   1058 127 0 19991111 1058 1058 1058               5379 rows × 15 columns여기까지 pykiwoom 패키지를 활용해 키움 openapi에 자동으로 로그인하여 가장 기본적인 종목 정보와 개별 주가 데이터를 수집해보았다.사실, 이러한 작업은 단순히 데이터를 수집하는 목적이라면 큰 의미가 없다. 증권사 앱 혹은 네이버 검색만으로 주가정보는 너무나 편리하게 확인할 수 있기 때문이다. 심지어 자체 DB를 구축한다 하더라도 일자별 거래데이터의 경우 yahoofinance와 같은 더 편리한 파이썬 패키지가 있기 때문에 굳이 금융사 api로 접근해서 데이터를 불러오지 않아도 된다.관성을 이기는 데이터" }, { "title": "넷플릭스의 콘텐츠 보유 현황과 수급 전략 분석 - 파이썬 비즈니스 데이터 분석", "url": "/posts/%EB%84%B7%ED%94%8C%EB%A6%AD%EC%8A%A4%EC%9D%98-%EC%BD%98%ED%85%90%EC%B8%A0-%EB%B3%B4%EC%9C%A0-%ED%98%84%ED%99%A9%EA%B3%BC-%EC%88%98%EA%B8%89-%EC%A0%84%EB%9E%B5-%EB%B6%84%EC%84%9D-%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EB%B9%84%EC%A6%88%EB%8B%88%EC%8A%A4-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%B6%84%EC%84%9D/", "categories": "2. 도메인, 비즈니스", "tags": "", "date": "2021-09-02 00:00:00 +0900", "snippet": "넷플릭스의 콘텐츠 보유 현황과 수급 전략 분석 분석 목표 : 넷플릭스에서 보유한 콘텐츠 현황과 최근 집중하는 콘텐츠 유형을 파악해 넷플릭스의 시장 점유를 위한 콘텐츠 수급 전략을 분석한다. 데이터 출처 : Netflix Movies and TV Shows, Kaggle 참고 사이트 : 영화, TV 관람 등급 표1. 가설 설정각 컬럼별 유효 데이터와 결측치 분포를 확인하고, 이에 따라 분석 목표에 도달하기 위한 가설을 설정한다.import pandas as pdimport numpy as npimport plotly.express as pximport plotly.graph_objects as gofrom plotly.subplots import make_subplotsimport matplotlib.pyplot as pltimport seaborn as snsimport warningswarnings.filterwarnings(action=&#39;ignore&#39;)netflix_r = pd.read_csv(&#39;./data/netflix_titles.csv&#39;)netflix_r   show_id type title director cast country date_added release_year rating duration listed_in description 0 s1 TV Show 3% NaN João Miguel, Bianca Comparato, Michel Gomes, R… Brazil August 14, 2020 2020 TV-MA 4 Seasons International TV Shows, TV Dramas, TV Sci-Fi &amp;amp;… In a future where the elite inhabit an island … 1 s2 Movie 7:19 Jorge Michel Grau Demián Bichir, Héctor Bonilla, Oscar Serrano, … Mexico December 23, 2016 2016 TV-MA 93 min Dramas, International Movies After a devastating earthquake hits Mexico Cit… 2 s3 Movie 23:59 Gilbert Chan Tedd Chan, Stella Chung, Henley Hii, Lawrence … Singapore December 20, 2018 2011 R 78 min Horror Movies, International Movies When an army recruit is found dead, his fellow… 3 s4 Movie 9 Shane Acker Elijah Wood, John C. Reilly, Jennifer Connelly… United States November 16, 2017 2009 PG-13 80 min Action &amp;amp; Adventure, Independent Movies, Sci-Fi… In a postapocalyptic world, rag-doll robots hi… 4 s5 Movie 21 Robert Luketic Jim Sturgess, Kevin Spacey, Kate Bosworth, Aar… United States January 1, 2020 2008 PG-13 123 min Dramas A brilliant group of students become card-coun… … … … … … … … … … … … … … 7782 s7783 Movie Zozo Josef Fares Imad Creidi, Antoinette Turk, Elias Gergi, Car… Sweden, Czech Republic, United Kingdom, Denmar… October 19, 2020 2005 TV-MA 99 min Dramas, International Movies When Lebanon’s Civil War deprives Zozo of his … 7783 s7784 Movie Zubaan Mozez Singh Vicky Kaushal, Sarah-Jane Dias, Raaghav Chanan… India March 2, 2019 2015 TV-14 111 min Dramas, International Movies, Music &amp;amp; Musicals A scrappy but poor boy worms his way into a ty… 7784 s7785 Movie Zulu Man in Japan NaN Nasty C NaN September 25, 2020 2019 TV-MA 44 min Documentaries, International Movies, Music &amp;amp; M… In this documentary, South African rapper Nast… 7785 s7786 TV Show Zumbo’s Just Desserts NaN Adriano Zumbo, Rachel Khoo Australia October 31, 2020 2019 TV-PG 1 Season International TV Shows, Reality TV Dessert wizard Adriano Zumbo looks for the nex… 7786 s7787 Movie ZZ TOP: THAT LITTLE OL’ BAND FROM TEXAS Sam Dunn NaN United Kingdom, Canada, United States March 1, 2020 2019 TV-MA 90 min Documentaries, Music &amp;amp; Musicals This documentary delves into the mystique behi… 7787 rows × 12 columnsnetflix_r.isnull().sum()show_id 0type 0title 0director 2389cast 718country 507date_added 10release_year 0rating 7duration 0listed_in 0description 0dtype: int64sns.heatmap(netflix_r.isnull()); show_id : 해당 데이터셋 이외에 병합할 테이블이 없기 때문에 id 컬럼은 제거한다. director: 감독에 대한 정보는 고려하지 않겠다. cast: 배우 역시 고려하지 않겠다.본 데이터에서 검증하고자 하는 가설은 아래와 같다.(H1 = 연구가설) H1 : 최근 특정 콘텐츠 유형(TV show or Movie)이 늘고 있다. (주요 컬럼 : type) H1 : 최근 특정 국가의 콘텐츠 비중이 늘고 있다. (주요 컬럼 : country) H1 : 최근 옛날(오래된) 콘텐츠 수급 비중이 늘고 있다. (주요 컬럼 : release_year) H1 : 최근 타겟층(관람등급)이 명확한 콘텐츠 비중이 늘고 있다. (주요 컬럼 : rating) H1 : 최근 길이(시리즈, 시간)가 짧은 콘텐츠 비중이 늘고 있다. (주요 컬럼 : duration) H1 : 최근 유통하는 콘텐츠 장르가 더 다양해지고 있다. (주요 컬럼 : listed_in, description)위 순서로 가설을 검증해나가며 넷플릭스의 시장 점유 전략에 대한 최종 결론을 도출하고자 한다.2. 기본 전처리본격적인 분석에 앞서 미리 처리해야할 사항은 아래와 같다. 분석에 사용할 컬럼만 가져온다. 사용하지 않을 행은 제거한다.(시간 누락) 추세 확인을 위해 시계열 데이터로 변환한다. 과거와 현재를 상대적으로 비교하기 위해 구간(시간)을 분할한다.netflix_r.head(3)   show_id type title director cast country date_added release_year rating duration listed_in description 0 s1 TV Show 3% NaN João Miguel, Bianca Comparato, Michel Gomes, R… Brazil August 14, 2020 2020 TV-MA 4 Seasons International TV Shows, TV Dramas, TV Sci-Fi &amp;amp;… In a future where the elite inhabit an island … 1 s2 Movie 7:19 Jorge Michel Grau Demián Bichir, Héctor Bonilla, Oscar Serrano, … Mexico December 23, 2016 2016 TV-MA 93 min Dramas, International Movies After a devastating earthquake hits Mexico Cit… 2 s3 Movie 23:59 Gilbert Chan Tedd Chan, Stella Chung, Henley Hii, Lawrence … Singapore December 20, 2018 2011 R 78 min Horror Movies, International Movies When an army recruit is found dead, his fellow… 2-1. 불필요한 컬럼 제거사용하지 않을 아래 컬럼들을 제거한다. show_id director castnflx = netflix_r.copy()nflx.drop([&#39;show_id&#39;,&#39;director&#39;,&#39;cast&#39;], axis=1, inplace=True)nflx.head(3)   type title country date_added release_year rating duration listed_in description 0 TV Show 3% Brazil August 14, 2020 2020 TV-MA 4 Seasons International TV Shows, TV Dramas, TV Sci-Fi &amp;amp;… In a future where the elite inhabit an island … 1 Movie 7:19 Mexico December 23, 2016 2016 TV-MA 93 min Dramas, International Movies After a devastating earthquake hits Mexico Cit… 2 Movie 23:59 Singapore December 20, 2018 2011 R 78 min Horror Movies, International Movies When an army recruit is found dead, his fellow… 2-2. 불필요한 행 제거nflx.isnull().sum()type 0title 0country 507date_added 10release_year 0rating 7duration 0listed_in 0description 0dtype: int64모든 가설은 넷플릭스에 콘텐츠가 올라간 시간에 기준을 두고 있고, 최근 전략을 파악하고자 하는 것이 분석 목표이므로 시간 정보(date_added)가 없는 행은 과감하게 삭제하도록 한다.nflx[&#39;date_added&#39;].isnull().sum()10nflx = nflx.loc[nflx[&#39;date_added&#39;].isnull()==False]nflx[&#39;date_added&#39;].isnull().sum()02-3. 데이터 타입 변환nflx.dtypestype objecttitle objectcountry objectdate_added objectrelease_year int64rating objectduration objectlisted_in objectdescription objectdtype: object날짜 데이터는 datetime 타입으로, 콘텐츠 길이(n부작, 시간)는 숫자로 변환한다. date_added, release_year -&amp;gt; datetime duration -&amp;gt; intnflx[&#39;date_added&#39;] = nflx[&#39;date_added&#39;].astype(&#39;datetime64&#39;)nflx[&#39;date_added&#39;]0 2020-08-141 2016-12-232 2018-12-203 2017-11-164 2020-01-01 ... 7782 2020-10-197783 2019-03-027784 2020-09-257785 2020-10-317786 2020-03-01Name: date_added, Length: 7777, dtype: datetime64[ns]# 최초 출시일에 월/일 정보가 누락되어 있으므로 1월 1일로 일괄 대체한다.(넷플릭스에서 유통된 연도가 같은 경우 시간상 역행이 발생하지 않도록 하기 위함)nflx[&#39;release_year&#39;] = pd.to_datetime(nflx[&#39;release_year&#39;], format=&#39;%Y&#39;)nflx[&#39;release_year&#39;]0 2020-01-011 2016-01-012 2011-01-013 2009-01-014 2008-01-01 ... 7782 2005-01-017783 2015-01-017784 2019-01-017785 2019-01-017786 2019-01-01Name: release_year, Length: 7777, dtype: datetime64[ns]콘텐츠 길이는 콘텐츠 타입에 따라 형식이 다르다. type이 TV Show라면 단일 시리즈인 경우 1 Season, 연속 시리즈인 경우 N Seasons로 표기된다. type이 TV Show지만 duration 단위가 Season 혹은 Seasons가 아닌 예외 케이스는 없다.nflx.loc[nflx[&#39;type&#39;]==&#39;TV Show&#39;,&#39;duration&#39;]0 4 Seasons5 1 Season11 1 Season12 1 Season16 1 Season ... 7767 1 Season7775 1 Season7777 2 Seasons7779 1 Season7785 1 SeasonName: duration, Length: 2400, dtype: object# 1 Season 혹은 N Seasons로 표기되지 않은 예외케이스 갯수 확인(nflx.loc[nflx[&#39;type&#39;]==&#39;TV Show&#39;,&#39;duration&#39;].str.contains(&#39;Season&#39;)==False).sum()0# 단위(Seasons, Season) 제거nflx.loc[nflx[&#39;type&#39;]==&#39;TV Show&#39;,&#39;duration&#39;] = nflx.loc[nflx[&#39;type&#39;]==&#39;TV Show&#39;,&#39;duration&#39;].apply(lambda x : x.split(&quot; &quot;)[0])nflx.loc[nflx[&#39;type&#39;]==&#39;TV Show&#39;,&#39;duration&#39;]0 45 111 112 116 1 ..7767 17775 17777 27779 17785 1Name: duration, Length: 2400, dtype: object type이 Movie라면 모두 min으로 표기한다. type이 Movie이지만 duration 단위가 min이 아닌 예외 케이스는 없다.nflx.loc[nflx[&#39;type&#39;]==&#39;Movie&#39;,&#39;duration&#39;]1 93 min2 78 min3 80 min4 123 min6 95 min ... 7781 88 min7782 99 min7783 111 min7784 44 min7786 90 minName: duration, Length: 5377, dtype: object# min으로 표기되지 않은 예외케이스 갯수 확인(nflx.loc[nflx[&#39;type&#39;]==&#39;Movie&#39;,&#39;duration&#39;].str.contains(&#39;min&#39;)==False).sum()0# 단위(min) 제거nflx.loc[nflx[&#39;type&#39;]==&#39;Movie&#39;,&#39;duration&#39;] = nflx.loc[nflx[&#39;type&#39;]==&#39;Movie&#39;,&#39;duration&#39;].apply(lambda x : x.split(&quot; &quot;)[0])nflx.loc[nflx[&#39;type&#39;]==&#39;Movie&#39;,&#39;duration&#39;]1 932 783 804 1236 95 ... 7781 887782 997783 1117784 447786 90Name: duration, Length: 5377, dtype: objectnflx[&#39;duration&#39;] = nflx[&#39;duration&#39;].astype(&#39;int64&#39;)# 데이터 확인nflx.head(3)   type title country date_added release_year rating duration listed_in description 0 TV Show 3% Brazil 2020-08-14 2020-01-01 TV-MA 4 International TV Shows, TV Dramas, TV Sci-Fi &amp;amp;… In a future where the elite inhabit an island … 1 Movie 7:19 Mexico 2016-12-23 2016-01-01 TV-MA 93 Dramas, International Movies After a devastating earthquake hits Mexico Cit… 2 Movie 23:59 Singapore 2018-12-20 2011-01-01 R 78 Horror Movies, International Movies When an army recruit is found dead, his fellow… 2-3. 시계열 데이터 변환앞서 설계한 모든 가설은 ‘최근’이라는 시간 축을 기준으로 하고 있다. 모든 분석 방향이 과거와 현재를 비교하는 과정이 될 것이므로 콘텐츠가 넷플릭스에 추가된 날짜를 인덱스로 하여 시계열 데이터로 변환한다.nflx = nflx.set_index(&#39;date_added&#39;).sort_index()nflx   type title country release_year rating duration listed_in description date_added                 2008-01-01 Movie To and From New York United States 2006-01-01 TV-MA 81 Dramas, Independent Movies, Thrillers While covering a story in New York City, a Sea… 2008-02-04 TV Show Dinner for Five United States 2007-01-01 TV-MA 1 Stand-Up Comedy &amp;amp; Talk Shows In each episode, four celebrities join host Jo… 2009-05-05 Movie Just Another Love Story Denmark 2007-01-01 TV-MA 104 Dramas, International Movies When he causes a car accident that leaves a yo… 2009-11-18 Movie Splatter United States 2009-01-01 TV-MA 29 Horror Movies After committing suicide, a washed-up rocker r… 2010-11-01 Movie Mad Ron’s Prevues from Hell United States 1987-01-01 NR 84 Cult Movies, Horror Movies This collection cherry-picks trailers, forgott… … … … … … … … … … 2021-01-15 Movie Wish You NaN 2021-01-01 TV-PG 102 Dramas, International Movies, LGBTQ Movies Singing and dreaming together, a talented sing… 2021-01-15 TV Show The Magicians United States, Canada 2020-01-01 TV-MA 5 TV Dramas, TV Sci-Fi &amp;amp; Fantasy When grad student Quentin Coldwater enters a c… 2021-01-16 Movie Death of Me United States, Thailand 2020-01-01 R 94 Horror Movies With no memory of the previous night, a vacati… 2021-01-16 Movie Radium Girls United States 2018-01-01 TV-14 103 Dramas, Independent Movies When the women at a radium factory begin to fa… 2021-01-16 Movie A Monster Calls United Kingdom, Spain, United States 2016-01-01 PG-13 108 Dramas, Sci-Fi &amp;amp; Fantasy Overwhelmed by his mother’s illness, a young b… 7777 rows × 8 columnsnflx.dtypestype objecttitle objectcountry objectrelease_year datetime64[ns]rating objectduration int64listed_in objectdescription objectdtype: object2-4. 데이터 구간(시간) 분할nflx[&#39;title&#39;].resample(&#39;Q&#39;).count().plot();nflx[&#39;title&#39;].resample(&#39;Y&#39;).count()date_added2008-12-31 22009-12-31 22010-12-31 12011-12-31 132012-12-31 32013-12-31 112014-12-31 252015-12-31 882016-12-31 4432017-12-31 12252018-12-31 16852019-12-31 21532020-12-31 20092021-12-31 117Freq: A-DEC, Name: title, dtype: int64넷플릭스는 초창기부터 서서히 콘텐츠를 늘려왔고, 2016년~2016년을 기점으로 폭발적으로 성장했다. 현재 데이터는 시간 별 분포 차이가 심하기 때문에 시간 축을 그대로 사용하는 것은 바람직하지 않다. 전체 기간을 몇 개의 구간으로 분할하여 위와 같이 구간(Y) 별로 너무 많은 데이터가 몰리거나, 너무 적은 데이터만 취합되지 않도록 하자.# 0부터 7까지, 각각 1111개씩 채워진 리스트 생성seps = []for i in range(1,7+1): sep = np.ones(1111,dtype=np.int8)*i seps.extend(sep)print(len(seps))7777# seps는 숫자로서의 의미가 없다. string으로 변환한다.nflx[&#39;seps&#39;] = sepsnflx[&#39;seps&#39;] = nflx[&#39;seps&#39;].astype(&#39;str&#39;)nflx.dtypestype objecttitle objectcountry objectrelease_year datetime64[ns]rating objectduration int64listed_in objectdescription objectseps objectdtype: objectcount_seps = nflx.groupby(&#39;seps&#39;).count()[&#39;title&#39;]plt.figure(figsize=(8,4))sns.barplot(x=count_seps.index ,y=count_seps.values)plt.ylabel(&#39;count&#39;)plt.xlabel(&#39;period(past --&amp;gt; now)&#39;)plt.show()총 7777개의 데이터를 시간 순으로 1111개씩 분할했다. seps가 1에 가까울수록 최근으로부터 멀어진다.3. 가설 검정아래 6가지 가설에 대하여 순차적으로 검증하고, 넷플릭스의 수급 전략을 분석한다. H1 : 최근 특정 콘텐츠 유형(TV show or Movie)이 늘고 있다. (주요 컬럼 : type) H1 : 최근 특정 국가의 콘텐츠 수급 비중이 늘고 있다. (주요 컬럼 : country) H1 : 최근 옛날(오래된) 콘텐츠 수급 비중이 늘고 있다. (주요 컬럼 : release_year) H1 : 최근 어린이 콘텐츠 비중이 줄고 있다. (주요 컬럼 : rating) H1 : 최근 길이(시리즈, 시간)가 짧은 콘텐츠 비중이 늘고 있다. (주요 컬럼 : duration) H1 : 최근 유통하는 콘텐츠 장르가 더 다양해지고 있다. (주요 컬럼 : listed_in, description)3-1. 특정 콘텐츠 유형에 대한 집중 수급 여부 분석최근 TV 시리즈물의 퀄리티나 제작 속도는 영화 산업의 발전 속도를 따라잡고 있다. 넷플릭스가 TV 시리즈물과 영화 중 어떤 콘텐츠에 더 집중하고 있는지 확인하고자 한다. H0 : 콘텐츠 유형(TV show or Movie)별 점유율은 전과 동일하다. H1 : 최근 특정 콘텐츠 유형(TV show or Movie)이 늘고 있다.nflx.groupby([&#39;seps&#39;,&#39;type&#39;])[&#39;title&#39;].count().reset_index()   seps type title 0 1 Movie 744 1 1 TV Show 367 2 2 Movie 802 3 2 TV Show 309 4 3 Movie 829 5 3 TV Show 282 6 4 Movie 779 7 4 TV Show 332 8 5 Movie 741 9 5 TV Show 370 10 6 Movie 764 11 6 TV Show 347 12 7 Movie 718 13 7 TV Show 393 count_type = pd.DataFrame(nflx.groupby([&#39;seps&#39;,&#39;type&#39;]).count()[&#39;title&#39;])count_type.columns = [&#39;count&#39;]count_type     count seps type   1 Movie 744 TV Show 367   2 Movie 802 TV Show 309   3 Movie 829 TV Show 282   4 Movie 779 TV Show 332   5 Movie 741 TV Show 370   6 Movie 764 TV Show 347   7 Movie 718 TV Show 393   count_type = pd.pivot_table(count_type, index=&#39;seps&#39;, columns=&#39;type&#39;)count_type.columns=[&#39;movie&#39;, &#39;tv&#39;]count_type   movie tv seps     1 744 367 2 802 309 3 829 282 4 779 332 5 741 370 6 764 347 7 718 393 list(np.arange(1,8))[1, 2, 3, 4, 5, 6, 7]plt.figure(figsize=(8,4))sns.lineplot(data=count_type, x=count_type.index, y=count_type[&#39;movie&#39;], label=&#39;Movie&#39;)sns.lineplot(data=count_type, x=count_type.index, y=count_type[&#39;tv&#39;], label=&#39;TV Show&#39;)plt.legend()plt.ylabel(&#39;the number of contents&#39;)plt.xlabel(&#39;period(past.. 1---&amp;gt;7 ..now)&#39;)plt.show()결론 :최근 영화보다 TV 시리즈가 늘고 있다. 귀무가설을 기각한다.특히 넷플릭스 서비스 초기보다 현 시점에서 TV 시리즈가 늘고 있는 것에 의미가 있다. 초기 영화 수급이 어려웠던 때보다 최근 자본이 받쳐주는 상황에서도 TV 시리즈 보유량을 늘리는 것을 보면 TV 시리즈에 대한 강한 시장 수요를 간접적으로 이해할 수 있다.3-2. 특정 국가를 겨냥한 콘텐츠 현황 분석콘텐츠 플랫폼의 경우 특정 국가 진입 및 초기 점유율 확보를 위해 해당 국가에서 생산한 콘텐츠를 전략적으로 확보하는 경향이 있다. 국가별 콘텐츠 비중을 살펴보고 최근 순위나 점유율의 변동이 있는지 살펴보도록 하자. H0 : 국가별 콘텐츠 수급 비중은 전과 동일하다. H1 : 최근 특정 국가의 콘텐츠 수급 비중이 늘고 있다.# 가장 최근 확보한 1111개 콘텐츠의 제작 국가별 비중nflx.loc[nflx[&#39;seps&#39;]==&#39;7&#39;,&#39;country&#39;].value_counts()[:10]United States 385India 112United Kingdom 38Japan 31Canada 29Philippines 25South Korea 24Spain 16Nigeria 15Indonesia 15Name: country, dtype: int64top10_countries = nflx.loc[nflx[&#39;seps&#39;]==&#39;7&#39;,&#39;country&#39;].value_counts()[:10].indextop10_countriesIndex([&#39;United States&#39;, &#39;India&#39;, &#39;United Kingdom&#39;, &#39;Japan&#39;, &#39;Canada&#39;, &#39;Philippines&#39;, &#39;South Korea&#39;, &#39;Spain&#39;, &#39;Nigeria&#39;, &#39;Indonesia&#39;], dtype=&#39;object&#39;)seps_by_top10 = nflx.loc[nflx[&#39;country&#39;].isin(top10_countries),[&#39;country&#39;,&#39;seps&#39;]]seps_by_top10   country seps date_added     2008-01-01 United States 1 2008-02-04 United States 1 2009-11-18 United States 1 2010-11-01 United States 1 2011-09-27 United States 1 … … … 2021-01-15 United States 7 2021-01-15 United States 7 2021-01-15 United States 7 2021-01-15 United States 7 2021-01-16 United States 7 4798 rows × 2 columnsseps_by_top10.loc[seps_by_top10[&#39;country&#39;] == top10_countries[0]].groupby(&#39;seps&#39;).count()   country seps   1 415 2 332 3 285 4 336 5 434 6 362 7 385 top10_countriesIndex([&#39;United States&#39;, &#39;India&#39;, &#39;United Kingdom&#39;, &#39;Japan&#39;, &#39;Canada&#39;, &#39;Philippines&#39;, &#39;South Korea&#39;, &#39;Spain&#39;, &#39;Nigeria&#39;, &#39;Indonesia&#39;], dtype=&#39;object&#39;)plt.figure(figsize=(12,8))for country in list(top10_countries): count_country = seps_by_top10.loc[seps_by_top10[&#39;country&#39;] == country].groupby(&#39;seps&#39;).count() sns.lineplot(data=count_country, x=count_country.index, y=count_country[&#39;country&#39;], label=country)plt.legend()plt.ylabel(&#39;The Count of Countries&#39;)plt.xlabel(&#39;period(past.. 1---&amp;gt;7 ..now)&#39;)plt.show()결론 :국가별 콘텐츠 수급 비중은 전과 다르다. 귀무가설을 기각한다.한 때, 인도 영화 수급 비중이 미국 본토 콘텐츠 수급 비중과 유사했던 적도 있으나 현재 미국 콘텐츠에 더 집중하고 있는 것으로 보인다. 동시에 영국 콘텐츠 수급 비중을 점차 줄여나가고 있다.3-3. 콘텐츠 수급 대상 연도 분석최근 국내에서는 과거 히트작이 재상영되거나 유튜브 등을 통해 다시 회자되는 시청 트랜드가 생겨나고 있다. 전 세계적으로 이러한 현상이 나타나고 있다면 넷플릭스에서도 최신 영화, 최신 드라마만 집중하지 않고 더 넓은 범위의 연도를 고려할 수 있다. H0 : 최근 옛날(오래된) 콘텐츠 수급 비중이 늘고 있다. H1 : 최근 옛날(오래된) 콘텐츠 수급 비중이 늘고 있다.nflx   type title country release_year rating duration listed_in description seps date_added                   2008-01-01 Movie To and From New York United States 2006-01-01 TV-MA 81 Dramas, Independent Movies, Thrillers While covering a story in New York City, a Sea… 1 2008-02-04 TV Show Dinner for Five United States 2007-01-01 TV-MA 1 Stand-Up Comedy &amp;amp; Talk Shows In each episode, four celebrities join host Jo… 1 2009-05-05 Movie Just Another Love Story Denmark 2007-01-01 TV-MA 104 Dramas, International Movies When he causes a car accident that leaves a yo… 1 2009-11-18 Movie Splatter United States 2009-01-01 TV-MA 29 Horror Movies After committing suicide, a washed-up rocker r… 1 2010-11-01 Movie Mad Ron’s Prevues from Hell United States 1987-01-01 NR 84 Cult Movies, Horror Movies This collection cherry-picks trailers, forgott… 1 … … … … … … … … … … 2021-01-15 Movie Wish You NaN 2021-01-01 TV-PG 102 Dramas, International Movies, LGBTQ Movies Singing and dreaming together, a talented sing… 7 2021-01-15 TV Show The Magicians United States, Canada 2020-01-01 TV-MA 5 TV Dramas, TV Sci-Fi &amp;amp; Fantasy When grad student Quentin Coldwater enters a c… 7 2021-01-16 Movie Death of Me United States, Thailand 2020-01-01 R 94 Horror Movies With no memory of the previous night, a vacati… 7 2021-01-16 Movie Radium Girls United States 2018-01-01 TV-14 103 Dramas, Independent Movies When the women at a radium factory begin to fa… 7 2021-01-16 Movie A Monster Calls United Kingdom, Spain, United States 2016-01-01 PG-13 108 Dramas, Sci-Fi &amp;amp; Fantasy Overwhelmed by his mother’s illness, a young b… 7 7777 rows × 9 columns‘출시일로부터 얼마나 지난 콘텐츠를 수급했는가?’ 에 대한 데이터를 확보한다.add_from_release = pd.DataFrame(nflx.index - nflx[&#39;release_year&#39;])add_from_release.columns = [&#39;days&#39;]add_from_release   days date_added   2008-01-01 730 days 2008-02-04 399 days 2009-05-05 855 days 2009-11-18 321 days 2010-11-01 8705 days … … 2021-01-15 14 days 2021-01-15 380 days 2021-01-16 381 days 2021-01-16 1111 days 2021-01-16 1842 days 7777 rows × 1 columns# 숫자만 남긴다.add_from_release[&#39;days&#39;] = add_from_release[&#39;days&#39;].apply(lambda x : str(x).split(&#39; &#39;)[0])add_from_release[&#39;days&#39;] = add_from_release[&#39;days&#39;].astype(&#39;int64&#39;)add_from_release   days date_added   2008-01-01 730 2008-02-04 399 2009-05-05 855 2009-11-18 321 2010-11-01 8705 … … 2021-01-15 14 2021-01-15 380 2021-01-16 381 2021-01-16 1111 2021-01-16 1842 7777 rows × 1 columns# 시계열 그래프 확인add_from_release.plot();역시나 데이터가 ‘최근’에 밀집되어 있어서 시계열 그대로 데이터 분포를 분석하기는 어렵다.이번에도 seps 컬럼을 붙여서 사용하도록 한다.add_from_release[&#39;seps&#39;] = nflx[&#39;seps&#39;]add_from_release   days seps date_added     2008-01-01 730 1 2008-02-04 399 1 2009-05-05 855 1 2009-11-18 321 1 2010-11-01 8705 1 … … … 2021-01-15 14 7 2021-01-15 380 7 2021-01-16 381 7 2021-01-16 1111 7 2021-01-16 1842 7 7777 rows × 2 columns# 각 시계열 구간 별 평균 집계add_from_release.groupby(&#39;seps&#39;).mean().plot()plt.axvline(x=3, color=&#39;r&#39;, linestyle=&#39;--&#39;);결론 :넷플릭스는 과거(1,2,3,4)에 비해 최근(5,6,7) 출시일이 오래 지난 콘텐츠를 많이 수급한 경향이 있다. 귀무가설을 기각한다. 넷플릭스 수급 현황을 보면서 국내와 마찬가지로 세계적으로 시간이 흐른 명작들을 다시 보고자 하는 소비자 수요도 확인해볼 수 있었다.3-4. 넷플릭스의 주요 소비자 타겟(연령대) 분석넷플릭스 사용자가 늘어나면서, 이탈율 관리를 위해 주 타겟층을 확실하게 확보하고자 하는 움직임을 보일 수 있다. 콘텐츠 플랫폼은 2~30대 사용자 수가 가장 많다.따라서 어린이 연령대의 비중이 낮아지는지, 반대로 성인 연령대 콘텐츠 비중이 높아지는지 함께 확인하는 것을 통해 넷플릭스가 집중하고 있는 타겟층을 분석할 수 있다. H0 : 어린이 콘텐츠 비중은 일정하게 유지되고 있다. H1 : 최근 어린이 콘텐츠 비중이 줄고 있다.(참고) 영화, TV 관람 등급 표nflx[&#39;rating&#39;].unique()array([&#39;TV-MA&#39;, &#39;NR&#39;, &#39;TV-PG&#39;, &#39;TV-14&#39;, &#39;PG&#39;, &#39;R&#39;, &#39;TV-G&#39;, &#39;G&#39;, &#39;TV-Y&#39;, &#39;TV-Y7-FV&#39;, &#39;TV-Y7&#39;, &#39;PG-13&#39;, nan, &#39;NC-17&#39;, &#39;UR&#39;], dtype=object)rating_dict = { &#39;kids&#39; : [&#39;G&#39;,&#39;TV-G&#39;,&#39;TV-Y&#39;], # 전체관람가 &#39;elem&#39; : [&#39;PG&#39;,&#39;TV-Y7&#39;,&#39;TV-Y7-FV&#39;,&#39;TV-PG&#39;], # 7세 이상 &#39;teen&#39; : [&#39;PG-13&#39;], # 13세 이상 &#39;youth&#39; : [&#39;TV-14&#39;], # 16세 이상 &#39;adult&#39; : [&#39;NC-17&#39;,&#39;R&#39;,&#39;TV-MA&#39;,&#39;NR&#39;,&#39;UR&#39;] # 18세 이상, 청불 포함}ratings = rating_dict.keys()rating_df = pd.DataFrame(columns=ratings)rating_list = []for i in range(1,8): # seps : 1~7 rating_list = [] for key in rating_dict.keys(): # rating : kids / elem / teen / youth / adult rating_list.append(nflx.loc[nflx[&#39;seps&#39;]==str(i),&#39;rating&#39;].str.contains(&#39;|&#39;.join(rating_dict[key])).sum()) rating_df = rating_df.append(pd.Series(rating_list,index=rating_df.columns), ignore_index=True)rating_df = rating_df.astype(&#39;int64&#39;)rating_df   kids elem teen youth adult 0 316 253 16 271 518 1 307 248 30 298 505 2 285 215 36 316 510 3 255 219 51 278 578 4 358 282 93 237 516 5 316 233 79 282 513 6 388 263 81 246 477 plt.figure(figsize=(8,6))for col in list(rating_df.columns): sns.lineplot(data=rating_df, x=rating_df.index, y=rating_df[col], label=col)plt.legend()plt.ylabel(&#39;The Count of Ratings&#39;)plt.xlabel(&#39;period(past.. 1---&amp;gt;7 ..now)&#39;)plt.show()결론 :adult 등급의 콘텐츠 수급은 줄어드는 반면 kids와 teen 등급 수급은 늘고 있다. 연구가설을 기각하고 귀무가설을 채택한다.가장 활발한 콘텐츠 이용자가 2~30대라는 점에서 전략적으로 해당 연령층에 집중할 것이라는 예상을 했으나 예상과 다른 결과를 확인했다. 넷플릭스는 2-30대 뿐만아니라 더 젋은 세대까지 확보하고자 한다. 이는 IT 기술과 영상 콘텐츠가 익숙한 Z세대를 고려했을때 충분히 납득가능한 결과다. (국내에서도 초등학생들이 네이버보다 유튜브로 먼저 검색한다고 한다)3-5. 콘텐츠 평균 재생 시간 분석세계적으로 틱톡과 같은 숏폼 콘텐츠가 유행이다. 또한, 왕좌의게임과 같은 드넓은 세계관의 장기 시리즈가 과거 인기였다면 최근에는 킹덤, 스위트홈처럼 짧고 자극적인 시리즈가 주목을 받고 있다. 이에 따라 최근 넷플릭스에서 콘텐츠를 수급할 때 재생 시간을 고려하는 경향이 있는지 여부를 확인하고자 한다. H0 : 평균 콘텐츠 재생 시간은 전과 동일하거나 더 길어지고 있다. H1 : 최근 재생 시간이 짧은 콘텐츠 비중이 늘고 있다.duration_df = pd.DataFrame(nflx.groupby([&#39;seps&#39;,&#39;type&#39;])[&#39;duration&#39;].mean())duration_df     duration seps type   1 Movie 88.951613 TV Show 1.662125   2 Movie 99.214464 TV Show 1.915858   3 Movie 102.677925 TV Show 1.627660   4 Movie 98.812580 TV Show 1.617470   5 Movie 98.291498 TV Show 1.624324   6 Movie 106.675393 TV Show 1.878963   7 Movie 100.000000 TV Show 1.972010   duration_df = pd.pivot_table(duration_df, index=&#39;seps&#39;, columns=&#39;type&#39;)duration_df.columns = [&#39;movie&#39;,&#39;tv&#39;]duration_df   movie tv seps     1 88.951613 1.662125 2 99.214464 1.915858 3 102.677925 1.627660 4 98.812580 1.617470 5 98.291498 1.624324 6 106.675393 1.878963 7 100.000000 1.972010 fig,axes = plt.subplots(1,2, figsize=(12,6))sns.lineplot(data=duration_df, x=duration_df.index, y=duration_df[&#39;movie&#39;], ax=axes[0])sns.lineplot(data=duration_df, x=duration_df.index, y=duration_df[&#39;tv&#39;], ax=axes[1])axes[0].set_title(&#39;Movie&#39;)axes[1].set_title(&#39;TV Show&#39;)axes[0].set_ylabel(&#39;The Mean of Durations&#39;)axes[1].set_ylabel(&#39;The Mean of Durations&#39;)axes[0].set_xlabel(&#39;Period&#39;)axes[1].set_xlabel(&#39;Period&#39;)plt.show()# 급격히 늘고 있는 TV Show 방영 차수 빈도 확인nflx.loc[nflx[&#39;type&#39;]==&#39;TV Show&#39;,&#39;duration&#39;].value_counts()1 16082 3783 1834 865 576 307 198 189 810 515 211 212 213 116 1Name: duration, dtype: int64결론 :오히려 최근 평균 콘텐츠 재생 시간이 증가하고 있다. 연구가설을 기각하고 귀무가설을 채택한다.특히 TV Show의 경우 시리즈물에 대한 수요가 더 커지는 것을 확인할 수 있다. 다만, 수치상 큰 차이가 없기 때문에 넷플릭스에서 전략적으로 의도했다기보다는 최근 제작되는 콘텐츠들이 더 풍성하고 퀄리티있게 제작되면서 자연스럽게 시리즈물이 많아지고, 영화 시간이 길어진 것으로 보인다.3-6. 넷플릭스에서 주로 수급하는 콘텐츠 장르 분석콘텐츠, OTT시장의 고도화된 개인화 추천서비스로 대중적인 취향보다 개별적 취향을 더 심도깊게 고민하고 기술적으로 분석하고 있다.넷플릭스는 추천서비스를 비즈니스에 적극적으로 활용하면서 가파른 성장을 이뤄낸 경험이 있다. 따라서 넷플릭스는 고객을 더 세분화하고 이에 따라 콘텐츠 장르를 더 다양하게 분류하고 수급하는 전략을 취할 수 있다. H0 : 최근 유통하는 콘텐츠 장르는 그 수가 과거와 같거나 적다. H1 : 최근 유통하는 콘텐츠 장르가 더 다양해지고 있다.# 전체 기간 중 장르별 콘텐츠 수nflx[&#39;listed_in&#39;].value_counts()Documentaries 334Stand-Up Comedy 321Dramas, International Movies 320Comedies, Dramas, International Movies 243Dramas, Independent Movies, International Movies 215 ... Cult Movies, Documentaries, LGBTQ Movies 1Cult Movies, Horror Movies, Thrillers 1Dramas, Horror Movies, Sci-Fi &amp;amp; Fantasy 1Action &amp;amp; Adventure, Anime Features, Classic Movies 1Comedies, Cult Movies, LGBTQ Movies 1Name: listed_in, Length: 491, dtype: int64넷플릭스에는 다큐멘터리가 가장 많고, 스탠드업코미디가 그 다음이다.nflx.groupby(&#39;seps&#39;)[&#39;listed_in&#39;].nunique().plot()plt.xlabel(&#39;Period&#39;)plt.ylabel(&#39;Count of Contents&#39;)plt.show()최근으로 올수록 더 다양한 장르를 보유하는 추세를 보인다. 콘텐츠 장르가 더 다양하지고 있으므로(1,2,3,4 Period &amp;lt; 5,6,7 Period) 귀무가설을 기각한다.해당 데이터에서는 추가로 더 많은 정보를 얻어낼 수 있을 것 같다. 과거에는 어떤 장르를 주로 수급했고, 최근 어떤 장르를 주로 수급하는지 알아보도록 하자.docum_count = nflx.loc[nflx[&#39;listed_in&#39;]==&#39;Documentaries&#39;].groupby(&#39;seps&#39;).count()[&#39;listed_in&#39;]plt.figure(figsize=(8,4))docum_count.plot()plt.ylabel(&#39;the count of documentary genre&#39;)plt.xlabel(&#39;Period&#39;)plt.show()전체 콘텐츠 수에서 다큐멘터리 장르가 차지하는 비중이 가장 높았는데, 이는 넷플릭스 초기에 많이 수급했던 것에 원인이 있었다. 최근 넷플릭스는 다큐멘터리 수급에 집중하지 않고 있다.# 최근 집중적으로 수급하는 장르 확인nflx.loc[nflx[&#39;seps&#39;]==&#39;7&#39;,&#39;listed_in&#39;].value_counts()Dramas, International Movies 45Kids&#39; TV 41Children &amp;amp; Family Movies, Comedies 38Children &amp;amp; Family Movies 35Comedies, Dramas, International Movies 34 ..Children &amp;amp; Family Movies, Classic Movies, Dramas 1International TV Shows, TV Dramas, TV Thrillers 1British TV Shows, Romantic TV Shows, TV Dramas 1Action &amp;amp; Adventure, Dramas, Romantic Movies 1Crime TV Shows, International TV Shows, TV Mysteries 1Name: listed_in, Length: 229, dtype: int64top5_genre = nflx.loc[nflx[&#39;seps&#39;]==&#39;7&#39;,&#39;listed_in&#39;].value_counts().index[:5]plt.figure(figsize=(12,5))for genre in list(top5_genre): genre_count = nflx.loc[nflx[&#39;listed_in&#39;]==genre].groupby(&#39;seps&#39;).count()[&#39;listed_in&#39;] sns.lineplot(x=genre_count.index, y=genre_count.values, label=genre)plt.xlabel(&#39;Period&#39;)plt.ylabel(&#39;The Count of Each Genre&#39;)plt.axvline(x=3, linestyle=&#39;--&#39;, color=&#39;r&#39;)plt.show()키즈 콘텐츠, 가족 콘텐츠의 비중이 4 Period 부터 꾸준히 상승하고 있다.결론 :과거 대비 다양한 장르를 확보하면서 다큐멘터리와 같은 콘텐츠 점유율이 높았던 장르의 비중이 낮아지고,더 다양한 고객층의 니즈를 반영하고자 하는 것을 확인할 수 있다.4. 종합 결론아래 6가지 가설 검정 과정을 통해 넷플릭스가 시장 점유를 위해 취하고 있는 콘텐츠 수급 전략을 분석해보았다. H1 : 최근 특정 콘텐츠 유형(TV show or Movie)이 늘고 있다. (주요 컬럼 : type) H1 : 최근 특정 국가의 콘텐츠 수급 비중이 늘고 있다. (주요 컬럼 : country) H1 : 최근 옛날(오래된) 콘텐츠 수급 비중이 늘고 있다. (주요 컬럼 : release_year) H1 : 최근 어린이 콘텐츠 비중이 줄고 있다. (주요 컬럼 : rating) H1 : 최근 길이(시리즈, 시간)가 짧은 콘텐츠 비중이 늘고 있다. (주요 컬럼 : duration) H1 : 최근 유통하는 콘텐츠 장르가 더 다양해지고 있다. (주요 컬럼 : listed_in, description)종합하면,넷플릭스는 영화보다 TV 시리즈 수급을 늘리고 있고, 자국(미국) 콘텐츠 확보에 힘을 싣고 있다. 또한 소비자 수요를 반영해 최신 콘텐츠뿐만 아니라 시간이 흐른 콘텐츠들을 함께 확보하고 있고 2~30대 주요 고객층 이외 어린이를 포함해 더 다양한 고객을 타겟으로 하고 있다.숏폼 콘텐츠가 유행하고 있지만 영화 산업이나 TV 드라마 산업에서 자체 콘텐츠의 길이를 줄이려는 시도를 하는 경향은 없으며 넷플릭스 역시 영상물의 재생 시간에 대해서는 수급 기준으로 고려하지 않는 것으로 보인다. 마지막으로 다양한 고객들의 수요를 맞추기 위해 다양한 콘텐츠 장르를 선보이면서 어린이 및 가족 콘텐츠 보유량을 늘려 다양한 고객층의 니즈를 반영하고자 한다.👉 파이썬 코드 보러 가기관성을 이기는 데이터" }, { "title": "커머스 연간 지출액을 통한 매출 개선 시뮬레이션 - 파이썬 프로덕트 데이터 분석", "url": "/posts/%EC%BB%A4%EB%A8%B8%EC%8A%A4-%EC%97%B0%EA%B0%84-%EC%A7%80%EC%B6%9C%EC%95%A1%EC%9D%84-%ED%86%B5%ED%95%9C-%EB%A7%A4%EC%B6%9C-%EA%B0%9C%EC%84%A0-%EC%8B%9C%EB%AE%AC%EB%A0%88%EC%9D%B4%EC%85%98-%ED%8C%8C%EC%9D%B4%EC%8D%AC-%ED%94%84%EB%A1%9C%EB%8D%95%ED%8A%B8-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%B6%84%EC%84%9D/", "categories": "3. 튜토리얼, 비즈니스 통계 분석 프로그래밍", "tags": "", "date": "2021-08-12 00:00:00 +0900", "snippet": "웹/앱 접속 시간에 따른 지출 현황 분석 및 매출 개선 전략 수립 분석 목표 : 서비스 내 고객 별 접속 현황과 연간 지출액을 살펴보고, 매출을 높이기 위한 전략을 수립한다. 데이터 출처 : Ecommerce Customers, Kaggle1. 가설 설정각 컬럼 정보를 확인하고, 분석 목표에 도달하기 위한 가설을 설정한다.import pandas as pdimport numpy as npfrom scipy import statsimport statsmodels.api as smfrom sklearn import model_selectionfrom sklearn.linear_model import LinearRegressionimport seaborn as snsimport matplotlib.pyplot as pltimport warningswarnings.filterwarnings(action=&#39;ignore&#39;)customer_r = pd.read_csv(&#39;./data/customers.csv&#39;).iloc[:,3:]customer_r   Avg. Session Length Time on App Time on Website Length of Membership Yearly Amount Spent 0 34.497268 12.655651 39.577668 4.082621 587.951054 1 31.926272 11.109461 37.268959 2.664034 392.204933 2 33.000915 11.330278 37.110597 4.104543 487.547505 3 34.305557 13.717514 36.721283 3.120179 581.852344 4 33.330673 12.795189 37.536653 4.446308 599.406092 … … … … … … 495 33.237660 13.566160 36.417985 3.746573 573.847438 496 34.702529 11.695736 37.190268 3.576526 529.049004 497 32.646777 11.499409 38.332576 4.958264 551.620145 498 33.322501 12.391423 36.840086 2.336485 456.469510 499 33.715981 12.418808 35.771016 2.735160 497.778642 500 rows × 5 columns각 컬럼은 아래 정보를 담고 있다. Avg.Session Length : 평균 세션 지속 시간 Time on App : App 접속 시간 Time on Website : Website 접속 시간 Length of Membership : 가입 기간 Yearly Amount Spent : 연간 지출액# 결측치 및 데이터 타입 확인customer_r.info()&amp;lt;class &#39;pandas.core.frame.DataFrame&#39;&amp;gt;RangeIndex: 500 entries, 0 to 499Data columns (total 5 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Avg. Session Length 500 non-null float64 1 Time on App 500 non-null float64 2 Time on Website 500 non-null float64 3 Length of Membership 500 non-null float64 4 Yearly Amount Spent 500 non-null float64dtypes: float64(5)memory usage: 19.7 KB# 데이터 분포 통계 확인customer_r.describe()   Avg. Session Length Time on App Time on Website Length of Membership Yearly Amount Spent count 500.000000 500.000000 500.000000 500.000000 500.000000 mean 33.053194 12.052488 37.060445 3.533462 499.314038 std 0.992563 0.994216 1.010489 0.999278 79.314782 min 29.532429 8.508152 33.913847 0.269901 256.670582 25% 32.341822 11.388153 36.349257 2.930450 445.038277 50% 33.082008 11.983231 37.069367 3.533975 498.887875 75% 33.711985 12.753850 37.716432 4.126502 549.313828 max 36.139662 15.126994 40.005182 6.922689 765.518462 각 컬럼의 단위는(분, 시간, 일, 월)는 정확히 알 수 없고, 값이 크고 작다에 대한 인지만 가능한 상황이다. 따라서 이번 분석에는 임의로 주어진 데이터의 단위를 납득 가능한 수준에서 부여하고자 한다. Avg.Session Length (분) Time on App (분) Time on Website (분) Length of Membership (월) Yearly Amount Spent (천 원)먼저, 세션 길이는 평균 33분으로 Time on App, Time on Website와 데이터 범위가 매우 유사하다.두 개의 Time on ~ 컬럼을 빈도(접속 횟수)로 이해하기에는 각 row가 연간 집계 데이터임을 감안할 때 적절하지 않다. 따라서 앞 3개 컬럼은 모두 단위를 ‘분’으로 통합하겠다. 정보 손실을 피하기 위해 소수점 이하는 제거하지 않고 ‘초’ 이하 단위로 본다.다음으로 Length of Membership의 경우 모든 데이터가 0~12 사이에 분포한다.‘월’ 단위로 고려하고, 정보 손실을 피하기 위해 소수점 이하는 제거하지 않고 ‘일’ 이하(일,시간,분..) 단위로 보겠다.마지막으로 분석 대상을 대형 플랫폼 기업이 아닌 국내 소규모 커머스로 가정하기 위해 Yearly Amount Spent는 ‘천 원’ 단위로 약속하고 마찬가지로 정보 손실을 피하기 위해 소수점 이하는 제거하지 않는다.# 전처리 전 원본 데이터 복사customer = customer_r.copy()# 에러를 줄이기 위해 컬럼명 변경customer.columns = [&#39;session&#39;,&#39;app&#39;,&#39;web&#39;,&#39;membership&#39;,&#39;spent&#39;]이미 정제된 데이터라 미리 전처리를 해야할 부분은 없었고 컬럼 이름만 보기 쉽게 정리했다.이제 본격적으로 분석에 들어가기 전에 어떤 과정을 거치며 최종 결론에 도달할지 단계적 가설을 설정한다.우리의 분석 목표는 ‘서비스 내 고객 별 접속 현황과 연간 지출액을 살펴보고, 매출을 높이기 위한 전략을 수립하는 것’이다.따라서 본 데이터에서 검증하고자 하는 가설은 아래와 같다.(H1 = 연구가설) H1: 유저의 세션 접속 시간이 길수록 연간 지출액은 높다. (주요 컬럼: spent~session) H1: 유저의 가입 기간이 길수록 연간 지출액은 높다. (주요 컬럼: spent~membership) H1: 해당 서비스는 웹보다 앱을 더 많이 사용한다. (주요 컬럼: app~web) H1: 앱 사용 시간이 많을 수록 연간 지출액은 높다. (주요 컬럼: spent~app) H1: 웹 사용 시간이 많을 수록 연간 지출액은 높다. (주요 컬럼: spent~web) H1: 가입 기간에 따라 앱이나 웹 선호도(사용 시간)는 다르다. (주요 컬럼: app+web~membership)2. 가설 검정앞서 설계한 5가지 가설을 순차적으로 검정해나가며 최종 전략을 수립하도록 한다.2-1. 평균 세션 접속 시간에 따른 연간 지출액 확인평균 세션 접속 시간이 긴 유저라면 충성도가 높을 것이고, 지출액도 높을 것이라 예상해볼 수 있다.과연 그러한지 상관계수를 통해 확인하고자 한다. H0: 유저의 세션 접속 시간과 연간 지출액은 서로 관련이 없다. H1: 유저의 세션 접속 시간이 길수록 연간 지출액은 높다.sns.scatterplot(data=customer, x=&#39;session&#39;,y=&#39;spent&#39;);x = customer[&#39;session&#39;]y = customer[&#39;spent&#39;]# 피어슨 상관계수 확인stats.pearsonr(x,y)(0.35508829470764325, 2.6450940333346556e-16)피어슨 상관분석 결과 p-value는 0에 수렴하고 상관계수는 0.36으로, 약한 양의 상관관계를 확인했다.결론:평균 세션 지속 시간은 연간 지출액과 양의 상관관계가 있다. 귀무가설을 기각한다.하지만 연관성이 강하지 않고, 모든 서비스가 그러하든 수치로 판단할 수 없는 많은 잠재 변수가 존재한다. 따라서 세션 지속 시간이 길기 “때문에” 연간 지출액이 상승하는 것인지 그 “인과성”을 확인할 수는 없다. 다만, 세션 지속 시간이 긴 유저는 본 서비스 매출에 기여하는 고객으로 해석할 수 있다.2-2. 가입 기간에 따른 연간 지출액 확인일반적으로 가입 기간과 지출액은 관련이 크지 않다. 점점 커머스 시장이 커지고 복잡해지면서 가입한 서비스를 굳이 해지하기보다 가입한 채 사용하지 않는 경향이 많기 때문이다. 그러나 만약 본 서비스가 사용 기간에 따라 충성도가 크고, 리텐션이 매우 좋은 서비스라면 가입 기간에 따라 연간 지출액이 높아질 수 있다.따라서 리텐션 효율이 좋은 서비스인지 알아보기 위해 가입 기간에 따른 연간 지출액을 확인하고자 한다. H0: 유저의 가입 기간과 연간 지출액은 서로 관련이 없다. H1: 유저의 가입 기간이 길수록 연간 지출액은 높다.sns.scatterplot(data=customer, x=&#39;membership&#39;, y=&#39;spent&#39;);매우 강한 선형 관계를 보인다. 구체화하기 위해 상관계수를 이어서 확인해보도록 한다.x = customer[&#39;membership&#39;]y = customer[&#39;spent&#39;]# 피어슨 상관계수 확인stats.pearsonr(x,y)(0.809083568247581, 4.806615782583552e-117)상관계수는 0.81, p-value는 0에 강하게 수렴한다. 강한 양의 상관관계를 나타낸다.결론:유저의 가입 기간과 연간 지출액은 강한 양의 상관관계가 있다. 귀무가설을 기각한다.평균 세션 유지 시간과 마찬가지로 가입 기간이 길어서 연간 지출액이 높다는 인과성을 확인할 수는 없다. 하지만 본 서비스가 가입 이후의 고객 경험이 매우 좋고, 시간에 따라 매출에 기여하는 충성고객 비율이 높다고 이해할 수 있겠다.2-3. 웹, 앱 사용 시간 비교3월까지 쇼핑 앱 사용자수 ‘3500만’- 70%가 앱으로 쇼핑, 정보통신신문위 기사를 근거로 본 서비스는 앱 사용빈도가 높을 것이라 예상할 수 있다.하지만 데이터에서 구매 경로를 확인할 수 없기 때문에, 웹의 사용시간과 앱 사용시간을 통해 간접적으로 구매 경로에 대한 유저 선호도를 확인해보고자 한다. H0: 해당 서비스는 웹보다 앱을 더 적게 사용하거나 사용 시간이 동일하다. H1: 해당 서비스는 웹보다 앱을 더 많이 사용한다.# 전체 구매 경로(app, web) 중 app 사용 비율customer[&#39;app_per&#39;] = customer[&#39;app&#39;] / (customer[&#39;app&#39;] + customer[&#39;web&#39;])customer[&#39;app_per&#39;].describe()count 500.000000mean 0.245180std 0.015740min 0.19349725% 0.23493650% 0.24553175% 0.257707max 0.289320Name: app_per, dtype: float64sns.histplot(customer[&#39;app_per&#39;]);결론:모든 유저의 앱 사용 비율은 최대 30%를 넘지 않는다. 연구가설을 기각한다.본 서비스는 예상과 달리 웹 사용 비율이 더 높다. 그러나 웹 사용 비율이 높은 것이 매출에 기여하는지는 알 수 없다. 따라서 앱 사용 시간에 따른 연간 지출액과 웹 사용 시간에 따른 연간 지출액을 비교해서 두 판매 경로의 효율을 체크할 필요가 있겠다.2-4. 앱 사용 시간에 따른 연간 지출액 확인앞서 확인한 것처럼 본 서비스는 웹 사용 비율이 더 높다. 그러나 국내 쇼핑의 70%가 앱에서 이루어진다는 보도를 고려할 때, 실제 매출은 웹 보다 앱에서 주로 발생할 가능성이 크다. 따라서 앱 사용 시간에 따른 연간 지출액을 상관분석을 통해 확인하고자 한다. H0: 앱 사용 시간과 연간 지출액은 서로 관련이 없다. H1: 앱 사용 시간이 많을 수록 연간 지출액은 높다.sns.scatterplot(data=customer, x=&#39;app&#39;, y=&#39;spent&#39;);산점도로 볼 때, 선형 관계가 있는 것 같다. 이어서 상관계수를 통해 확인한다.x=customer[&#39;app&#39;]y=customer[&#39;spent&#39;]stats.pearsonr(x,y)(0.4993277700534505, 6.905842369970937e-33)상관분석 결과 상관계수는 0.5, p-value는 0에 수렴하여 양의 상관관계가 존재함을 알 수 있다.결론:유저의 app 사용 시간은 연간 지출액과 관련이 있다. 분석 결과 양의 상관관계가 있으므로 귀무가설을 기각한다.이어서 web 사용 시간과 연간 지출액의 상관 분석 결과를 비교하면, 어떤 구매 경로가 더 효율이 좋은지 확인할 수 있다.2-5. 웹 사용 시간에 따른 연간 지출액 확인앱과 마찬가지로 웹 사용 시간도 연간 지출액과 관련이 있을 것이라 예상할 수 있다. 앱보다 사용 비율이 압도적으로 높고, 서비스를 사용한다는 것은 작은 결제라도 연결되기 때문이다. 분석을 통해 상관계수를 확인하고, 앞에서 확인한 app 사용 시간과 연간 지출액의 상관계수와 비교해보자. H0: 웹 사용 시간과 연간 지출액은 서로 관련이 없다. H1: 웹 사용 시간이 많을 수록 연간 지출액은 높다.sns.scatterplot(data=customer, x=&#39;web&#39;, y=&#39;spent&#39;);시각적으로는 app과 spent의 관계보다 상관성이 약해 보인다. 이어서 피어슨 상관계수를 확인해보자.x = customer[&#39;web&#39;]y = customer[&#39;spent&#39;]stats.pearsonr(x,y)(-0.002640844672158962, 0.9530290603515617)예상과 달리 상관관계가 없다. p_value는 0.5보다 크고 상관계수를 신뢰할 수 없기에 app, web 사용 시간 별 연간 지출액에 대한 결론은 아래와 같이 정리한다.결론:예상과 달리 web 사용 시간과 연간 지출액은 서로 관련이 없다. 연구가설을 기각한다.2-3, 2-4와 종합하면, 유저는 웹에서 시간을 많이 보내지만 실제 결제는 app을 통해 발생하는 것을 알 수 있다. 전략적으로 결제가 발생하는 app 사용 시간을 늘리기 위한 서비스 개편이 필요해 보인다.2-6. 가입 기간에 따른 구매 경로에 대한 선호도 차이 분석마지막으로, 가입 기간으로 고객 그룹을 나누기 위해 가입 기간에 따라 웹 사용 시간과 앱 사용 시간을 확인한다. 그룹을 나누는 것은 고객군 별 매출 향상을 위한 전략을 달리 설계하기 위함이다. H0: 가입 기간에 따라 앱이나 웹 선호도(사용 시간)는 서로 차이가 없다. H1: 가입 기간에 따라 앱이나 웹 선호도(사용 시간)는 다르다.먼저, 가입 기간을 각각 유효한 그룹으로 나누기 위해 분포를 확인합니다.customer[&#39;membership&#39;].describe()count 500.000000mean 3.533462std 0.999278min 0.26990125% 2.93045050% 3.53397575% 4.126502max 6.922689Name: membership, dtype: float64sns.histplot(customer[&#39;membership&#39;]);데이터가 정규분포를 따른다. 그룹은 중위수(50%)를 기준으로 2개로 나누도록 한다.md = np.median(customer[&#39;membership&#39;])customer[&#39;group&#39;] = customer[&#39;membership&#39;].apply(lambda x:&#39;A&#39; if x &amp;amp;lt; md else &#39;B&#39;)customer[&#39;group&#39;].value_counts()B 250A 250Name: group, dtype: int64customer.loc[customer[&#39;group&#39;]==&#39;B&#39;,&#39;app&#39;].describe()count 250.000000mean 12.076809std 1.004755min 9.47777825% 11.36570650% 12.01301475% 12.777711max 15.126994Name: app, dtype: float64# group 별 분포 시각화sns.barplot(x=[&#39;A&#39;,&#39;B&#39;], y=customer[&#39;group&#39;].value_counts());print(&#39;서비스 사용 기간이 평균 미만인 고객의 평균 app 사용 시간 : {:.2f}&#39;.\\ format(customer.loc[customer[&#39;group&#39;]==&#39;A&#39;,&#39;app&#39;].mean()))print(&#39;서비스 사용 기간이 평균 미만인 고객의 평균 web 사용 시간 : {:.2f}&#39;.\\ format(customer.loc[customer[&#39;group&#39;]==&#39;A&#39;,&#39;web&#39;].mean()))서비스 사용 기간이 평균 미만인 고객의 평균 app 사용 시간 : 12.03서비스 사용 기간이 평균 미만인 고객의 평균 web 사용 시간 : 37.05print(&#39;서비스 사용 기간이 평균 이상인 고객의 평균 app 사용 시간 : {:.2f}&#39;.\\ format(customer.loc[customer[&#39;group&#39;]==&#39;B&#39;,&#39;app&#39;].mean()))print(&#39;서비스 사용 기간이 평균 이상인 고객의 평균 web 사용 시간 : {:.2f}&#39;.\\ format(customer.loc[customer[&#39;group&#39;]==&#39;B&#39;,&#39;web&#39;].mean()))서비스 사용 기간이 평균 이상인 고객의 평균 app 사용 시간 : 12.08서비스 사용 기간이 평균 이상인 고객의 평균 web 사용 시간 : 37.07각 그룹 별 app, web 사용 시간에 유의미한 차이가 있다고 보기 어렵다. 두 그룹 모두 web을 선호한다.결론:서비스 사용 기간(가입 기간)에 따라 구매 경로(app, web)에 대한 선호도는 차이가 없다. 연구가설을 기각한다. 그러나 앞서 확인한 것처럼 실제 구매가 발생하는 곳은 app이므로 web 사용자를 app으로 옮겨올 필요가 있다. app에 대한 선호도를 높일 수 있다면, app 사용 시간을 늘리고 연간 지출액을 증가시킬 수 있다.2-7. 가입 기간에 따른 구매 경로 별 연간 지출액 확인가입 기간과 상관없이 모두 web을 선호하고, app 사용 시간 및 web 사용 시간에 차이가 없었다. 그렇다면 서비스 이용 시간이 동일하다면, web과 app 모두에서 가입 기간이 긴 유저가 연간 지출액이 높을지 확인하고자 한다.앞서 가입 기간이 길수록 연간 지출액이 높다는 것을 확인했기 때문에 web, app 동일하게 그러한지 파악하기 위함이다. H0: app, web 둘 중 하나만 가입 기간이 오래된 그룹(B)이 연간 지출액이 높다. H1: app, web 모두 가입 기간이 오래된 그룹(B)이 연간 지출액이 높다.fig, axes = plt.subplots(1,2, figsize=(10,4))# app/web 사용 시간에 따른 연간 지출액을 그룹으로 나눠 확인한다.sns.scatterplot(data=customer, x=&#39;app&#39;, y=&#39;spent&#39;, hue=&#39;group&#39;, ax=axes[0])sns.scatterplot(data=customer, x=&#39;web&#39;, y=&#39;spent&#39;, hue=&#39;group&#39;, ax=axes[1])plt.show()두 구매 경로(app, web) 모두 B 그룹이 연간 지출액이 높은 경향을 보인다. 귀무가설을 기각한다. 차이가 분명하게 있으므로 그룹을 2개로 나누지 않고 모든 가입 기간에 대해 산점도를 확인해보겠다.fig, axes = plt.subplots(1,2, figsize=(10,4))# app/web 사용 시간에 따른 연간 지출액을 그룹으로 나눠 확인한다.sns.scatterplot(data=customer, x=&#39;app&#39;, y=&#39;spent&#39;, hue=&#39;membership&#39;, ax=axes[0])sns.scatterplot(data=customer, x=&#39;web&#39;, y=&#39;spent&#39;, hue=&#39;membership&#39;, ax=axes[1])plt.show()결론:두 구매 경로(app, web) 모두 서비스 사용 기간(가입 기간)이 길면 연간 지출액도 높다.2-6과 종합하면, 모든 가입 기간과 모든 구매 경로에 대해서 고객은 일관성 있는 특징을 보인다. 즉, 가입 기간 별로 고객 그룹을 나눈다거나 구매 경로 별 선호도 기반으로 고객 그룹을 나눌 수 없다. 따라서 매출 개선 전략 타깃은 고객군을 나누지 않고 전체 유저를 대상으로 한다.3. 분석 결과아래 7가지 가설 검정 과정을 통해 웹/앱 접속 시간에 따른 지출 현황을 분석해보았다. H1: 유저의 세션 접속 시간이 길수록 연간 지출액은 높다. H1: 유저의 가입 기간이 길수록 연간 지출액은 높다. H1: 해당 서비스는 웹보다 앱을 더 많이 사용한다. H1: 앱 사용 시간이 많을수록 연간 지출액은 높다. H1: 웹 사용 시간이 많을 수록 연간 지출액은 높다. H1: 가입 기간에 따라 앱이나 웹 선호도(사용 시간)는 다르다. H1: app, web 모두 가입 기간이 오래된 그룹(B)이 연간 지출액이 높다.정리하면,유저의 가입 기간이 길수록 지출액이 높다는 점에서 충성 고객이 많은 고효율 서비스다. 결제는 app에서 발생하지만 사용자가 web에 머무는 시간이 더 많고, 이는 가입 기간이 길더라도 동일하게 나타나는 현상이다.위 분석을 토대로 아래의 방법으로 매출 개선을 시도해볼 수 있다.매출 개선 전략 :“web에서 사용자가 머무는 주요 페이지를 app에 최적화하여 점진적으로 app 사용 시간을 늘린다.”4. 매출 개선 시뮬레이션app 사용 비중이 늘어났을 때 어느 정도의 매출 개선이 예상되는지 회귀 분석을 통해 예측해보도록 하자.# 회귀분석 목적으로 새로운 데이터프레임을 다시 생성ct_lin = customer_r.copy()ct_lin.columns = [&#39;session&#39;,&#39;app&#39;,&#39;web&#39;,&#39;membership&#39;,&#39;spent&#39;]# 정규성 검정for col in ct_lin.columns: st, pv = stats.kstest(ct_lin[col],&#39;norm&#39;) print(&#39;vector : {} - p_value : {}&#39;.format(col,pv)) if pv &amp;lt; 0.05: print(&#39;--&amp;amp;gt; 정규분포 O&#39;) else: print(&#39;--&amp;gt; 정규분포 X&#39;)vector : session - p_value : 0.0--&amp;gt; 정규분포 Ovector : app - p_value : 0.0--&amp;gt; 정규분포 Ovector : web - p_value : 0.0--&amp;gt; 정규분포 Ovector : membership - p_value : 0.0--&amp;gt; 정규분포 Ovector : spent - p_value : 0.0--&amp;gt; 정규분포 OX = ct_lin.iloc[:,:-1] # spent 제외 나머지 모든 컬럼X_ = (X - np.min(X))/(np.max(X)-np.min(X)) # 독립변수 정규화 작업y = ct_lin.iloc[:,-1] # spent# 표준화된 독립변수 분포 확인X_.hist();회귀분석의 신뢰도 확보를 위해 변수들의 정규성을 검정했고, 회귀분석의 정확도를 높이기 위해 모든 컬럼의 데이터 범위를 0과 1 사이로 통일시켰다.다음으로 테스트 셋을 따로 분리하고 예측할 모델 성능을 평가한다.X_train, X_test, y_train, y_test = model_selection.train_test_split(X_,y,test_size=0.2,shuffle=True)# 데이터가 잘 분할되었는지 확인print(&#39;The distribution of training set : {} and {}&#39;.format(X_train.shape, y_train.shape))print(&#39;The distribution of test set : {} and {}&#39;.format(X_test.shape, y_test.shape))The distribution of training set : (400, 4) and (400,)The distribution of test set : (100, 4) and (100,)lr = LinearRegression()lr.fit(X_train, y_train)lr.predict(X_test)array([478.51058369, 449.88143676, 411.39031632, 530.3528004 , 488.21386745, 449.73882129, 565.33149069, 491.37434266, 381.29515047, 398.97215612, 554.53025792, 460.71323737, 589.64343949, 409.31004883, 609.23503452, 418.30819157, 448.25048868, 500.68451076, 525.98929327, 527.94173467, 627.91669757, 606.15396051, 498.16392866, 519.19453143, 703.10815952, 533.95199327, 623.12370825, 571.83177477, 347.94951233, 514.09880783, 479.43710906, 540.20256643, 494.06564279, 501.40467521, 484.69696931, 616.65667535, 497.52537355, 505.35428713, 495.37792452, 284.01048684, 424.28292475, 536.23633553, 501.99175929, 549.80182898, 464.26833261, 506.88692522, 518.49951848, 608.60628647, 424.87394181, 423.67045631, 478.83072748, 472.18810295, 590.66708726, 523.75155655, 523.16045797, 413.09116445, 598.72923402, 473.31721848, 465.05781685, 414.35281299, 475.49375111, 511.31138277, 439.8882148 , 435.69041158, 479.53589169, 520.66421506, 548.81178739, 459.64125271, 435.00894641, 445.73302682, 542.98592376, 464.03496306, 478.39591238, 315.39792167, 542.31575921, 457.46719547, 429.201549 , 357.22097032, 414.61837048, 521.47665544, 313.6723082 , 402.50046751, 473.36249895, 552.93736345, 458.56018325, 456.37602465, 583.09659469, 574.68074709, 349.86393209, 255.04108793, 498.4940258 , 438.02540873, 557.91441062, 563.53682494, 532.53380615, 375.3153187 , 551.18583722, 590.20906496, 384.39727334, 495.40860513])# 모델 정확도 확인lr.score(X_test, y_test)0.989284333947629매우 높은 예측 정확도를 가진 선형 회귀 모델을 완성했다. 이 모델을 사용해 app 사용 시간이 늘어남에 따라 연간 지출액이 어떻게 상승할지 예측해보자.먼저, 테스트 셋에서 하나의 샘플을 추출하여 app 사용 시간을 점진적으로 늘린 새로운 테이블을 생성한다.(사용자 수는 동일하다는 가정 하에, app 사용 시간은 10%씩 늘어나고 web 사용 시간은 10%씩 감소시키는 것으로 논의를 단순화하겠다.)# 테스트셋 확인X_   session app web membership 0 0.751425 0.626620 0.929816 0.573101 1 0.362306 0.393016 0.550801 0.359869 2 0.524953 0.426378 0.524803 0.576396 3 0.722409 0.787050 0.460890 0.428434 4 0.574861 0.647702 0.594748 0.627768 … … … … … 495 0.560784 0.764183 0.411098 0.522589 496 0.782491 0.481592 0.537882 0.497028 497 0.471354 0.451931 0.725412 0.704722 498 0.573625 0.586699 0.480394 0.310634 499 0.633178 0.590837 0.304887 0.370560 500 rows × 4 columns# 무작위 인덱스 설정idx = np.random.randint(10)idx4# 무작위로 1개 샘플 추출sample = X_test.iloc[idx]samplesession 0.217829app 0.486975web 0.354718membership 0.624770Name: 482, dtype: float64sample[&#39;session&#39;]0.21782894037206957pers = np.arange(0,1,0.1) # 0부터 0.9까지 0.1씩 증가하는 배열 생성session_list = []app_list = []web_list = []membership_list = []for p in pers: session_list.append(sample[&#39;session&#39;]) app_list.append(sample[&#39;app&#39;]*(1+p)) web_list.append(sample[&#39;web&#39;]*(1-p)) membership_list.append(sample[&#39;membership&#39;])test_dict = { &#39;session&#39;:session_list, &#39;app&#39;:app_list, &#39;web&#39;:web_list, &#39;membership&#39;:membership_list}# 데이터프레임 생성testset = pd.DataFrame(test_dict)testset   session app web membership 0 0.217829 0.486975 0.354718 0.62477 1 0.217829 0.535673 0.319246 0.62477 2 0.217829 0.584370 0.283774 0.62477 3 0.217829 0.633068 0.248302 0.62477 4 0.217829 0.681765 0.212831 0.62477 5 0.217829 0.730463 0.177359 0.62477 6 0.217829 0.779160 0.141887 0.62477 7 0.217829 0.827858 0.106415 0.62477 8 0.217829 0.876555 0.070944 0.62477 9 0.217829 0.925253 0.035472 0.62477 session과 membership은 동일하고 app은 10%씩 증가, web은 10%씩 감소하는 데이터 테이블을 생성했다.이제 앞에서 만든 모델에 데이터를 넣고 연간 지출액을 예측해본다.lr.predict(testset)array([488.21386745, 500.62367008, 513.0334727 , 525.44327532, 537.85307795, 550.26288057, 562.6726832 , 575.08248582, 587.49228845, 599.90209107])sns.lineplot(x=np.arange(0,10,1), y=lr.predict(testset).tolist())plt.xticks(np.arange(0,10,1))plt.xlabel(&#39;app x (1.n%)&#39;)plt.ylabel(&#39;prediction&#39;)plt.show()# 앱 사용 시간 증가에 따른 연간 지출액 증가율pd.DataFrame(lr.predict(testset)).pct_change()   0 0 NaN 1 0.025419 2 0.024789 3 0.024189 4 0.023618 5 0.023073 6 0.022552 7 0.022055 8 0.021579 9 0.021123 유저의 app 사용 시간이 10%씩 늘어남에 따라 연간 지출액은 2%씩 상승한다. web 사용 시간을 10%씩 줄였음에도 지출액 증가를 보이는 것은 매우 유의미하다. 또한 무작위로 추출한 유저(sample 행)의 실제 연간 지출액을 보면 아래와 같이 예측값과 차이가 크지 않다.# sample 유저의 실제 연간 지출액y_test.iloc[idx]494.6386097568927# sample 유저의 예상 연간 지출액lr.predict(testset)[0]488.2138674508592시뮬레이션 결과:유저의 app 사용 시간을 기존 대비 10%씩 늘릴 수 있다면 연간 지출액은 2%씩 상승 가능하다. 따라서 현재 web에서 머무는 시간이 많은 페이지를 확인하여 app에서도 동일하게 서비스를 이용할 수 있도록 하고, 점차적으로 app의 사용성을 개선하여 web에서 app으로 전환하는 비율을 높이면서 동시에 신규 사용자를 app으로 유치시키는 전략을 취하는 것이 좋겠다.관성을 이기는 데이터" }, { "title": "맥 homebrew - 깃 설치 및 깃허브 연동", "url": "/posts/%EB%A7%A5-homebrew-%EA%B9%83-%EC%84%A4%EC%B9%98-%EB%B0%8F-%EA%B9%83%ED%97%88%EB%B8%8C-%EC%97%B0%EB%8F%99/", "categories": "3. 튜토리얼, 금융 분석 프로그래밍 응용", "tags": "", "date": "2020-05-17 00:00:00 +0900", "snippet": "먼저 homebrew를 설치할 것이다. homebrew는 맥에서 git과 같은 다양한 툴을 편하게 설치하게 해준다.git 사이트에서도 이렇게 homebrew 사용을 권장한다.Install Homebrew 아래의 코드를 복사해서 터미널에 붙여넣는다.정상적으로 설치가 되면 이렇게 password를 입력하라고 하는데 그냥 enter쳐주면 다음 연결시에도 password 확인 없이 진행하게 된다.homebrew를 설치했다면, cask도 간단히 설치해준다. cask는 애플의 사파리, 구글 크롬같은 도구들의 설치를 도와주는 유용한 패키지다.드디어 git을 설치한다.% brew install git을 해주면 터미널이 열심히 돌아간다.깃 설치가 완료되었다면%git config –global user.name “~(사용자이름)~”%git config –global user.email “~(이메일계정)~”를 실행한다.다음에 깃을 사용할 때 항상 입력한 사용자 이름과 계정을 사용하도록 하는데, 매번 입력하기 귀찮기 때문에 해주는 설정이다.계정이 잘 생성되었는지 아래 명령어로 확인해본다.% git config –list계정을 정상적으로 만들었다면 이제 SSH 공개키를 연결해줘야 합한다. Git에 내 컴퓨터를 인증하는 과정이다.먼저 ssh파일이 이미 있는 것은 아닌지 확인해본다.아래 캡쳐화면과 같이% cd ~/.ssh  를 실행했을때 파일 혹은 경로를 찾을 수 없다고 나온다면 % ssh-keygen 명령어로 key를 생성해준다.잘 생성되었는지 확인하고, ssh- 공개키 번호를 확인한다. 흰색블라인드 처리된 부분이 공개키이므로, 이를 복사해준다.(ssh-rsa부터 끝까지 모두 복사한다.)복사했다면, github 계정으로 들어가서 설정 -&amp;gt; SSH and GPG keys로 들어간다.들어가서 New SSH Key를 눌러주고, 공개키 Title과  복사한 Key를 입력해준다.정상적으로 인증이 되었다면 아래와 같은 화면을 볼 수 있다.다음으로 .gitignore 파일을 생성해준다. git이 모든 파일을 탐색할 필요가 없기 때문에 굳이 추적하지 않아도 되는 파일들을 .gitingore 파일에 작성해주면, git은 추적을 통과하게 된다. 깃이 설치된 경로에서 아래 명령어를 실행한다.% touch .gitignore% ls.gitignore이라는 파일이 보이지 않는다.맥의 경우 위 단축키로 숨김파일을 확인해보면,.폴더 안에 .git 과 .gitignore이 사이좋게 들어가있다.이제 .gitignore을 코드에디터로 실행해준다.이렇게 깃이 추적하지 않길 원하는 파일 혹은 폴더 명들을 상대경로로 적어준다. 이제 거의 끝났다. github repositories로 올려줄 것이다.깃 상태를 확인해보자.% git status는 현재 깃의 추적결과 (커밋 상태)를 확인해주니 습관적으로 실행해보도록 한다.git add를 해달라고 한다.% git add –all . 명령어를 통해 전부 깃에 추가해주고,add 후에는 반드시 커밋도 해준다. “” 안에는 자유롭게 커밋한 내용을 작성해주시면 된다.% git commit -m “My Django girls app, first commit”이제 깃허브 사이트에 들어가서 새 repository를 생성해준다.생성하면 이렇게 레포지토리 주소를 https로 확인할 수 있다. 복사해서 아래 명령어 &amp;lt;주소&amp;gt;에 넣어준다.% git remote add origin &amp;lt;주소&amp;gt; 내 깃을 저 깃허브 레포지토리 주소와 연결해준다.% git push -u origin master master(관리자)권한으로 git을 연결/복사한다.이렇게 명령어를 실행하면 아까 add 했던 내용들이 원격으로 넘어가서 이제 github에서도 확인할 수 있다관성을 이기는 데이터" }, { "title": "pythonanywhere - 장고 서버 구축", "url": "/posts/pythonanywhere-%EC%9E%A5%EA%B3%A0-%EC%84%9C%EB%B2%84-%EA%B5%AC%EC%B6%95/", "categories": "3. 튜토리얼, 비즈니스 통계 분석 프로그래밍", "tags": "", "date": "2020-05-17 00:00:00 +0900", "snippet": "웹으로 모델을 서빙해야 하는 상황이 자주 생긴다. pythonanywhere로 간단한 개인 서버를 띄워보고 싶다면, 딱 3가지만 생각하면 된다. 로컬에서 개발하고 Github에 원격으로 올리고 외부 호스팅 업체를 사용한다. 로컬 개발하고머신러닝 모델 배포나 일반 웹서비스를 위해 html 페이지를 로컬 서버에 띄워봤을 수 있다. 주소창에 localhost : ~ / ~ 3000  이런 식으로 나타나는데 이것이 로컬 서버다. 외부에서는 접속할 수 없다. Github에 원격으로 올리고내 컴퓨터에서 작성한 코드를 clone, post 와 같은 명령어로 github에서 가져오기도 하고 repositories로 올릴 수도 있다. 다른 사람들과 코드를 공유하고, 내 코드가 변경을 하다가 이전 버전으로 돌아가고 싶은 경우 모든 기록이 github에 남아있기 때문에 쉽게 복구할 수도 있다. 외부 호스팅 업체를 사용한다.AWS와 같이 서버를 임대해주는 서비스들이다. pythonanywhere의 경우 test 서버를 잠깐 사용해보기 위한 환경을 무료로 제공한다. 실제로 서버를 운영하게 되면, 일반적으로 트래픽 발생량에 따라 빌려쓴 서버 회사는 우리에게 비용을 청구한다. 이 3가지 과정은 항상 같이 움직인다. 그리고 실력이 늘고 더 고도화된 웹을 개발한다 하더라도 크게 다르지 않다. 생각해보자. 로컬 서버에서 여러가지 테스트를 하며 웹에 이것 저것 띄우고 만들어본다. 3일 밤낮을 새면서 만든 코드가 갑자기 사라지면 큰일나니 만드는 과정에서 깃에 add 해주고, github에 올려둔다. 그러다보니 멋진 웹이 탄생했고, pythonanywhere과 같은 호스팅 업체를 통해 외부 서버에 띄워서 다른 사람들도 이용할 수 있도록 했다. 이렇게 배포한 뒤에 트래픽 관리가 어려운 경우도 당연히 생길 수 있다. 이왕 비용을 더 지불할 겸, 더 안정적인 aws로 호스팅 업체를 바꾼다. 앗, 그런데 사용자들이 내 웹 페이지에 에러를 발견해서 알려줬다. 이럴때에는 코드를 수정해야하는데 무작정 바꿔버릴 수는 없다. 로컬 환경에서 테스트를 거치고 문제가 없다면 다시 깃허브를 거쳐 외부 서버에 올려 새로운 웹을 보여준다.그럼 이제 pythonanywhere 사이트를 사용해보자. 먼저 회원가입을 할 때, beginner로 설정해야 과금이 되지 않는다. 가입하면 이렇게 Dashboard를 볼 수 있다. Consoles에서 bash를 선택해서 git을 사용할 수 있게 열어준다.Bash를 열었으면, github에 원격으로 띄운 repository url을 복사해서 git clone 명령어를 통해 호스팅하는 서버에 repository를 생성한다.$ tree my-first-blog명령어를 통해 로컬환경과 구조를 비교해볼 수 있다. 코드는 ‘로컬 -&amp;gt; 깃허브 -&amp;gt; pythonanywhere’ 로 거쳐왔다는 것을 다시 기억하자.현재 호스팅 서버에 내 로컬에 있던 코드를 복사해왔으니, 개발을 위한 가상환경도 만들어주자. 로컬에서의 과정과 다르지 않다.$ cd my-first-blog my-first-blog 폴더로 들어간다.$ vertualenv –python=python3.6 myenv-여기서 ‘myenv’라는 이름의 가상환경을 만들어준다.$ source myenv/bin/activate-가상환경 myenv를 실행한다.그럼 코드 앞에 (myenv)가 표시된다.(가상환경에 잘 들어온 것이다.)$ pip install django~=2.0 익숙한 장고를 설치해본다.장고가 설치되었다면 DB를 초기화한다.$ python manage.py migrate 이어서 관리자계정을 만들어준다.계정 설정을 완료했다면 다시 pythonanywhere 사이트 Web 탭으로 들어가 Web app을 생성해준다.Next를 누르면 파이썬 버전과 프레임워크를 설정하는 부분이 나온다. Django를 선택하지 말고 manual configuration(수동 설정)을 선택한다. 파이썬은 우리가 계속 가상환경으로 세팅한 것처럼 3.6으로 지정한다.그럼 이렇게 성공적으로 Web app이 생성된 것을 확인할 수 있다. 이제 가상환경과 WSGI 파일을 설정해주도록 한다. 아래에 Virtualenv는 가상환경 경로를 설정해주는 부분이다. 앞서 bash에서 만든 가상환경 myenv의 경로를 작성해주면 된다./home/&amp;lt;계정&amp;gt;/my-first-blog/myenv 같은 형식이었다.다음으로 WSGI 파일에 코드를 작성해준다. 가상환경 바로 위 설정인 Code 항목에서 WSGI configuration file: (링크).py 파일을 클릭한다.아래와 같이 코드를 작성해주면 된다. 참고로 sw930601은 개인 계정 ID이므로 코드 항목(위 이미지)에서 Working directory가 어떤 이름으로 되어 있는지 각자 확인해야 한다.  이제 저장하고, 가장 상단의 사이트 주소를 클릭하고 웹을 구경해본다.로컬 환경에서 띄웠던 웹이 똑같이 나온다. 휴대폰을 들고 적힌 주소로 들어가보면 동일한 사이트가 나타난다. 이제 Rest api 등 통신 방식을 활용해 서버측 모델을 사용자에게 랜더링해주면 된다.관성을 이기는 데이터" }, { "title": "Python Django - blog, admin", "url": "/posts/Python-Django-blog-admin/", "categories": "1. 기술, 웹, 자바스크립트", "tags": "", "date": "2020-05-17 00:00:00 +0900", "snippet": " 장고걸스 튜토리얼을 참고했습니다.manage.py 가 있는 파일 경로로 들어가서 아래 명령어를 실행한다.% python manage.py startapp blog manage.py 파일이 있는 경로에서 blog 파일을 생성한다.그럼 이렇게 blog라는 이름의 폴더가 생성되고,apps.py 파일, migrations 폴더를 포함해 여러 가지 항목들이 패키지로 설치된 것을 확인할 수 있다. (*참고) 장고 걸스에서 가져온 현재 디렉터리 모습blog 모델을 생성했다면 이 App이 설치되었다고 알려줘야한다. mysite에서 다시 settings.py를 실행한다.INSTALLED_APPS를 찾아서 ‘blog’를 추가해준다.이렇게 ‘blog’ 앱이 설치된 것을 인식시켜줬다면, 다음으로 blog에 대해서 어떤 모습으로 구현할 것인지 상세 코드를 작성해줘야한다.blog에서 models.py를 연다.장고 걸스에서 표준으로 공유한 코드를 넣어준다. 아래 코드는 블로그로서 필요한 기능들을 넣는 동작을 수행한다.이제, 우리가 만든(models.py) blog에 대해 migration을 해주면,Post라는 새로운 모델이 생성된다.% python manage.py migrate blog 데이터베이스에 모델을 추가해준다.정상적으로 잘 반영이 되었고 생성된 Post모델을 불러와 admin에 등록한다. 관리자 페이지를 만드는 과정이다.이제 % python manage.py runserver를 입력해서 서버를 구동시켜본다.기존 서버 주소에 /admin/을 붙여주면 관리자 페이지로 이동한다. http://127.0.0.1:8000/admin/ 우리는 아직 아이디/비밀번호가 없기 때문에 superuser(관리자)를 생성해줘야 한다.잠깐 서버를 종료하고% python manage.py createsuperuser를 실행하면 계정을 생성하는 명령어들이 순차적으로 나온다.생성된 계정을 입력하면 아래와 같이 관리자 화면으로 넘어간다.Blog니까 글도 한번 써보자.포스팅까지 정상적으로 잘된다.관성을 이기는 데이터" }, { "title": "맥에 Django 설치", "url": "/posts/%EB%A7%A5%EC%97%90-Django-%EC%84%A4%EC%B9%98/", "categories": "1. 기술, 웹, 자바스크립트", "tags": "", "date": "2020-05-16 00:00:00 +0900", "snippet": "% python -m pip install django-먼저, 장고를 설치한다.% django-admin startproject mysite .-데이터베이스와 서버 세팅을 도와준다.그럼 장고를 설치한 폴더의 하위에 mysite 폴더가 생성되고, 아래와 같이 settings.py를 확인할 수 있다.열어서 몇가지 세팅을 해준다. TIME_ZONE은 웹에서 시간대 표시할 때 참고하는 지역이니 Asia/Seoul로 설정해주고,STATIC_ROOT = os.path.join(BASE_DIR, ‘static’) 를 통해 정적 파일 경로를 설정해준다.ALLOWED_HOSTS는 빈값으로 되어 있다. 아래와 같이 채워준다.데이터베이스는 sqlite3으로 구동되는 것으로 보인다.저장 후 terminal로 돌아가서 migration을 해준다.migration이 에러 없이 성공적으로 완료되었다면, 이제 서버를 실행시켜본다.http://127.0.0.1:8000/ -&amp;gt; 서버가 생성되었다! 들어가보면 이렇게 장고 페이지가 보인다.관성을 이기는 데이터" }, { "title": "맥 zsh - conda install 에러, pip install로 대체", "url": "/posts/%EB%A7%A5-zsh-conda-install-%EC%97%90%EB%9F%AC-pip-install%EB%A1%9C-%EB%8C%80%EC%B2%B4/", "categories": "3. 튜토리얼, 비즈니스 통계 분석 프로그래밍", "tags": "", "date": "2020-05-16 00:00:00 +0900", "snippet": "이번에 새로 산 맥에 장고를 설치하려다가 conda install 관련 에러가 있어 내용을 기록한다.우선, 사용자폴더에서 새롭게 sw_python이라는 폴더를 생성하고, 가상환경도 만들어 줬다. 가상환경 이름은 재미없지만 django_venv이다.가상환경은 잘 생성되었다. conda activate django_venv 명령어로 가상환경을 실행한다.이어서 conda install 명령어를 통해 django를 설치해줬다.그리고, 장고를 실행하기 위한 mysite와 하위 파일들을 생성하려고 시도해보았으나 No module named ‘django’ 메시지가 뜨면서 실행되지 않는다. 이어서 django –version 으로 설치가 되었는지 확인해보니 command not found: django 라고 알려준다. 아래 이미지에서 문제 상황을 볼 수 있다.혹시나해서 conda list 를 통해 django 가 설치되지 않았는지 다시 확인해보았다.django는 설치가 되어 있는데 인식을 못하는 것 같다. 이렇게 conda로 설치했을때 모듈을 찾지 못한다는 에러가 나오면 아나콘다 경로 문제일 수 있다. 복잡하니 그냥 pip로 설치해주자.잘 동작한다. mysite 폴더도 멀쩡하게 생성되었다.관성을 이기는 데이터" }, { "title": "맥(Mac) 터미널 경로설정 - 아나콘다(Anaconda), 2020년 5월 10일 기준 updated", "url": "/posts/%EB%A7%A5-Mac-%ED%84%B0%EB%AF%B8%EB%84%90-%EA%B2%BD%EB%A1%9C%EC%84%A4%EC%A0%95-%EC%95%84%EB%82%98%EC%BD%98%EB%8B%A4-Anaconda-2020%EB%85%84-5%EC%9B%94-10%EC%9D%BC-%EA%B8%B0%EC%A4%80/", "categories": "3. 튜토리얼, 비즈니스 통계 분석 프로그래밍", "tags": "", "date": "2020-05-10 00:00:00 +0900", "snippet": "오랜만에 맥에 아나콘다를 설치하고 가상 환경을 띄우려는데, 에러가 발생했다.구글링 해보면 대부분export PATH=”/Users/username/anaconda/bin:$PATH”로 설정하면 된다는 내용이 많다.동일하게 시도했을때, 아래와 같이 에러가 발생한다.초기화도 해보았으나 No action taken. 이라는 모호한 내용이 출력된다.구글링 해보니 2019년 말 기준 맥 터미널이 zsh환경이 default로 잡히면서 몇 가지 업데이트로 인해 경로를 다르게 설정해주어야 한단다.우선 경로는 conda.sh 파일이 있는 폴더로 찾아본다. ~~ anaconda3/etc/profile.d 경로 하위에 conda.sh 가 있기 때문에 아래와 같이 export 해준다.이제 아래와 같이 작성하고,$source conda.sh$conda activate임의의 가상 환경 (name:dataScience)을 실행한다.jupyter notebook을 진입한 가상환경에 설치해줘야한다..Go Go~ (설치 과정 생략) ~~(이하 생략)가상환경도 정상적으로 들어왔고, NotebookApp이 실행된 것을 확인할 수 있다.관성을 이기는 데이터" }, { "title": "JavaScript - &#39;객체 참조&#39;, assign()", "url": "/posts/JavaScript-%EA%B0%9D%EC%B2%B4-%EC%B0%B8%EC%A1%B0-assign/", "categories": "1. 기술, 웹, 자바스크립트", "tags": "", "date": "2019-11-10 00:00:00 +0900", "snippet": "자바스크립트에서 객체는 복사되지 않고, 참조된다. 이러한 이유에서 예상하지 못했던 값의 변화로 에러를 경험하곤 한다.사람은 현실에서든, 코드에서든 원본이 변하는 것을 좋아하지 않는다. 원본은 유일해야 하며 필요에 따라 수정할 경우 복사본을 이용하는 것이 좋다. 이런 점에서 객체를 참조하고, 원본을 수정할 수 있는 자바스크립트의 특성은취약점이다.자바스크립트는 prototype을 통해 얼마든지 객체 원본을 변형할 수 있다. ES2015부터 이 취약점을 해결하기 위해 assign()이라는 메서드를 제공하고 있다. 특정 객체를 전달할 때 원본을 참조하는 것이 아니라 새로 만들어서(복사) 전달하자는 컨셉이다.사실 assign() 메서드가 등장하기 전까지는 주로 json을 활용했다. 객체를 json 형태로 변환시키고 다시 변환된 json을 새로운 객체로 저장하면서 같은 내용의 서로 다른 객체를 만드는 방식이다. let car = { color:&#39;red&#39;, price:500, fe:9 } let carJson = JSON.stringify(car) //obj를 json 문자열로 변환let newCar = JSON.parse(carJson) // json 문자열을 객체로 변환console.log(newCar) // { color: &#39;red&#39;, price: 500, fe: 9 }그러나 json변환을 거치는 것은 코드가 길어지고 객체의 복사가 잦은 코드일수록 매우 피곤한 작업이 된다. 새로 등장한 assign() 메서드는 이에 비해 편리하고 간결한 문법을 제공한다.let car = { color:&#39;red&#39;, price:500, fe:9 }let newCar = Object.assign({}, car) // 새로운 객체 {}를 만들고 car 객체 복사console.log(newCar) // { color: &#39;red&#39;, price: 500, fe: 9 } The Object.assign() method is used to copy the values of all enumerable own properties from one or more source objects to a target object. It will return the target object.-mozilla.org문법의 명세 Object.assign(target, …sources)에서 assign은 다수의 parameters를 전달받는데, 첫 번째는 만들고자 하는 target object, 그 외에는 모두 복사하고자 하는 source objects다. 즉 위의 코드에서 newCar로 새로이 만들고자 하는 객체 {}를 target으로 설정해주고, source object로서 car 객체를 이용했다고 할 수 있다. assign() 메서드를 사용할 때에 있어서 주의할 점은 ’객체 병합’ 현상이다. 만들고자 하는 target object를 빈 객체로 두지 않고 먼저 내용을 담은 경우 source object에 같은 속성이 포함되어 있다면 ’덮어쓰기’를 한 것처럼 병합이 이루어진다.새로운 객체를 담을 변수 returnedTarget은 target object로 만들어진다. 그렇다는 것은 target(객체)은 returnedTarget과 내용이 동일하다는 것을 의미한다. assign() 메서드에서 객체를 만들고 그 객체를 returnedTarget에 반환한 것이므로 target을 확인해보면 { a : 1, b : 2}가 아닌 { a : 1, b : 4, c : 5 }로 출력되는 것이다.따라서 객체의 원본이 변하지 않도록 하기 위해서는 아래와 같이 target object는 빈 객체로 만들어주고 source objects를 넣어주는 방식으로 코드를 작성하는 것이 좋다.이렇게 assign() 메서드를 활용하면 객체의 원본을 유지하고 참조 관계를 완전히 단절시킬 수 있다.정리하면, 변수의 상태가 변하면 에러가 발생할 가능성이 증가하는데 특히 원본이 저장된 변수의 상태가 변하는 것은코드에 치명적인 문제를 야기할 수 있다.따라서 객체가 다른 변수에 넘겨질 때 “참조가 아닌 복사가 되도록”하기 위해 assign()을 적극적으로 사용할 필요가 있다.관성을 이기는 데이터" }, { "title": "JavaScript - 콜백", "url": "/posts/JavaScript-%EC%BD%9C%EB%B0%B1/", "categories": "1. 기술, 웹, 자바스크립트", "tags": "", "date": "2019-11-05 00:00:00 +0900", "snippet": "What is “callback hell”?Asynchronous JavaScript, or JavaScript that uses callbacks, is hard to get right intuitively. A lot of code ends up looking like this:fs.readdir(source, function (err, files) { if (err) { console.log(&#39;Error finding files: &#39; + err) } else { files.forEach(function (filename, fileIndex) { console.log(filename) gm(source + filename).size(function (err, values) { if (err) { console.log(&#39;Error identifying file size: &#39; + err) } else { console.log(filename + &#39; : &#39; + values) aspect = (values.width / values.height) widths.forEach(function (width, widthIndex) { height = Math.round(width / aspect) console.log(&#39;resizing &#39; + filename + &#39;to &#39; + height + &#39;x&#39; + height) this.resize(width, height).write(dest + &#39;w&#39; + width + &#39;_&#39; + filename, function(err) { if (err) console.log(&#39;Error writing file: &#39; + err) }) }.bind(this)) } }) }) }})http://callbackhell.com/위 코드는 자바스크립트에서 가장 악명 높은 콜백 지옥을 구현한 것이다. 코드를 작성할 때 비동기 처리를 해야할 경우가 자주 생기게 되는데, 이 때 작성자의 의식의 흐름대로 연속적으로 콜백 함수를 사용하다보면 이렇게 알아보기 힘들 정도로 코드가 복잡해진다. 이러한 시각적 복잡성을 해결하기 위해 es6부터 Promise라는 개념이 도입되었다.let p1 = new Promise(function(resolve, reject){ //true-&amp;gt;resolve, false-&amp;gt;reject setTimeout(function() { console.log(&#39;hello p1&#39;); resolve(); // ..............................성공했을 때, }, 1500);});let p2 = new Promise(function(resolve, reject){ setTimeout(function() { console.log(&#39;hello p2&#39;); reject(); // ..................................실패했을 때 }, 2300);});Promise.all([p1, p2]).then(function() { console.log(&#39;pass&#39;) // ..........둘 다 성공해야 출력}).catch(function() { console.log(&#39;failed&#39;) // ......실패가 있으면 출력});코드의 흐름은 다음과 같다.1. new를 통해 Promise를 생성해준다.2. Promise는 매개변수 resolve, reject를 갖는다3. 생성자 선언과 동시에 비동기 콜백이 각각 실행된다.4. 코드가 실행되면 1.5초 뒤 ‘hello p1’, 2.3초 뒤 ‘hello p2’가 실행되며 ’hello p2’는 reject일 때 실행된 것이므로 Promise.all- 에서는 catch 메소드가 실행된다.만약 resolve가 실행될 시간이 더 늦도록 다음과 같이 코드를 수정한다면 어떻게 출력될까?let p1 = new Promise(function(resolve, reject){ //true-&amp;gt;resolve, false-&amp;gt;reject setTimeout(function() { console.log(&#39;hello p1&#39;); resolve(); // ..............................성공했을 때, }, 3300);});let p2 = new Promise(function(resolve, reject){ setTimeout(function() { console.log(&#39;hello p2&#39;); reject(); // ..................................실패했을 때 }, 2300);});Promise.all([p1, p2]).then(function() { console.log(&#39;pass&#39;) // ..........둘 다 성공해야 출력}).catch(function() { console.log(&#39;failed&#39;) // ......실패가 있으면 출력});그렇다면 콘솔은1. hello p22. failed3. hello p1순서로 보여주는데, Promise.all- 에서 catch 메소드는 reject가 나오는 순간 바로 동작하기 때문이다. 이렇게 Promise는 상대적으로 콜백 지옥을 읽기가 수월한 형태로 보여준다. *비동기처리 뿐만 아니라  코드 작성 시점에서는 각각의 단위가 언제 확정될지 모르는 상황에서도  promise를 응용해서 다양하게 사용이 가능하다는 장점이 있다. 앞서 본 코드에서 볼 수 있듯이 Promise를 사용해도 콜백 함수가 그대로 노출되어 있기 때문에 함수로 둘러싼 형태는 여전히 존재하며 기존의 복잡한 코드를 문단으로 나눈 것 같은 효과에 그치게 된다.관성을 이기는 데이터" }, { "title": "JavaScript - 함수 Parameter vs Argument", "url": "/posts/JavaScript-%ED%95%A8%EC%88%98-Parameter-vs-Argument/", "categories": "1. 기술, 웹, 자바스크립트", "tags": "", "date": "2019-11-02 00:00:00 +0900", "snippet": "함수의 목적은 ’재사용’ 이다. 아주 간단한 sum() 함수를 살펴보자.function sum(x, y){ return x + y}console.log(sum(1,2)) //3console.log(sum(2,4)) //6console.log(sum(4,6)) //10console.log(sum(10,20)) //30 이 함수를 통해서 계산(합산)하고자 하는 데이터를 얼마든지 입력하고 그 결과를 반환받을 수 있다. 좀 더 복잡한 함수를 생각해보자. function impulse(x,y,z){ return ((x+y)-z)*x*y*z} console.log(impulse(3,2,1)) //24console.log(impulse(6,5,4)) //840console.log(impulse(30,20,10)) //240000console.log(impulse(9999999,8888,777)) //691157605537832500000세 물체의 무게(x, y, z)에 따른 충격량의 총합을 계산해주는(말도 안되지만 그렇다치자) complex() 함수가 있다고 가정해보자. 우리는 impulse(역학-충격량) 라는 함수 명을 통해 이것이 충격량을 계산해주는 식이라는 것을 예상할 수 있다. 그리고 함수 내부적으로 어떻게 동작하는지, 굳이 난해한(말도 안되는) 수학을 이해하지 않고도 함수를 사용해 값을 얻어낼 수 있게 된다.함수의 재사용성이다. 함수를 한 번 만들어 놓으면 100만 번이든 100억 번이든 얼마든지 쓰고 또 쓸 수 있다.parameter(매개변수)와 argument(전달 인자)가 무엇인지 그리고 어떻게 다른지 알아보겠다. 함수의 재사용성을 이해했다면 두 개념 역시 쉽게 이해할 수 있다.앞에서 sum()이라는 함수에 x, y라는 미지수가 들어있었다. 그리고 이 함수는 x + y를 return 했다. 함수가 받아내는 x와 y는 “변할 수 있는 값”이라는 뜻의 ‘변수’이며 ‘함수를 매개로 입력 값을 전달하는 변수’라는 의미에서 ‘매개변수’(parameter)라고 한다. 반대로 전달 인자는 이 매개변수를 통해 함수 내부로 전달하고자 하는 인자를 뜻한다. 즉, 입력할 ‘값’이다.sum(1,2)sum(2,4)sum(4,6)impulse(50,40,4)impulse(200,20,2)impulse(1000,100,200)위 함수 괄호() 안에 들어 있는 숫자(여기서는 숫자지만 문자가 올 수도 있다.)는 모두 ‘전달 인자’(argument)라고 부른다.함수를 만들 때매개변수를 작성해주고, 추후 함수를 이용하고자 할 때전달 인자를 입력해준다.관성을 이기는 데이터" }, { "title": "JavaScript - 가변인자, 클래스, 배열 순회", "url": "/posts/JavaScript-%EA%B0%80%EB%B3%80%EC%9D%B8%EC%9E%90-%ED%81%B4%EB%9E%98%EC%8A%A4-%EB%B0%B0%EC%97%B4-%EC%88%9C%ED%9A%8C/", "categories": "1. 기술, 웹, 자바스크립트", "tags": "", "date": "2019-11-02 00:00:00 +0900", "snippet": "자바스크립트의 높은 자유도는 협업을 하거나 에러를 수정할 때 불편함으로 다가온다. 따라서 고의적으로 명시적 코드를 작성해줄 필요가 있다. 처음 입문하더라도 명시적인 코드와 관련해 대표적인 3가지 유형을 알고 넘어가면 좋다. ‘가변 인자’, ‘클래스’ 그리고 ‘배열 순회’에 대해 알아보자.1. 가변 인자자바스크립트의 함수는 인자를 적게 받거나 다른 타입으로 받더라도 오류가 발생하지 않는다. 매개변수를 지정하지 않고 변할 수 있는 인자, 즉 가변 인자를 받을 수 있도록 되어 있기 때문이다.function sum() { let res = 0; for (let i = 0; i &amp;amp;lt; arguments.length; i++) { res += arguments[i]; // 선언하지 않은 arguments를 사용하고 있다. } return res;}arguments는 함수 내부적으로 가지고 있으며 우리가 선언하지 않았음에도 동작한다. arguments는 무엇일까?function sum() { let res = 0; for (let i = 0; i &amp;lt; arguments.length; i++) { res += arguments[i]; // 선언하지 않은 arguments를 사용하고 있다. } console.log(arguments); return res;}sum(1,2,3,4)코드를 실행하면 아래와 같이 출력된다.[Arguments] {&#39;0&#39;:1, &#39;1&#39;:2, &#39;2&#39;:3, &#39;3&#39;:4}arguments는 객체다. 이러한 형태의 객체를 유사 배열이라고도 한다. parameter가 정의되어 있지 않다는 점과 유사 배열로 동작한다는 점은 직관적이지 않고 불편하다. 유사배열이 아니라 우리에게 익숙한 (오리지날?)배열을 돌려받기 위해 같은 함수를 다른 방식으로 작성해보자.function sum(...args) { let res = 0; for (let i = 0; i &amp;lt; args.length; i++) { res += args[i]; // 선언하지 않은 arguments를 사용하고 있다. } console.log(args); return res;}sum(1,2,3,4)이렇게 작성하면, …args라는 parameter로 가변 인자가 들어온다는 것을 명시적으로 확인할 수 있다. 출력하면 아래와 같이 배열을 반환해준다.[1,2,3,4]마음이 편안하다.2. 클래스자바스크립트에서 클래스라는 문법이 생겨 클래스에서 constructor 명시할 수 있게 되었다. 또한 클래스는 new 연산자로 호출하지 않으면  즉시 에러가 난다는 점에서 명시적 코드를 요구한다.함수는 new 연산자 사용을 강제할 수 없지만 class는 반드시 new로 인스턴스를 생성해야만 동작한다. 함수 형태의 코드를 보자.function Car(make, model, year){ this.make = make; this.model = model; this.year = year;}// descriptiveCar 메소드는 상속받는 모든 객체에 필요하지 않기 때문에 prototype에 따로 저장한다.Car.prototype.descriptiveCar = function(){ return ` make : ${this.make} model : ${this.model} year : ${this.year} `;}위 함수와 동일한 기능을 하도록 클래스로 구현해보자.class Car { constructor(make, model, year) { this.make = make; this.model = model; this.year = year; } // 생성자에 바로 저장되는 부분 descriptiveCar() { return ` make : ${this.make} model : ${this.model} year : ${this.year} `; } // 프로토타입에 저장되는 부분}Car 클래스는 생성자다. 비유적으로 표현하면 붕어빵이라는 객체를 생성해내는 붕어빵 틀이다. Car라는 객체 생성 공장(생성자)을 통해 새로운 객체를 만들어보자.class fistCar extends Car { constructor(make, model, year) { super(make, model, year); }} // firstCar만 따로 가지는 속성이 없기 때문에 constructor를 작성해주지 않아도 되지만 명시하는 것이 권장된다.class secondCar extends Car { constructor(make, model, year) { super(make, model, year); } descriptiveCar() { return &#39;this is my second Car&#39; } // descriptiveCar 메소드를 오버라이딩. 하위 스코프에서 새로 메소드를 지정해주면 상위 스코프는 참조하지 않는다.}// secondCar는 descriptiveCar() 메소드를 직접 내장하고 있지 않고 프로토타입을 참조하므로 다음과 같이 호출해야 한다.console.log(secondCar.prototype.descriptiveCar())생성자를 constructor로 명시하고 상속관계를 만들어줘서 기존의 __proto__와 prototype으로 작성된 코드를 개선했다. 이렇게 클래스를 활용해 코드를 짜면 extends라는 키워드로 상속 관계를 좀 더 분명하게 확인할 수 있게 되고, 파이썬과 같은 다른 객체 지향 언어에서 사용하는 super()를 통해 부모 클래스에 대해 코드로 명시할 수 있어서 더 직관적이고 편리하다.기존에 함수로 상속을 구현할 때 상위 함수의 prototype을 직접 연결해줘야 했지만 자바스크립트에 class가 도입되면서 extends를 활용하면 이렇게 편하게 상속 관계를 만들 수 있다. 물론 자바스크립트의 class는 다른 객체 지향 언어의 개념과는 차이가 있기 때문에 __proto__ 그리고 prototype에 대한 이해를 필요로 한다.3. 배열 순회ES5부터 등장한 배열 순회 메소드는 다음과 같다.&amp;gt; forEach  map, filter,sort,reduce, every/some, indexOf, …이 중 자주 비교가 되는 forEach와 map을 다뤄보자.기존 자바스크립트는 배열의 데이터를 활용하고자 할 때 주로 for문과 같이 반복문을 사용했다. 하지만 조건문과 반복문처럼 조건에 값(데이터)이 들어가는 형태는 에러가 발생할 경우 종종 디버깅을 어렵게 한다. 값을 잘못 입력하거나 값이 저장된 변수가 변하면 코드는 동작하지만 잘못된 결과가 나올 수 있기 때문이다. (에러를 확인하기 어렵다는 뜻이다.)따라서 항상 코드를 작성할 때에는 최대한 값(데이터)을 분리시키는 것이 필요하다. 예시 코드를 보자.let car = [ {make : &#39;audi&#39;, model : &#39;a4&#39;}, {make : &#39;bmw&#39;, model : &#39;m4&#39;}, {make : &#39;kia&#39;, model : &#39;k5&#39;}];for(let i=0; i&amp;lt;car.length; i++) { console.log(`${i+1}. make : ${car[i].make} / model : ${car[i].model}`)}1. make : audi / model : a42. make : bmw / model : m43. make : kia / model : k5이러한 코드 형태를 ’명령형 패러다임‘이라고 한다. 조건을 매번 확인하고 실행하는 식으로, 변수 i가 1씩 증가하며 계속 교체된다. 이 방식은 인덱스를 잘못 잡거나 변수에 초기값이 잘못 들어가는 경우 초과 인덱스와 같은 형식으로 에러가 난다면 다행이지만 더 복잡한 코드에서는문제를 확인하기 어렵다.forEach()는 이런 문제를 해결한다.let car = [ {make : &#39;audi&#39;, model : &#39;a4&#39;}, {make : &#39;bmw&#39;, model : &#39;m4&#39;}, {make : &#39;kia&#39;, model : &#39;k5&#39;}];/*for(let i=0; i&amp;lt;car.length; i++) { console.log(`${i+1}. make : ${car[i].make} / model : ${car[i].model}`)}*/car.forEach(function(eachCar, idx) { console.log(`${idx+1}. make : ${eachCar.make} / model : ${eachCar.model}`);});forEach()는 전달인자로 함수를 받는다. 이 함수의 경우 콜백 함수로 세 인자를 받아 호출된다.첫번째 인자로 배열 원소의 값(currentValue)을, 두번째 인자로 각 원소의 인덱스(index)를, 세번째 인자로 배열(array) 자체를 받는다.보통 데이터인 배열 원소에 접근하고자 하므로 첫번째 인자만 기재하는 경우가 많다. 여기서는 인덱스까지 넣어주었다. forEach는 각 요소에 접근할 때마다 콜백함수를 호출하기 때문에 여기서는 콜백함수를 총 세번 호출했다.콘솔을 확인해보면 이전 코드와 동일한 결과값이 나온다.1. make : audi / model : a42. make : bmw / model : m43. make : kia / model : k5그렇다면 map은 어떻게 동작할까? map은 forEach와 같이 첫번째 전달인자로 함수를 받고, 그 함수 역시 동일하게 currentValue, index, array 순서로 인자를 가지고 호출되는 콜백 함수다. 차이점은 콜백함수의 결과(return)값들로 구성된 새로운 배열을 반환한다는 것이다.역시 코드를 보자.let car = [ {make : &#39;audi&#39;, model : &#39;a4&#39;}, {make : &#39;bmw&#39;, model : &#39;m4&#39;}, {make : &#39;kia&#39;, model : &#39;k5&#39;}];/*for(let i=0; i&amp;lt;car.length; i++) { console.log(`${i+1}. make : ${car[i].make} / model : ${car[i].model}`)}*//*car.forEach(function(eachCar, idx) { console.log(`${idx+1}. make : ${eachCar.make} / model : ${eachCar.model}`);});*/cars = car.map(function(eachCar, idx){ return `${idx+1}. make : ${eachCar.make} / model : ${eachCar.model}`;})console.log(cars)forEach를 사용할 때와 형태는 비슷하다. 눈에 띄는 차이가 있다면 forEach는 console.log를 바로 찍은 반면, map은 return을 해줬다는 점이다. 앞서  언급했듯 map의 기능은 return된 값들로 새로 배열을 만든다. 따라서 콘솔창을 확인해보면 하나의 배열로 묶여있다.[1. make : audi / model : a4 2. make : bmw / model : m4 3. make : kia / model : k5]관성을 이기는 데이터" }, { "title": "JavaScript - 객체 참조, concat()", "url": "/posts/JavaScript-%EA%B0%9D%EC%B2%B4-%EC%B0%B8%EC%A1%B0-concat/", "categories": "1. 기술, 웹, 자바스크립트", "tags": "", "date": "2019-10-26 00:00:00 +0900", "snippet": " 생활코딩 이고잉님 강의(JavaScript Immutability)수강 중 정리한 내용입니다.배열 객체는 내장된 push 메소드를 통해 배열에 원소를 추가하고, 추가한 원소를 반환할 수 있다.그런데 만약 여러 변수에 특정 배열을 똑같이 담고 그 특정 배열에 push를 통해 원소를 추가한다면 원소가 추가된 새로운 배열이 모든 변수에 공유된다.자바스크립트에서는 변수가 객체를 담으면 실제로는 객체의 위치를  가리키고 있는 것이기 때문에 이러한 값이 저장된 변수가 복사되었을 때 객체 자체가 복사되는 것이 아닌 하나의 위치에 있는 원본(유일한) 객체를 가리키는 위치가 복사된다.  그래서 push를 통해 원본(유일한) 객체에 원소를 추가한다면 공유된 여러 변수가 모두 변화된 값을 가진다.만약 이러한 특징을 이용해서 수많은 변수를 동시에 변경하려는 ‘의도’를 가지지 않고 이러한 특성을 잘 알지 못한 채 수 많은 변수를 동시에 변경시켜버리는 ‘실수’를 저지른다면 코드 전체가 망가질 것이다. 이렇게 오류를 범하는 일을 방지하기 위해 자바스크립트는 concat() 이라는 함수를 제공한다.concat()은 호출한 배열에 원소를 추가해서 ‘새로운 배열’을 만들고 그 배열을 가리키는 위치를 반환하는 메소드다. score는 수정되지 않았고 score에 4가 추가된 새로운 배열이 afterScore의 값으로 지정된 것을 볼 수 있다.관성을 이기는 데이터" }, { "title": "JavaScript - const(), freeze()", "url": "/posts/JavaScript-const-freeze/", "categories": "1. 기술, 웹, 자바스크립트", "tags": "", "date": "2019-10-26 00:00:00 +0900", "snippet": " 생활코딩 이고잉님 강의(JavaScript Immutability) 수강 중 정리한 내용입니다.자바스크립트의 높은 자유도로 인해 의도적으로 수정을 제한하는 문법이 생겨났는데, const와 freeze다. const는 변하지 않는 값. 상수를 지정하는 키워드로 let과 반대되는 개념이다. (const와 let이 등장하지 않았던 시절의 var와 비교해볼 수도 있다.)const 키워드를 통해 한번 값이 지정되면 그 값은 변할 수 없다. 값이 특정 메모리에 위치한다는 것을 생각했을 때 const로 지정된 ‘이름(name)’ 을 변경할 수 없다는 의미다. const가 나오기 이전 ES5에서 객체의 자유도에 제한을 주기 위한 메소드 freeze()가 등장했는데, const과 자주 비교되는 메소드다. const가 값 자체 혹은 이름을 변경할 수 없도록 해준다면 freeze()는 ‘객체를 얼린다’라는 의미로 객체의 내용(속성 값)을 고정시킨다.예시 코드를 확인해보자.이렇게 const로 선언한 o1은 o2로 변경할 수 없고(에러) freeze()로 얼린 o1의 속성 name 값(‘kim’)을 ’park’으로 변경하려고 했으나 여전히 {name:’kim’}을 유지하고 있는 것을 볼 수 있다. 이렇게 둘은 값을 불변하게 한다는 점에서 비슷한 것 같지만 분명한 차이가 있다.const()는 특정 값을 고정하는데 그 대상은 원시형 데이터의 경우 값 자체일 것이고 객체의 경우 객체를 가리키는 위치일 것이기 때문에 값 자체 혹은 가리키는 객체 위치를 바꿀 수 없도록 하지만 freeze()는 특정 위치에 있는 객체 원본(유일한)을 변경하지 못하게 한다. 그렇기 때문에 만약 let으로 선언한 o1을 얼리고(freeze), 다른 객체를 가리키도록 한다면 다음과 같이 수정될 수 있다.자바스크립트에서 var 대신 let과 const를 쓰는 것이 주류가 된 것처럼 점차 자바스크립트가 가진 자유도를 제한하고자 하는 코드를 작성하려는 움직임이 많아지고 있다. 읽기 쉽고 더욱 안전하며 협업에도 유리하기 때문이다. 자바스크립트의 자유를 맘껏 누리면서도 필요에 따라 가능하다면 명시적인 코드로 작성해보는 것이 좋겠다.관성을 이기는 데이터" }, { "title": "JavaScript - constructor, instance", "url": "/posts/JavaScript-constructor-instance/", "categories": "1. 기술, 웹, 자바스크립트", "tags": "", "date": "2019-10-25 00:00:00 +0900", "snippet": " Object는 Object instance를  생성하는 ‘생성자(constructor)’입니다.Function은 Function instance를 생성하는 ‘생성자(constructor)’입니다.이 문장의 뜻을 살펴보자. 인스턴스(instance)는 ‘객체’로 봐도 무방하다. 생성자가 객체를 만들 때, 그 시점의 객체를 인스턴스라고 하기 때문이다. 결국 맥락의 차이이기 때문에 특별히 다른 것으로 ‘억지로’ 생각하지 않아도 된다.그렇다면 생성자는 무엇일까? 생성자는 인스턴스를 만드는 객체 혹은 함수를 말하는데, 함수 인스턴스를 생성하는 생성자를 ’Function‘ 객체 인스턴스를 생성하는 생성자를 ’Object‘라고 한다.  우리는 자바스크립트에서 함수가 객체라는 것을 알고 있다. 그렇다면 두 생성자 Function과 Object는 어떤 관계가 있을까?자바스크립트에서 모든 객체를 생성할 때 생성자 Object가 존재한다. 그리고 Object는 자신의 prototype에 객체들이 공유할만한 property들을 가지고 있다. 객체 원형이라 불리는 이 Object는 함수 객체인데, 이것이 바로 Function의 instance다. 정리하면 자바스크립트에서 Object는 Function의 instance, Object 객체는 Object의 instance이며 결국 모든 객체가 Function(함수)에서 파생된 함수 객체다.관성을 이기는 데이터" }, { "title": "JavaScript - 객체와 Prototype", "url": "/posts/JavaScript-%EA%B0%9D%EC%B2%B4%EC%99%80-Prototype/", "categories": "1. 기술, 웹, 자바스크립트", "tags": "", "date": "2019-10-21 00:00:00 +0900", "snippet": " 페이스북 ‘프론트엔드개발그룹’ 에서 피드백받은 내용을 추가하였습니다.(표시 /*)자바스크립트는 프로토타입(Prototype) 기반 언어다. 자바, 파이썬처럼 객체지향이라는 점에서 맥락을 같이하지만 클래스 기반이 아닌 프로토타입 기반이기 때문에 ’상속’ 개념이 없다./*자바스크립트에도 상속이 있다고 합니다.class-based oop에서와 다른 방식의, 다른 형태의 상속으로,*ES 명세에 있는 다음 구절이 이를 잘 설명해주고 있습니다.“In a class-based object-oriented language, in general, state is carried by instances, methods are carried by classes, and inheritance is only of structure and behaviour. In ECMAScript, the state and methods are carried by objects, while structure, behaviour, and state are all inherited.”**/대신 자바스크립트는 프로토타입의 복사(Cloning)와 확장을 통해 새로운 객체를 만들어내게 된다. 이러한 차이점 때문에 자바스크립트에서 제대로 된 객체지향적 접근을 하기 위해서는 프로토타입에 대해 반드시 이해해야 한다. 프로토타입을 왜 사용해야 하는지 그리고 어떻게 동작하는지에 대해 간단한 메커니즘을 살펴보자.프로토타입은 ’객체 원형’을 말한다. 그리고 모든 객체는 최상위 객체인 객체 원형을 참조한다./*Object.prototype을 참조하지 않는 객체들도 존재한다고 합니다.예로 Object.create(null); 로 생성되는 객체가 있습니다.*/객체가 내용을 담고 있는 형태는 복사가 아닌 참조 형태로 그 원형의 위치를 가리키는 것과 같은 맥락이다. 물론 내부적으로 매우 복잡하게 연결된 구조를 가지기 때문에 Prototype Object와 Prototype Link에 대해 이해할 필요는 있지만 작성자가 부족하여 본문에서 다루지 않겠다.본론으로 돌아와서, 객체의 참조 방식은 메모리를 효율적으로 사용하는 것과 매우 밀접하게 관련이 있다. 프로토타입의 개념이 없다고 가정하면 코드의 양이 길어졌을 때 그리고 객체가 한눈에 보기 어려울 정도로 많아졌을 때 불필요한 메모리가 매우 많이 낭비된다. 왜 그럴까?대부분의 객체는 유사한 기능을 많이 가지고 있다. 가장 쉬운 예로 배열에는 항상 length가 있다. 사실 length는 알고리즘 문제에서도 매우 많이 접하는데 배열에 접근하고 사용할 때에 자주 등장한다. 코드는 재사용성이 매우 중요하다. 만약 이렇게 length와 같이 매우 자주 쓰이고 대부분의 객체에서 활용할 수 있는 속성을 모든 객체가 따로따로 원본 형태 그대로 가지고 있기보다 특정 객체에 필수적인 기능들을 만들어 놓고 모든 객체들이 이를 이용할 수 있도록 한다면 메모리를 절약할 수 있다./*(맥락상 수정 없이 인용했습니다)배열의 인스턴스의 length는 배열의 프로토타입 객체에 들어있는 프로퍼티가 아닙니다.(실제로 있긴 하나, 대부분의 경우에서 코딩할 때 참조하는 녀석이 얘는 아닙니다.). 각 인스턴스의 개별적인 값을 요구하기 때문에 프로토타입에 넣는 게 메모리 절약 면에서 큰 효과를 발휘하지도 않구요.*/여기서 말한 특정 객체는 Object라는  최상위 객체(전역 객체)다. Object는 객체가 필요한 주요 속성들을 모두 가지고 있고/*이 부분은‘대부분의 객체가 필요로 하는 기본적인 속성들’으로 이해해주시면 좋겠습니다.*/모든 객체는 Object에서 파생된다. 이해를 돕기 위해 빈 객체를 하나 만들어보았다.무슨 내용인지는 몰라도 분명 빈 객체를 만들었는데 객체 안에 __proto__라는 속성이 들어가 있는 것을 볼 수 있다. __proto__가 바로 우리가 만든 객체를 객체 원형 Object()와 연결해주는 숨겨진 속성이다./*정확히는Object Prototype을 참조합니다.사실 이 부분은 하단에부가 설명을 해두었지만오해의 소지가 있는 부분이라수정합니다.*/__proto__의 생성자(constructor)를 보면 Object()가 있다. 그리고 Object()가 함수(f)라는 것도 알 수 있다. __proto__ 는 단순히 객체 원형과 연결해주는 속성이 아니라 상위 객체를 가리키는데, 다음과 같이 상위 객체를 지정해주면 __proto__가 총 2번 등장해서 가장 깊숙하게는(최종적으로) 객체 원형까지 연결되는 모습을 볼 수 있다./*모든 객체의 프로토타입 체인 최상위 값은 객체 원형이 아닌‘null’이라고 합니다.*/마치 스코프를 보는 것 같다. 프로토타입 역시 스코프처럼 상하로 연결되는 구조를  가지고 있기 때문에 비슷하게 이해해주면 좋다. 이번에는 생성자 함수를 통해 __proto__를 살펴보자.new를 통해 생성자 함수를 사용했기 때문에 생성자(constructor)는 자기 자신이다. 그리고 prototype이라는 새로운 속성이 등장했는데, 이는 생성자 함수만 가질 수 있는 객체의 ’원본’을 뜻한다.prototype에서 똑같은 속성들이 반복되는데, 생성자 함수로 만들어졌기 때문에 자기 자신을 원형으로 가지고 있는 것이다.마지막으로, __proto__는 상위 객체, 최종적으로는 객체 원형을 가리킨다고 했지만 정확히는 상위 객체의 ‘prototype’를 가리키고 있다.  그리고 자바스크립트가 ES6부터 class라는 문법을 제공하지만 파이썬과 같이 다른 언어에서 사용하는 class와는 분명한 차이가 있다.관성을 이기는 데이터" }, { "title": "JavaScript - DOM(Document Object Model) 제어", "url": "/posts/JavaScript-DOM-Document-Object-Model-%EC%A0%9C%EC%96%B4/", "categories": "1. 기술, 웹, 자바스크립트", "tags": "", "date": "2019-10-20 00:00:00 +0900", "snippet": " DOM(Document Object Model)DOM은 브라우저가 제공하는 객체(BOM-Browser Object Model) 중 하나로 BOM의 가장 상위 객체인 Window의 하위 객체라고 볼 수 있다. 넓은 의미로 DOM은 웹브라우저가  HTML 문서를 인식하는 방식을 말하며 객체 참조를 통해 이루어진다. DOM이 제공하는 기능은 C(create), R(read), U(update), D(delete) 4가지다.DOM을 사용해보자.&amp;amp;lt;html&amp;amp;gt; &amp;amp;lt;head&amp;amp;gt; &amp;amp;lt;/head&amp;amp;gt; &amp;amp;lt;body&amp;amp;gt; &amp;amp;lt;div id=&#39;el&#39;&amp;amp;gt;&amp;amp;lt;/div&amp;amp;gt; &amp;amp;lt;/body&amp;amp;gt;&amp;amp;lt;/html&amp;amp;gt;javaScript로 Html을 조작하기 위해 DOM을 사용해서 접근할 수 있다.let elmts = document.getElementsByTagName(&#39;div&#39;);이렇게 getElements라는 복수 형태를 사용하면 변수 elements는 배열 형태로 div를 가진다.따라서 현재 코드에는 단 하나의 div만 존재하지만 div가 여러개 있다고 가정하면, 첫 번째 div에 접근하기 위해서는 elements[0]으로 불러내면 된다.만약 배열 인덱스를 이용하고 싶지 않다면 단수 형태로 변수를 만들어준다.let elmt = document.getElementById(&#39;el&#39;)이렇게 getElementById 메소드를 사용하면 id값으로 접근할 수 있고, 하나의 element만 특정해서 접근이 가능하다. 그러나 getElement는 Id를 지정해주지 않는 이상 클래스를 포함해 모든 태그를 복수형으로 받아와야 한다는 불편함이 있다.따라서 가장 첫 번째 요소만 가져오기를 원한다면, querySelector 를 사용해주는 것이 편하다.let elmt = document.querySelector(&#39;div&#39;);이렇게 해주면 가장 첫 번째 div element를 입력받게 된다. 뿐만아니라 querySelector를 사용하면 띄어쓰기(엘리먼트 하위), &amp;gt;(자식 태그), 아이디 선택자(#),클래스 선택자(.)를  통해 원하는 태그에 접근할 수 있다.따라서 querySelector는 태그명이 유일한 사용자 정의 태그에 접근하거나 특정 깊이의 위치에 있는 하나의 태그에 접근하기에 매우 효율적이다. 만약 getElementsByTagName을 사용한 것처럼 리스트로 돌려받고 싶다면 ’querySelectorAll‘을 사용하면 된다.웹이 점점 발전하면서 그저 HTML 문서를 보여주는 것이 아닌 사용자가 입력하고 브라우저가 반응하는 형태(반응형이 아니라 이벤트를 설명하고 있다.)의 UI가 필요해졌다. 그러나 순수한 자바스크립트만으로는 그저 작성된 그대로 시간 순서대로 한 줄씩 읽어나갈 뿐 사용자의 입력을 기다리는 식의 동작은 불가능하다. 대신 브라우저는 자체적으로 ‘Event’(사건, 이벤트)를 제공한다. 팝업을 띄우거나 입력을 받고 페이지를 전환하는 등의 기능을 말한다. 이것이 바로 DOM 객체의 역할이며 이러한 이벤트를 동작시킬 수 있는 메소드를 지닌다. 자바스크립트는 DOM 객체를 통해 브라우저가 제공하는 이벤트를 제어할 수 있다.&amp;amp;lt;div&amp;amp;gt; &amp;amp;lt;form id=&quot;js_obj&quot;&amp;amp;gt; &amp;amp;lt;button type=&#39;isevent&#39;&amp;amp;gt; &amp;amp;lt;span&amp;amp;gt;Click&amp;amp;lt;/span&amp;amp;gt; &amp;amp;lt;/button&amp;amp;gt; &amp;amp;lt;/form&amp;amp;gt;&amp;amp;lt;/div&amp;amp;gt;위 코드는 아래 캡쳐 화면과 같이 사용자가 클릭할 수 있는 버튼을 만들어낸다.Click 버튼을 누르게 되면 브라우저는 ’isevent’라는 이벤트를 발생시킨다. 이때 DOM은 ’isevent’ 이벤트를 제어할 수 있도록 기능을 제공한다.form = document.querySelector(&#39;#js_obj&#39;);function popAlert(event){ event.preventDefault() alert(&#39;hi&#39;)};form.addEventListener(&#39;isevent&#39;,popAlert);먼저 아이디 선택자(#js_obs)를 이용해 HTML의 버튼 form을 가져왔다. 그리고 form.addEventListener 메소드를 이용해 ‘isevent’ 이벤트를 동작시키면 이벤트를 기다리던 브라우저는 ‘hi’라는 메시지를 띄워준다.관성을 이기는 데이터" }, { "title": "JavaScript - Scope와 Closer", "url": "/posts/JavaScript-Scope%EC%99%80-Closer/", "categories": "1. 기술, 웹, 자바스크립트", "tags": "", "date": "2019-10-19 00:00:00 +0900", "snippet": "ES5까지 자바스크립트에서 변수를 참조할 수 있는 유효 범위의 단위는 함수였다. 예를 들어 코드에 함수가 하나 존재한다면 함수 밖의 변수는 Global Scope, 함수 안의 변수는 Local Scope 내에서만 호출할 수 있다.변수를 선언할 수 있는 유일한 방법은 var를 키워드를 사용하는 것인데, var는 변수의 중복과 키워드 생략을 허용한다는 점에서 의도하지 않은 결과를 나타낼 수 있다.그리고 유효범위, 즉 스코프의 단위가 함수라는 점에서 함수 내의 for문 또는 if문에서만 필요한 변수가 함수 전역에 머물러 있기 때문에 변수 사용이 끝난 시점에도 불필요한 메모리를 낭비하게 된다.변수를 선언할 때 ’var’ 키워드를 사용한다면 이때의 변수는 항상 포함된 함수 scope 혹은 Global Scope에 만들어진다. 따라서 중복해서 변수 이름이 사용될 때의 에러 발생을 사전에 방지하기 위해서라도 변수 선언에는 let, const 만 사용하는 게 좋다. 뿐만 아니라 var 키워드를 통해 변수를 선언하게 되면 호이스팅(Hoisting) 문제도 존재하는데, 호이스팅은 var 선언문이나 function 선언문을 해당 스코프의 선두에 옮긴 듯한 동작을 말한다.먼저 변수가 생성되는 과정을 보면 선언,  초기화, 값 할당으로 이루어져 있다. 그런데 var를 사용하면 선언과 초기화가 동시에 이루어진다.이와 달리 let, const는 초기화 단계가 변수를 선언하는 라인에서 이루어지기 때문에 그보다 앞서 호출하게 되면 참조 에러(ReferenceError)가 발생한다. 사실 자바스크립트는 let, const를 포함해 모든 선언문을 호이스팅하기 때문에 스코프가 시작됨과 동시에 선언이 이루어진다고 볼 수 있다. 다만 var는 스코프가 시작될 때 이미 해당 메모리를 확보하고 변수를 ‘undefined’로 초기화까지 해버리기 때문에 선언 라인 이전에 함수를 호출해도 에러가 발생하지 않는다는 점에서 차이가 있다.현대식 자바스크립트 문법에서는 이러한 복잡성을 최소화하고 자바스크립트의 지나친 자유도를 의도적으로 막는 추세이므로 var라는 키워드는 머릿속에서 지우는 편이 좋다. 앞서 ES5까지 스코프의 단위가 함수라고 했다. ES6부터 자바스크립트가 let, const를 지원하면서 자바스크립트가 따르는  ’함수 레벨 스코프’에 추가로 ’블록 레벨 스코프’에만 존재하는 변수를 선언하는 것이 가능해졌다. 이를 통해서 함수의 중복을 방지하고 메모리를 효율적으로 관리할 수 있게 되었다. 그리고 스코프와 관련해서 반드시 알아야 할 필수적인 개념이 있는데 바로 클로저(Closer)다. 자바스크립트에서는 함수가 리턴(return)되는 순간 함수가 가지고 있던 변수가 저장되어서 함수 스코프가 사라져도 변수에 접근할 수 있는데, 스코프 안에 스코프가 존재하는 형태인 스코프 체인(ScopeChain)에서 변수의 값을 보존하고 넘겨받는 개념으로 클로저라 부른다.이를 통해 외부의 접근으로부터 값을 은닉해 보호하거나 인자 혹은 콜백 함수의 사용을 위해 잠시 필요한 함수를 일회성으로  용하는 등으로 유용하게 활용할 수 있다.관성을 이기는 데이터" }, { "title": "JavaScript - 객체와 동적 바인딩, 값의 이동", "url": "/posts/JavaScript-%EA%B0%9D%EC%B2%B4%EC%99%80-%EB%8F%99%EC%A0%81-%EB%B0%94%EC%9D%B8%EB%94%A9-%EA%B0%92%EC%9D%98-%EC%9D%B4%EB%8F%99/", "categories": "3. 튜토리얼, 시계열 예측 및 계량 분석 방법론", "tags": "", "date": "2019-10-08 00:00:00 +0900", "snippet": "객체란  객체는 데이터를 표현하는 도구이면서 현실의 구체적 대상을 추상적으로 묘사하기 위해 사용된다. 코드는 언제나 현실의 추상화 작업을 거치게 되는데, 추상화란 보여주고자 하는 핵심적인 요소 이외의 것들은 모두 은폐하는 것을 말한다. 복잡한 현실세계를 모두 코드로 표현할 수 없을뿐더러 표현했다 하더라도 그것을 읽는 것은 불가능하다. 따라서 추상화가 필요하며 이때 가장 핵심적인 것들을 객체에 담아 보여준다.객체의 구성 Property  앞에서 객체를 데이터를 표현하는 도구라고 했다. 이는 property라는 형태를 가지고 있기 때문이다. property란, 객체를 통해 접근할 수 있는 key와 그런 key가 가지는 대응하는 값 value가 서로 쌍을 이루는 형태를 말하며 이를 통해 데이터를 불러와 사용하거나 반대로 입력할 수 있다. Method  객체가 단순히 값(데이터)만 다루는 것이라면 현실의 복잡한 문제를 담을 수 없을 것이다. 현실에는 데이터 이외에 시간의 흐름에 따라 존재하는 ‘동작’이 존재한다. 코드에서는 함수가 이 역할을 담당하며 객체에서도 함수를 사용할 수 있다. 자바스크립트에서 함수는 ‘값’이기 때문에 객체의 속성과 쌍을 이루는 값으로 함수를 배치시킬 수 있다. 이렇게 함수가 배치되면서 객체는 ‘동작’을 할 수 있게 된다. 즉 Property가 객체를 통해 활용가능한 데이터라면 Method는 객체가 가지는 기능인 것이다.객체 생성 방식 리터럴  객체를 생성하는 가장 기본적인 형태로, 코드를 작성하는 사람이 일방적으로 데이터나 기능을 모두 구현해 놓은 모습이다.let serve = { rice : 2, water : 2, kimchi : 1, bread : 1, beef : 2} 이는 처음 자바스크립트에 입문하면 자주 볼 수 있는 객체의 형태로, 직관적이며 이해하기 쉽다. 그러나 이미 고정된 형태로 주어지기 때문에 코드 보수나 수정 혹은 재사용성에 있어서 좋은 형태는 아니다. 항상 코드를 작성할 때에는 데이터와 코드를 분리하는 습관을 가지는 것이 권장된다. 함수  데이터와 코드를 분리한 객체의 형태로, 함수의 인자를 통해 객체 데이터를 입력받을 수 있다. function order(rice, water, kimchi, bread, beef){ let serve = { rice : rice, water : water, kimchi : kimchi, bread : bread, beef : beef }; return serve;} 이렇게 객체를 다시 함수로 둘러싸게 되면 코드가 길어지지만 단 한번 객체를 선언해주면 데이터의 변화가 있을 때 또 다른 객체를 선언하거나 객체 자체를 수정할 필요가 없어진다. 함수 호출 시 데이터를 입력하는 형태이기 때문에 재사용이 가능하고 수정이 쉬워지는 것이다. 따라서 코드의 효율이 높다. new  다음은 자바스크립트에서 제공하는 문법 ‘new’ 연산자를 사용해 객체를 만드는 방식이다. function order(rice, water, kimchi, bread, beef){ this.rice = rice, this.water = water, this.kimchi = kimchi, this.bread = bread, this.beef = beef}let serve = new order(2,2,1,1,2) 앞에서 본 것처럼 함수 안에 객체를 새로 선언하는 형태와 달리 ‘this’라는 빈 객체를 통해 값을 주고 있다. this는 new 연산자를 통해 함수를 호출함과 동시에 함수 내에서 만들어지는 객체로 처음에는 아무 값도 가지지 않은 상태다. 이렇게 비어있는 객체에 속성을 추가하고 객체의 데이터에 접근(데이터 생성)할 수 있으며 호출된 함수의 반환값은 this가 된다. 그리고 당연하게도 새로운 객체를 형성하는 구조가 아닌 존재하는 객체를 사용하는 형태이기 때문에 콜론(:)이 아닌 ‘=’를 통해 데이터를 입력받는다.동적 바인딩  new 연산자를 통해 객체를 만드는 방식을 알아보았는데, 사실 여기서 동적 바인딩의 개념을 확인할 수 있다. 동적 바인딩이란 객체를 생성한 이후에 객체의 속성을 추가 혹은 삭제하는 것을 말한다. 예를 들어 this.rice = rice는 rice 속성과 인자로 들어온 rice 값을 함께 this 객체에 추가해준 것이다. 동적 바인딩의 형태는 크게 두 가지인데, 하나는 this.rice 와 같이 객체의 속성을 식별자 이름(rice), 즉 코드 그대로 가져오는 것이고 다른 하나는 this[‘rice’] 처럼 문자열 값으로 가져오는 것이다. 문자열 형태로 가져온다는 것은 this[‘rice-double’] 혹은 this[‘rice double’]과 같이 특수문자나 띄어쓰기도 포함할 수 있다는 것을 말한다. 코드에서 빠져나와 데이터만 입력받는 공간에서는 값을 가져오는 동적 바인딩 형태를 취하는 것이 좋다.값의 이동  값의 이동을 이해하는 것은 변수와 객체의 사용에 앞서 매우 중요한 부분이다. 변수는 데이터를 담는 무형의 상자와 같다. 그리고 데이터는 크게 원시형 데이터와 객체로 나뉘는데, 숫자 혹은 문자열과 같은 원시형 데이터는 변수에 그대로 담을 수 있는 반면 객체는 그것이 메모리 안의 특정 공간에 존재하는 위치를 ‘전달’한다. 즉, 변수는 객체 자체가 아니라 객체가 존재하는 메모리의 위치를 담고 있는 것이다. 그런 점에서 변수는 객체를 참조한다고 하며 값의 이동은 서로 다르게 작동한다.let a = 1let b = a //b = 1a = 2 //b = 1 모든 값은 유일하다. 다시 말해 a=1, b=a에서 b는 a가 가지고 있는 1을 동시에 가지는 것이 아니라 ‘복사’해서 새로운 1을 저장한다. 그렇기 때문에 a의 값을 2로 변경해도 b는 여전히 1을 가지고 있는 것이다. 이와 달리 객체는 그 자체가 복사되지 않으며 유일한 원본 형태를 유지한다. 그 이유는 변수가 객체를 직접 담고 있는 것이 아니라 객체의 ‘위치’를 가리키고 있기 때문이다.let a = { x:1 };let b = a; // b = { x:1 }a.x = 2; // b = { x:2 }  b=a를 통해 b는 a가 참조하고 있는 객체의 위치와 같은 곳을 가리키게 된다. a.x = 2는 객체 원본을 변경하는 것이며 b 역시 참조하는 객체가 수정되었지만 b가 가지고 있는 참조 위치는 그대로이기 때문에 여전히 a가 가리키는 변경된 객체를 가리키고 있다. 만약 a가 참조하는 객체를 건드리지 않고 새로운 위치 값을 가지거나, 새로운 원시형 데이터를 가진다면 당연하게도 b에는 아무런 영향을 주지 않는다. let a = { x:1 };let b = a; // b = { x:1 }a = 2; // b = { x:1 }  이렇게 객체를 참조하는 변수에서는 값의 이동 이후에도 변수간 연결이 일어날 수 있고 연결이 끊어질 수도 있다는 점에서 예상치 못한 에러를 겪을 수 있다. 따라서 이러한 유형의 값을 변경하고자 할 때에는 변수의 참조값을 변경할 것인지(참조하는 객체 수정), 참조값을 바라보는 위치값을 변경할 것인지(새로운 객체 전달) 아니면 새로운 데이터 타입으로 변경하는 것인지를 분명히 하는 것이 필요하다.관성을 이기는 데이터" }, { "title": "JavaScript - 6 원칙", "url": "/posts/JavaScript-6-%EC%9B%90%EC%B9%99/", "categories": "3. 튜토리얼, 시계열 예측 및 계량 분석 방법론", "tags": "", "date": "2019-10-05 00:00:00 +0900", "snippet": " 우아한형제들 김민태님의 ‘자바스크립트 부트캠프(패스트캠퍼스)’ 수강 중, 허가를 받고 정리한 내용입니다.1. 코드 중간중간에 데이터를 넣지 마라.  코드는 읽는 시간이 작성하는 시간보다 훨씬 길다. 따라서 읽고 이해하기 쉽게 데이터는 데이터대로 따로 배치하는 것이 좋다. 그래서 간단한 수식 하나라도 변수를 통해, 네이밍을 해주는 것이 중요하다. 처음 코딩에 입문할 때 변수를 따로 선언해주는 것이 더 복잡하게 느껴지고 코드의 길이도 길어지는 것 같아 의식의 흐름대로 작성하는 경우가 많은데, 그렇게 되면 추후에 수정하거나 변경할 일이 생기면 알아보기도 어려울뿐더러 수정은 더더욱 어렵다. 그러니 변수 선언과 적절한 네이밍은 습관처럼 연습하자.2. if, switch문을 남발하지 말자.  코드에 대한 정확한 이해 없이 코딩 테스트를 준비하거나 알고리즘 공부를 해본 경우, if문을 남발해본 경우가 있을 것이다. 이것 역시 의식의 흐름대로 코드를 작성한 경우인데, 함수를 정해두고 필요한 때에 적절히 사용하는 것과 처음부터 끝까지 조건문으로 작성하는 경우는 코드의 질이 다를 수밖에 없다. 자바스크립트는 함수의 활용 능력에 따라 역량이 달라진다는 것을 명심하자. 그리고 조건문에서는 항상 데이터가 코드 내부에 조건으로 따라붙는데, 이는 앞 단락에서 언급한 이유로 역시 사용을 최소화하는 것이 좋다.3. 중복을 제거하자.  중복 제거는 사실 코드 리팩토링에서 가장 첫 번째로 고려해야 할 사항이다. 중복된 코드를 변수에 할당해주고, 코드 전체의 길이를 줄여주는 것은 가독성을 높이는 데에 이점을 주기 때문에 반드시 중복을 끝까지 찾아내서 적절하게 변환해줘야 한다.4. 함수와 객체를 이해하자.  현대적 자바스크립트 코딩 방식은 문’보다 ‘식’을 주로 쓰는 스타일을 선호하기 때문에 습관적으로 조건문, 반복문부터 사용할 것이 아니라 함수와 객체를 잘 사용하기 위해 노력하는 것이 권장된다.  자바스크립트에서 데이터는 각각 하나의 값이며, 그러한 데이터 중 원시 데이터 타입을 제외하면 모든 데이터 타입이 객체다. 즉 배열, 함수, 정규표현식 등은 모두 객체로 표현되며 동시에 그 자체로 값이다. 함수가 값이라는 것은 항상 호출에 실패하지 않으며, 특정 값을 반환한다는 것을 말한다. 또한 인수 처리가 느슨한 자바스크립트의 특성상 주어진 인수의 수가 맞지 않아도 함수 호출에 실패하지 않는다. 그러한 특징 때문에 함수를 사용할 때 인자 값 검증 역시 매우 중요하다. 또한 함수는 ‘문’으로도 만들 수 있는데, 변수에 할당하지 않고 이름을 가지고 있으며 function example(){–return A}와 같이 선언만 해주는 형태이다. 이렇게 선언된 함수는 코드에서 값으로 사용된다. 이와 달리 함수 표현’식’은 재귀 함수 형태가 아닌 이상 이름을 가질 필요는 적다. 변수에 이름을 지정해서 사용하면 되기 때문이다. 마지막으로 자바스크립트의 함수 트렌드는 기호를 사용하는 화살표 함수(한 줄 함수)이므로 꼭 연습하자.5. 객체지향 프로그래밍과 캡슐화  함수형 언어이자 객체지향 언어인 자바스크립트는 내부적으로 코드는 한 줄씩 읽어나가겠지만 사람은 그렇게 기계적으로 한 줄씩 읽을 수가 없다. 따라서 항상 코드는 이해하기 쉽도록 작성해야 하고 그때 필요한 것이 캡슐화다. 특정 함수 혹은 객체가 내부적으로 어떻게 구현되어 있는지 상세히 찾아보지 않아도 적절한 네이밍과 필요한 인자만 알기 쉽게 보여준다면  엄청난 양의 코드를 이해하는 데에 시간을 덜 쏟아도 될 것이다. 다만 자바스크립트가 인자 입력을 매우 유연하게 받는 특징 때문에 사용하기는 쉽지만 에러의 원인을 감지하기가 어려웠는데, ES6부터는 function sum(a, b… args)와 같은 방식으로 함수를 선언할 수 있게 되었다. 다시 한번 언급하지만 코드를 작성하는 것보다 읽고 이해하는 시간이 훨씬 오래 걸리기 때문에 코드를 몇 글자 더 작성하는 것이 피곤하다고 느끼지 말고 어떻게 하면 더 명시적으로 이해하기 쉽게 작성할 수 있을까? 하는 고민을 항상 하도록 하자.6. 세미콜론(;) 작성하기  자바스크립트는 모든 ‘식’의 끝에 세미콜론(;)을 붙여주기로 약속되어 있다. 물론 자바스크립트는 내부적으로 세미콜론(;)이 없어도 ‘대부분’ 오류 없이 해석할 수 있다. 단, ‘대부분’이라는 것은 100%는 아니라는 것을 말한다. 언제나 코드는 티끌 하나 차이 없이 완벽해야 한다. 그래야만 어떤 상황에서든 똑같은 결과를 보여주기 때문이다. 세미콜론(;)을 빼먹는 실수 때문에 혹시라도 원하는 결괏값이 나오지 않는다면 그 작은 오류를 찾는데에 수많은 시간을 써야만 할 것이다. 귀찮다고 생각하지 말고 아주 작은 양의 코드를 작성할 때에도 항상 세미콜론(;)을 붙이는 습관을 들이자. 글에도 문장이 끝났다는 것을 보여주는 마침표(.)가 있듯이 자바스크립트에서 하나의 실행 단위를 보여주는 ‘식’에도 세미콜론(;)을 붙여줘야 안정적이고, 보기 좋은 코드가 아니겠는가.관성을 이기는 데이터" }, { "title": "JavaScript - 식, 그리고 문", "url": "/posts/JavaScript-%EC%8B%9D-%EA%B7%B8%EB%A6%AC%EA%B3%A0-%EB%AC%B8/", "categories": "4. 실전, 글로벌 매크로 분석", "tags": "", "date": "2019-09-28 00:00:00 +0900", "snippet": "자바스크립트의 문법은 크게 ‘식’과 ‘문’으로 나뉜다.식(expression)은 곧 값을 의미하며 문(statement)은 식을 이루는 방식을 지시, 제어한다. 예를 들어 if(조건문)나 while(반복문)과 같은 문법이 ’문’이라면  1+1, 2&amp;gt;1, [a,b,c], function(x,y) 등은 ’식’이다.단, 값의 이동은 ‘문’이다. var a = 1과 같은 대입 연산을 말한다. 여기서 또 하나의 특징은 값이라고 정의된 모든 데이터는 변수에 넣을 수 있다는 것이다. 이러한 특징 때문에 함수와 배열 역시 특정 변수에 담을 수 있다. 그리고 함수는 ‘식’이기 때문에 항상 ‘값’을 반환하는데 이때 우리는 ’return’을 사용해서 함수를 통해 호출하고자 하는 값을 지정해줄 수 있다.반대로  if, while, for와 같은 ’문’은 실행의 흐름을 제어할 뿐 어떤 값을 스스로 반환하지 않기 때문에 ’return’을 사용할 수 없으며 내부에서 ‘식’을 활용해 값을 할당하는 방식을 사용한다.관성을 이기는 데이터" }, { "title": "블록체인 개념적 구현(javascript)", "url": "/posts/%EB%B8%94%EB%A1%9D%EC%B2%B4%EC%9D%B8-%EA%B0%9C%EB%85%90%EC%A0%81-%EA%B5%AC%ED%98%84-javascript/", "categories": "1. 기술, 머신러닝, 딥러닝", "tags": "", "date": "2019-09-26 00:00:00 +0900", "snippet": "블록체인은 데이터를 여러 블록으로 이어붙이며 저장하고, 저장하는 과정에서 Hash(문자열과 같은 특정 데이터를 해시 함수를 통해 일정한 길이의 데이터로 변환한 값)변환을 수행하기 때문에 양자 연산이 아닌 이상 해독하는 것은 어렵다.5분안에 블록체인을 만들어보자. 물론 우리가 만들 블록체인은 누군가와 거래를 하거나 장부에 거래 내용을 기록하거나 하는 서비스용 네트워크는 아다. 혼자 가지고 놀 LEGO를 조립하는 수준이다. 하지만 블록체인이 도대체 어떻게 생겨먹었는지 알기 위해 딱 좋은 난이도라 생각한다.전체 코드는 아래와 같다.const crypto = require(&#39;crypto&#39;);let blockchain = [];const genesisBlock = { index: blockchain.length, timestamp: Date(), data: &quot;1stblock&quot;, dataHash: crypto.createHash(&#39;sha256&#39;).update(&quot;firstblock&quot;).digest(&#39;hex&#39;), previousHash: &#39;&#39;, headerHash: crypto.createHash(&#39;sha256&#39;).update(&quot;firstblockheader&quot;).digest(&#39;hex&#39;)}let createBlock = function(data){ let block = { index : blockchain.length, timestamp : Date(), data : data, dataHash : crypto.createHash(&#39;sha256&#39;).update(data).digest(&#39;hex&#39;), previousHash : blockchain[blockchain.length-1].headerHash, headerHash : crypto.createHash(&#39;sha256&#39;).update( data + genesisBlock.previousHash).digest(&#39;hex&#39;) } blockchain.push(block);}blockchain.push(genesisBlock);createBlock(&#39;2ndblock&#39;);createBlock(&#39;3rdblock&#39;);createBlock(&#39;4thblock&#39;);createBlock(&#39;5thblock&#39;);console.log(blockchain);코드를 실행하면 아래 이미지의 우측 화면과 같이 각 블록 안에 담긴 내용물을 확인할 수 있다.Visual Studio Code먼저 위 코드는 세 영역으로 구성된다.Visual Studio Code 해시 함수 사용을 위해 ‘crypto’ 모듈을 호출하고, 블록체인 배열을 담을 변수를 생성한다. 제네시스(생성자, init 같은 역할) 블록을 생성하고, 그 뒤로 블록을 이어 붙이는 함수를 작성한다. 함수를 실행해서 블록체인을 구성하고 콘솔을 통해 실행한다.그렇게 실행된 오른쪽 콘솔 화면은 제네시스 블록부터 시작해서 총 5개의 블록을 보여주고 있다.Visual Studio Code내용물을 살펴보자. 각각의 블록을 대표하는 headerHash는 모두 그다음 블록의 previousHash와 같은 값을 가지고 있다. 즉 모든 블록이 headerHash와 previousHash로 연결이 되어있는 구조다.만약, 특정 블록의 데이터를 고의로 변경한다면 해당 블록의 headerHash가 변경되고 변경된 headerHash는 그 다음 블록의 previousHash와 일치하지 않기 때문에 블록체인 내부에서 변경된 블록을 포함한 일련의 과정은 모두 무효로 처리된다. 물론 이러한 프로세스는 현재 코드에는 구현되어 있지 않다.무작위(인간의 수준에서)로 나열된 hash는  어떻게 만들어진 것일까?Visual Studio Code여기서는 편의상 ‘crypto’ 라는 모듈을 호출해서 이미 구현된 ’SHA256’이라는 해시 함수를 사용했다. ‘SHA’는 Secure Hash Algorithm이라는 뜻에서 NSA에서 설계한 표준 암호 알고리즘이다.update()는 암호화하고자 하는 데이터를, digest()는 보여주고자 하는 방식을 입력받는다. 여기서는 ‘firstblock’이라는 데이터를 SHA256 함수로 암호화하고 ’hex’ 방식을 통해 Hash값을 얻었다.그리고 Timestamp라는 변수를 볼 수 있는데, 비트코인의 Timestamp와 조금 다르다. 비트코인에서 Timestamp는 유닉스 시간으로 1970년 1월 1일 00:00:00시부터 현재까지 경과한 시간을 초로 환산하여 저장하고 있지만 여기서는 이해를 돕기 위해 date() 함수를 통해 현재 시간을 그대로 입력받고 있다.Visual Studio Code시간순으로 기록한다는 점이 블록체인의 핵심이고, 각 블록은 그 순서대로 고유 번호를 부여받는다. 이러한 특성을 구현하기 위해 index라는 변수를 두고 blockchain.length를 통해 블록체인의 길이를 저장하는 방식으로 구현했다. 그리고 createBlock()이라는 함수는 argument로 ‘data’를 입력받는데 여기서는 블록 순서를 네이밍 해서 입력해주었지만 거래 내역 혹은 기록할 정보들을 문자열 그대로 입력할 수도 있다.참고로, 블록을 만드는 과정에서 해시 함수를 사용할 때 주의할 점은 전역 변수로 지정하지 않은 ’headerHash’ 변수를 그대로 사용할 수는 없다는 것이다. 전역 변수는 프로그래밍할 때 스코프를 고려하지 못해 나타나는 에러로, 블록체인을 구현할 때 특히 헷갈릴 수 있다.Visual Studio Code이렇게 우리는 ‘previousHash’와 ’headerHash’를 연결하는 것을 통해 미니 블록체인을 구현해보았다. 이번 실습으로 블록체인에서 해시함수가 담당하는 ’위변조를 방지하는 역할’에 대해 이해했다면 이제는 이를 기반으로 로직을 한 층씩 더 쌓아나가며 더 큰 블록체인을 만들어갈 수 있다.관성을 이기는 데이터" }, { "title": "Web - HTML", "url": "/posts/Web-HTML/", "categories": "1. 기술, 웹, 자바스크립트", "tags": "", "date": "2019-09-24 00:00:00 +0900", "snippet": "HyperText Markup LanguageHTML은 프로그래밍 언어가 아닌 ’마크업 언어’ 라고 불린다. 즉, 제목과 단락을 구분하고, 문서의 전체적인 틀을 표시하는 다양한 ‘태그’들로 이루어진 ‘약속된 표기 방식’정도로 이해할 수 있다. 그러한 이유로 마크업 언어인 HTML은 시간의 흐름과 제어에 따라 한 줄씩 읽어나가며 실행시키는 JavaScript와 같은 프로그래밍 언어라고 보기 어렵다.참고로 현재 가장 진보된 HTML인 HTML5에서는 ’시맨틱 태그(Semantics)‘가 추가되면서 코드를 볼 때 글의 각 구성 요소에 대해 명확한 구분이 가능하게 되었다.이제 HTML 코드가 어떻게 생겼는지 살펴보자. 다음은 투두리스트 화면을 만드는 예시코드다.&amp;amp;lt;!DOCTYPE html&amp;amp;gt;&amp;amp;lt;html lang=&quot;en&quot;&amp;amp;gt;&amp;amp;lt;head&amp;amp;gt; &amp;amp;lt;meta charset=&quot;UTF-8&quot;&amp;amp;gt; &amp;amp;lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&amp;amp;gt; &amp;amp;lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;ie=edge&quot;&amp;amp;gt; &amp;amp;lt;script src=&quot;https://kit.fontawesome.com/e71d03d7ce.js&quot;&amp;amp;gt;&amp;amp;lt;/script&amp;amp;gt; &amp;amp;lt;link rel=&quot;stylesheet&quot; href=&quot;style.css&quot;&amp;amp;gt; &amp;amp;lt;title&amp;amp;gt;Document&amp;amp;lt;/title&amp;amp;gt;&amp;amp;lt;/head&amp;amp;gt;&amp;amp;lt;body&amp;amp;gt; &amp;amp;lt;header&amp;amp;gt; &amp;amp;lt;span class=&quot;js-weather&quot;&amp;amp;gt; &amp;amp;lt;i class=&quot;fas fa-map-marker-alt&quot;&amp;amp;gt;&amp;amp;lt;/i&amp;amp;gt; &amp;amp;lt;i class=&quot;fas fa-temperature-low&quot;&amp;amp;gt;&amp;amp;lt;/i&amp;amp;gt; &amp;amp;lt;/span&amp;amp;gt; &amp;amp;lt;div class=&quot;js-clock&quot;&amp;amp;gt; &amp;amp;lt;h1 class=&quot;js-title&quot;&amp;amp;gt;&amp;amp;lt;/h1&amp;amp;gt; &amp;amp;lt;/div&amp;amp;gt; &amp;amp;lt;form class=&#39;js-form form&#39;&amp;amp;gt; &amp;amp;lt;input type=&quot;text&quot; placeholder=&quot;what is your name?&quot;/&amp;amp;gt; &amp;amp;lt;/form&amp;amp;gt; &amp;amp;lt;form class=&#39;js-toDoForm&#39;&amp;amp;gt; &amp;amp;lt;input type=&quot;text&quot; placeholder=&quot;Write a to do&quot;/&amp;amp;gt; &amp;amp;lt;/form&amp;amp;gt; &amp;amp;lt;/header&amp;amp;gt; &amp;amp;lt;aside&amp;amp;gt; &amp;amp;lt;div class=&quot;toDoList&quot;&amp;amp;gt; &amp;amp;lt;h4 class=&quot;js-greetings greetings&quot;&amp;amp;gt;&amp;amp;lt;/h4&amp;amp;gt; &amp;amp;lt;span class=&quot;toDoTitle&quot;&amp;amp;gt;To Do&amp;amp;lt;/span&amp;amp;gt; &amp;amp;lt;ul class=&quot;js-toDoList&quot;&amp;amp;gt;&amp;amp;lt;/ul&amp;amp;gt; &amp;amp;lt;/div&amp;amp;gt; &amp;amp;lt;/aside&amp;amp;gt; &amp;amp;lt;footer&amp;amp;gt;&amp;amp;lt;/footer&amp;amp;gt; &amp;amp;lt;span class=&quot;js-weather&quot;&amp;amp;gt;&amp;amp;lt;/span&amp;amp;gt; &amp;amp;lt;script src=&quot;clock.js&quot;&amp;amp;gt;&amp;amp;lt;/script&amp;amp;gt; &amp;amp;lt;script src=&quot;greeting.js&quot;&amp;amp;gt;&amp;amp;lt;/script&amp;amp;gt; &amp;amp;lt;script src=&quot;todo.js&quot;&amp;amp;gt;&amp;amp;lt;/script&amp;amp;gt; &amp;amp;lt;script src=&quot;bg.js&quot;&amp;amp;gt;&amp;amp;lt;/script&amp;amp;gt; &amp;amp;lt;script src=&quot;weather.js&quot;&amp;amp;gt;&amp;amp;lt;/script&amp;amp;gt;&amp;amp;lt;/body&amp;amp;gt;&amp;amp;lt;/html&amp;amp;gt;여기서 집중해야 할 것은 붉은색 글씨들이다.앞에서 HTML을 ‘약속된 표기방식’이라고 표현했다. 기본적으로 HTML은 다음과 같은 일종의 ‘약속된’ 모습을 보여주기 때문이다.&amp;amp;lt;html&amp;amp;gt; &amp;amp;lt;head&amp;amp;gt;&amp;amp;lt;/head&amp;amp;gt; &amp;amp;lt;body&amp;amp;gt;&amp;amp;lt;/body&amp;amp;gt;&amp;amp;lt;/html&amp;amp;gt; 문서를 시작한다.머리를 그린다. 몸통을 그린다. 문서를 끝낸다. 안에는 우리가 작성할 HTML 문서의 언어 양식, 검색을 위한 키워드, 저작자, 그리고 가져올 스타일시트 등 문서에서 시각적으로 나타나지 않는 부분들을 작성하게 된다. 태그 안쪽 코드만 따로 보자.```html&amp;lt;head&amp;gt; &amp;lt;meta charset=&quot;UTF-8&quot;&amp;gt; &amp;lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&amp;gt; &amp;lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;ie=edge&quot;&amp;gt; &amp;lt;script src=&quot;https://kit.fontawesome.com/e71d03d7ce.js&quot;&amp;gt;&amp;lt;/script&amp;gt; &amp;lt;link rel=&quot;stylesheet&quot; href=&quot;style.css&quot;&amp;gt; &amp;lt;title&amp;gt;Document&amp;lt;/title&amp;gt;&amp;lt;/head&amp;gt;```&amp;lt;script src - 로 시작하는 부분이 있다. 보통 &amp;lt;body&amp;gt; 하단에 작성되며 미리 만들어 놓은 javascript 코드를 가져오는 역할을 한다. 안에 위치하는 경우 특정 사이트에서 제공하는 디자인을 사용하기 위해 미리 해당 영역에 코드를 넣어줄 필요가 있기 때문이다. 여기서는 &quot;fontawesome&quot; 사이트에서 제공하는 디자인을 사용했는데,  src=&quot;&amp;lt;https://kit.fontawesome.com/e71d03d7ce.js&amp;gt;&quot; 와 같이 해당 사이트로 가는 링크가 연결된다. 이런 모습이 아니라면 대부분의 우리가 작성한 자바스크립트 코드는  태그의 안쪽, 하단에 작성하는 것이 보통이다. 짧은 코드에서는 자바스크립트를 가져오는 링크가 어디에 속하든 에러를 발생시키지 않지만 복잡한 코드에서는 웹브라우저를 동적으로 제어하는 자바스크립트가 미리 앞서서 나오는 것보다 하단에서 컨트롤하는 것이 관리 측면에서 더 좋기 때문이다.이어서 안쪽의 태그를 보자.```html&amp;lt;meta charset=&quot;UTF-8&quot;&amp;gt;&amp;lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&amp;gt;&amp;lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;ie=edge&quot;&amp;gt;``` 태그는 언어 양식, 검색을 위한 키워드, 저작자 등을 표시하는 태그다. 특별히 자주보게 될 부분은 &#39;utf-8&#39;인데, 브라우저에서 작동하는 문자 인코딩 방식으로 대부분의 언어를 지원하기 때문에 영어가 아닌 한국어 등의 사용을 위해 반드시 작성해주어야 한다.다음은 다.```html&amp;lt;body&amp;gt; &amp;lt;header&amp;gt; &amp;lt;span class=&quot;js-weather&quot;&amp;gt; &amp;lt;i class=&quot;fas fa-map-marker-alt&quot;&amp;gt;&amp;lt;/i&amp;gt; &amp;lt;i class=&quot;fas fa-temperature-low&quot;&amp;gt;&amp;lt;/i&amp;gt; &amp;lt;/span&amp;gt; &amp;lt;div class=&quot;js-clock&quot;&amp;gt; &amp;lt;h1 class=&quot;js-title&quot;&amp;gt;&amp;lt;/h1&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;form class=&#39;js-form form&#39;&amp;gt; &amp;lt;input type=&quot;text&quot; placeholder=&quot;what is your name?&quot;/&amp;gt; &amp;lt;/form&amp;gt; &amp;lt;form class=&#39;js-toDoForm&#39;&amp;gt; &amp;lt;input type=&quot;text&quot; placeholder=&quot;Write a to do&quot;/&amp;gt; &amp;lt;/form&amp;gt; &amp;lt;/header&amp;gt; &amp;lt;aside&amp;gt; &amp;lt;div class=&quot;toDoList&quot;&amp;gt; &amp;lt;h4 class=&quot;js-greetings greetings&quot;&amp;gt;&amp;lt;/h4&amp;gt; &amp;lt;span class=&quot;toDoTitle&quot;&amp;gt;To Do&amp;lt;/span&amp;gt; &amp;lt;ul class=&quot;js-toDoList&quot;&amp;gt;&amp;lt;/ul&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/aside&amp;gt; &amp;lt;footer&amp;gt;&amp;lt;/footer&amp;gt; &amp;lt;span class=&quot;js-weather&quot;&amp;gt;&amp;lt;/span&amp;gt; &amp;lt;script src=&quot;clock.js&quot;&amp;gt;&amp;lt;/script&amp;gt; &amp;lt;script src=&quot;greeting.js&quot;&amp;gt;&amp;lt;/script&amp;gt; &amp;lt;script src=&quot;todo.js&quot;&amp;gt;&amp;lt;/script&amp;gt; &amp;lt;script src=&quot;bg.js&quot;&amp;gt;&amp;lt;/script&amp;gt; &amp;lt;script src=&quot;weather.js&quot;&amp;gt;&amp;lt;/script&amp;gt;&amp;lt;/body&amp;gt;```  태그 안에  태그가 있습니다. 앞에서 봤던 와 비슷하게 생겼다. 는 본문의 제목 부분을 알려주는 시맨틱 태그다. 이전에는 이러한 태그들이 없었지만 도입부에 언급했듯이 조금 더 명시적으로 코드를 작성하고, 검토하기 쉽게 HTML5의 등장과 함께 생겨났다.시맨틱 태그는 제목과 브라우저 상단을 표시하는 태그, 글 목록을 보여주는 태그, 본문을 보여주는 태그, 광고 등 부수적인 내용을 담고 있는 태그, 사업자 관련 내용 혹은 저작권 등을 표시하는 태그로 이루어져 있다. 디자인에 따라 본문이나 광고란이 더 필요할 수도 있는데 이 때는 이나 태그를 추가해주면 된다.시맨틱 태그 중 부터 살펴보겠다.```html&amp;lt;header&amp;gt; &amp;lt;span class=&quot;js-weather&quot;&amp;gt; &amp;lt;i class=&quot;fas fa-map-marker-alt&quot;&amp;gt;&amp;lt;/i&amp;gt; &amp;lt;i class=&quot;fas fa-temperature-low&quot;&amp;gt;&amp;lt;/i&amp;gt; &amp;lt;/span&amp;gt; &amp;lt;div class=&quot;js-clock&quot;&amp;gt; &amp;lt;h1 class=&quot;js-title&quot;&amp;gt;&amp;lt;/h1&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;form class=&#39;js-form form&#39;&amp;gt; &amp;lt;input type=&quot;text&quot; placeholder=&quot;what is your name?&quot;/&amp;gt; &amp;lt;/form&amp;gt; &amp;lt;form class=&#39;js-toDoForm&#39;&amp;gt; &amp;lt;input type=&quot;text&quot; placeholder=&quot;Write a to do&quot;/&amp;gt; &amp;lt;/form&amp;gt;&amp;lt;/header&amp;gt;```브라우저의 상단 부분을 보여주는 내용을 가져왔다. 사실 작성자가 만든 투두 리스트에서 가 &#39;상단&#39;이라고 보기는 어렵다. **본문이 없기 때문이다.**![](/assets/images/posts/6-1.webp)여기서 는 어느 부분일지 생각해보자....![](/assets/images/posts/6-2.webp)&amp;amp;lt;header&amp;amp;gt;&amp;amp;lt;/header&amp;amp;gt;이 부분이다.가장 윗부분에 작은 2개의 아이콘은```html&amp;lt;span class=&quot;js-weather&quot;&amp;gt; &amp;lt;i class=&quot;fas fa-map-marker-alt&quot;&amp;gt;&amp;lt;/i&amp;gt; &amp;lt;i class=&quot;fas fa-temperature-low&quot;&amp;gt;&amp;lt;/i&amp;gt;&amp;lt;/span&amp;gt;```이 코드로 실행된다. 태그는 와 함께 언급되는 가장 많이 쓰이는 태그 중 하나다. 지금은 그냥 **아, 저부분은 본문에서 한 단락, 혹은 특정 부분을 표시해주는구나!** 라고 이해하고 넘어가도 충분하다.그 뒤에 class는 태그에 이름을 붙여주는 부분이다. 우리는 태그에 class 혹은 id를 생성해줄 수 있다. class와 id의 차이는 class는 여러 번, id는 단 한 번만 그 이름을 가질 수 있다는 것이다.예를 들어서 &#39;철수&#39;, &#39;동수&#39;, &#39;민수&#39;는 **&#39;철수&#39;, &#39;동수&#39;, &#39;민수&#39;** 라는 **id**를 가지면서도 **&#39;진달래반&#39;** 이라는 하나의 **class**를 가질 수 있다. 이것을 아이들의 입장에서 보면 &#39;철수&#39;라는 id는 &#39;철수&#39;만 가지고 있는 고유한 것이고 &#39;진달래반&#39;이라는 class는  &#39;철수&#39;도 가지고 &#39;동수&#39;도 가지고 &#39;민수&#39;도 가지고 있다. 그래서 class는 총 3번 사용되고, id는 각 1번씩만 사용될 수 있는 것이다.또한 class 태그에 띄어쓰기는 &quot;**2개의 클래스**&quot;를 구분하는 구분자다. i 태그는 &#39;fas&#39;라는 클래스와 &#39;fa-map-marker-alt&#39;라는 클래스 2개를 동시에 가진다. 이렇게 2개 이상의 클래스를 &#39;다중 클래스&#39;라고 한다. 그렇다면 &#39;다중 id&#39;도 있을까?**없다.**철수는 &#39;진달래반&#39;이면서도 &#39;노란색을 좋아하는 사람들의 모임&#39;에 가입했을 수도 있다. 그러나 철수는 &#39;명수&#39;라는 이름도 함께 가질 수는 없다. 그렇게 된다면 얘를 &quot;**철수이면서 명수이기도 한 친구야!&quot;** 라고 불러야할까? id는 고유하다.참고로 태그의 i는 &#39;이탤릭체&#39;라는 뜻에서의 &#39;i&#39;였다. 하지만 디자인적 요소는 CSS에서 모두 담당하게 되면서 html은 그 역할을 할 필요가 없어졌다. 대신에 사용자들은  태그를 icon 혹은 image 를 표시하기 위해 사용하게 되었다. 결국 모든 코드는 시각적인 요소가 중요하고 읽고, 쓰기가 쉬워야 하는데  태그가 딱! 적절했기 때문이었을까? 자연스럽게 현재 그러한 부분은  모두 태그가 담당하고 있다.**관성을 이기는 데이터**" }, { "title": "HTML element, CSS", "url": "/posts/HTML-element-CSS/", "categories": "1. 기술, 웹, 자바스크립트", "tags": "", "date": "2019-09-24 00:00:00 +0900", "snippet": "HTML은 다양한 요소(element)들로 구성되어 있다.요소란, ’&amp;lt;태그&amp;gt; 내용 &amp;lt;/태그&amp;gt;’ 에서 밑줄 친 전체를 뜻한다. 즉, 태그를 포함해 태그로 감싼 내용까지 모두 ’요소’라고 한다. 그리고 요소는 어떤 형태를 만들어내는데, 그 형태를 기준으로 블록 요소와 인라인 요소로 나눌 수 있다.그렇게 요소를 이루는 태그 중 &amp;lt;div&amp;gt; 태그와  태그를 살펴볼텐데, &amp;lt;div&amp;gt;와 태그는 그 자체만으로 특별한 기능을 갖고 있지 않아서 다방면으로 사용될 수 있다. 그래서 무분별하게 사용될 수 있다.아래 이미지는 블록 요소와 인라인 요소에 css로 배경색을 넣었을 때 어떻게 표현되는지 보여준다.codepen.io‘hello block’은 화면 끝까지, ’hello inline’은 글자가 끝날 때까지만 색칠이 되었다. 그렇다. ’&amp;lt;div&amp;gt;~’ 는 블록 요소, ’~’ 은 인라인 요소다. 그리고 ’&amp;lt;div&amp;gt;~’ 요소의 영역이 해당 줄 전체를 포함하기 때문에 줄 바꿈이 일어난다는 것을 알 수 있다.아래 이미지는 블록 요소와 인라인 요소에 css로 사이즈를 변경했을 때 어떻게 표현되는지 보여준다.codepen.io인라인 요소인 ‘~’ 은 heigh, width 값을 가지지 않기 때문에 텍스트에만 배경색이 칠해진다. 반면 ’&amp;lt;div&amp;gt;~’ 는 ’박스’의 모습으로 아주 예쁘게 만들어졌다.그러나 인라인 요소도 박스처럼 그릴 수 있다. ‘~’은 ‘&amp;lt;div&amp;gt;~’처럼 padding 속성을 가지고 있기 때문에 이를 이용하면 된다.codepen.io그러나 padding을 잘 못 다루면 다른 영역도 침범하게 돼서 전체 디자인이 망가질 수 있다.이럴때는, display:block; 을 넣어주면 되겠다.codepen.io인라인 요소를 다룰 때 또 주의해야 할 점은 블록 요소를 포함할 수 없다는 것이다. 아래 이미지는 블록 요소인 &amp;lt;h1&amp;gt; 태그를 인라인 요소 으로 감쌌을 때 해당 요소에 css 적용이 안되는 것을 보여준다.codepen.io제목 태그인 &amp;lt;h1&amp;gt;~&amp;lt;h6&amp;gt;는 모두 ’블록 요소’이므로 인라인 요소가 아니라 블록 요소에 직접 접근해서 css를 적용해야 한다.codepen.io관성을 이기는 데이터" }, { "title": "NodeJS - NPM, PM2", "url": "/posts/NodeJS-NPM-PM2/", "categories": "1. 기술, 웹, 자바스크립트", "tags": "", "date": "2019-09-23 00:00:00 +0900", "snippet": "Pm2는 NodeJS의 Package Manager인 ‘NPM’을 통해 설치할 수 있는 Package 중 하나로 단순 반복 작업을 도와주면서 에러를 실시간으로 감시하게 해주는 고마운 모듈이다. 생활코딩으로 유명하신 ‘이고잉’님의 말을 빌리면 NPM은 NodeJS계의 앱스토어다. 주 기능은 NodeJS를 사용하는 서버 측 관리자에게 편의를 제공하는 것이다.NodeJS로 구동하는 서버가 무너지지 않게 유지해주고, 코드를 리로드 하고 잘 동작하는지 확인하는 과정을 전체를 편리하게 해주면서 전체 프로세스 과정에서 매번 발생하는 로그를 실시간으로 보여준다. pm2 monit왼쪽은 현재 구동하고 있는 스크립트, 오른쪽은 전역에서 발생하는 로그를 보여준다. 강제로, 혹은 실수로 서버가 다운된다면오른쪽에서 그러한 내용을 보여주면서 다시 서버를 살려놓는다.사용 방법은 아래와 같다.우선 npm을 통해 전역에 pm2를 설치해준다.npm install pm2 -g다음으로 pm2를 실행하고 모니터링을 시작한다.pm2 start main.js --watch아래 이미지는 pm2가 실시간으로 모니터링하는 모습니다.pm2 start main(main.js) –watch여기서 pm2 start main 혹은 main.js 까지만 타이핑해도 실행이 되지만 –watch 까지 작성해주는 것이 좋다.“코드를 리로드 하고 잘 동작하는지 확인하는 과정을 전체를 편리하게 해 준다” 라는 pm2의 두 번째 기능을 잘 활용하기 위해서다.pm2 start main --watch이를 통해 위 vscode 캡쳐화면서에서 드래그된 20번째 라인을 수정함과 동시에 nodejs를 재시작하지 않고도 페이지가 변하는 모습을 볼 수 있다.웹 동작 화면코드를 작성하고 제대로 동작하는지 확인하기 위해서는 웹브라우저 화면뿐만 아니라 코드에 문제가 없는지 지속적으로 추적하고콘솔을 확인해야한다. 즉, 모든 동작마다 실행되는 log들을 한 번에 볼 필요가 생긴다.여기 붉은 박스(Global logs)에서 동작 및 에러를 실시간으로 감시할 수 있지만 좀 더 가볍게, 로그만 뽑아서 보고싶다면pm2 logs를 사용하면 된다.pm2 logs (log)관성을 이기는 데이터" } ]
